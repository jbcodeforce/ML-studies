<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://jeromeboyer.net/ML-studies/ml/classifier/ rel=canonical><link href=../ rel=prev><link href=../unsupervised/ rel=next><link rel=icon href=../../assets/logo.drawio.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>Classifier - Machine Learning Studies - Jerome Boyer</title><link rel=stylesheet href=../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#classifiers class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Machine Learning Studies - Jerome Boyer" class="md-header__button md-logo" aria-label="Machine Learning Studies - Jerome Boyer" data-md-component=logo> <img src=../../assets/logo.drawio.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Machine Learning Studies - Jerome Boyer </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Classifier </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/jbcodeforce/ML-studies title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> About </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../ class=md-tabs__link> Machine Learning </a> </li> <li class=md-tabs__item> <a href=../deep-learning/ class=md-tabs__link> Deep Learning </a> </li> <li class=md-tabs__item> <a href=../../genAI/ class=md-tabs__link> Generative AI </a> </li> <li class=md-tabs__item> <a href=../../techno/airflow/ class=md-tabs__link> Techno </a> </li> <li class=md-tabs__item> <a href=../../coding/ class=md-tabs__link> Coding </a> </li> <li class=md-tabs__item> <a href=../../solutions/ class=md-tabs__link> Solutions </a> </li> <li class=md-tabs__item> <a href=https://jbcodeforce.github.io class=md-tabs__link> Home </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Machine Learning Studies - Jerome Boyer" class="md-nav__button md-logo" aria-label="Machine Learning Studies - Jerome Boyer" data-md-component=logo> <img src=../../assets/logo.drawio.png alt=logo> </a> Machine Learning Studies - Jerome Boyer </label> <div class=md-nav__source> <a href=https://github.com/jbcodeforce/ML-studies title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <label class=md-nav__link for=__nav_1 id=__nav_1_label tabindex=0> <span class=md-ellipsis> About </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> About </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../guide_for_ai/ class=md-nav__link> <span class=md-ellipsis> Guide for AI/ML </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_3> <label class=md-nav__link for=__nav_1_3 id=__nav_1_3_label tabindex=0> <span class=md-ellipsis> Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_1_3> <span class="md-nav__icon md-icon"></span> Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../concepts/ class=md-nav__link> <span class=md-ellipsis> Core concepts </span> </a> </li> <li class=md-nav__item> <a href=../../concepts/maths/ class=md-nav__link> <span class=md-ellipsis> Math summary </span> </a> </li> <li class=md-nav__item> <a href=../../concepts/skill/ class=md-nav__link> <span class=md-ellipsis> Data scientist skill </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Architecture </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Architecture </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../architecture/sol-design/ class=md-nav__link> <span class=md-ellipsis> Solution Design </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_5> <label class=md-nav__link for=__nav_1_5 id=__nav_1_5_label tabindex=0> <span class=md-ellipsis> Data management </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_5_label aria-expanded=false> <label class=md-nav__title for=__nav_1_5> <span class="md-nav__icon md-icon"></span> Data management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../data/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../data/features/ class=md-nav__link> <span class=md-ellipsis> Feature Engineering </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Machine Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Machine Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Classifier </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Classifier </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#perceptron class=md-nav__link> <span class=md-ellipsis> Perceptron </span> </a> </li> <li class=md-nav__item> <a href=#adaline class=md-nav__link> <span class=md-ellipsis> Adaline </span> </a> </li> <li class=md-nav__item> <a href=#logistic-regression class=md-nav__link> <span class=md-ellipsis> Logistic regression </span> </a> </li> <li class=md-nav__item> <a href=#maximum-margin-classification-with-support-vector-machines-svm class=md-nav__link> <span class=md-ellipsis> Maximum margin classification with support vector machines (SVM) </span> </a> </li> <li class=md-nav__item> <a href=#decision-trees class=md-nav__link> <span class=md-ellipsis> Decision Trees </span> </a> </li> <li class=md-nav__item> <a href=#combining-weak-to-strong-learners-via-random-forests class=md-nav__link> <span class=md-ellipsis> Combining weak to strong learners via random forests </span> </a> </li> <li class=md-nav__item> <a href=#k-nearest-neighbor-classifier-knn class=md-nav__link> <span class=md-ellipsis> k-nearest neighbor classifier (KNN) </span> </a> </li> <li class=md-nav__item> <a href=#see-also class=md-nav__link> <span class=md-ellipsis> See also </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../unsupervised/ class=md-nav__link> <span class=md-ellipsis> Unsupervised Learning </span> </a> </li> <li class=md-nav__item> <a href=../../anomaly/ class=md-nav__link> <span class=md-ellipsis> Anomaly detection </span> </a> </li> <li class=md-nav__item> <a href=../../neuro-symbolic/ class=md-nav__link> <span class=md-ellipsis> Hybrid AI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Deep Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Deep Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../deep-learning/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../nlp/ class=md-nav__link> <span class=md-ellipsis> NLP </span> </a> </li> <li class=md-nav__item> <a href=../../coding/ddp/ class=md-nav__link> <span class=md-ellipsis> Distributed Data Parallel </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Generative AI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Generative AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../genAI/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/review/ class=md-nav__link> <span class=md-ellipsis> Skill set </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/prompt-eng/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> </a> </li> <li class=md-nav__item> <a href=../../architecture/sol-design/ class=md-nav__link> <span class=md-ellipsis> Solution Design </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/rag/ class=md-nav__link> <span class=md-ellipsis> RAG </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/agentic/ class=md-nav__link> <span class=md-ellipsis> Agentic AI </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/anthropic/ class=md-nav__link> <span class=md-ellipsis> Anthropic </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/openai/ class=md-nav__link> <span class=md-ellipsis> OpenAI </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/mcp/ class=md-nav__link> <span class=md-ellipsis> MCP </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/mistral/ class=md-nav__link> <span class=md-ellipsis> Mistral </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/cohere/ class=md-nav__link> <span class=md-ellipsis> Cohere </span> </a> </li> <li class=md-nav__item> <a href=../../techno/watsonx/ class=md-nav__link> <span class=md-ellipsis> WatsonX.ai </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Techno </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Techno </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../techno/airflow/ class=md-nav__link> <span class=md-ellipsis> Airflow </span> </a> </li> <li class=md-nav__item> <a href=../../techno/feature_store/ class=md-nav__link> <span class=md-ellipsis> Feature Store </span> </a> </li> <li class=md-nav__item> <a href=../../kaggle/ class=md-nav__link> <span class=md-ellipsis> Kaggle </span> </a> </li> <li class=md-nav__item> <a href=../../techno/opensearch/ class=md-nav__link> <span class=md-ellipsis> OpenSearch </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/python-code/ class=md-nav__link> <span class=md-ellipsis> Python studies </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/yarfba/ai-ml/sagemaker class=md-nav__link> <span class=md-ellipsis> SageMaker </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/spark-studies/ class=md-nav__link> <span class=md-ellipsis> Spark studies </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_8> <label class=md-nav__link for=__nav_5_8 id=__nav_5_8_label tabindex=0> <span class=md-ellipsis> UI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_8_label aria-expanded=false> <label class=md-nav__title for=__nav_5_8> <span class="md-nav__icon md-icon"></span> UI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../techno/gradio/ class=md-nav__link> <span class=md-ellipsis> Gradio </span> </a> </li> <li class=md-nav__item> <a href=../../techno/streamlit/ class=md-nav__link> <span class=md-ellipsis> Streamlit </span> </a> </li> <li class=md-nav__item> <a href=../../techno/taipy/ class=md-nav__link> <span class=md-ellipsis> TaiPy </span> </a> </li> <li class=md-nav__item> <a href=../../techno/nicegui/ class=md-nav__link> <span class=md-ellipsis> NiceGUI </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../techno/gcp/ class=md-nav__link> <span class=md-ellipsis> GCP </span> </a> </li> <li class=md-nav__item> <a href=../../techno/watsonx/ class=md-nav__link> <span class=md-ellipsis> WatsonX.ai </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Coding </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Coding </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../coding/ class=md-nav__link> <span class=md-ellipsis> Coding getting started </span> </a> </li> <li class=md-nav__item> <a href=../../coding/visualization/ class=md-nav__link> <span class=md-ellipsis> Data Visualization </span> </a> </li> <li class=md-nav__item> <a href=../../coding/haystack/ class=md-nav__link> <span class=md-ellipsis> Haystack.ai </span> </a> </li> <li class=md-nav__item> <a href=../../coding/langchain/ class=md-nav__link> <span class=md-ellipsis> LangChain </span> </a> </li> <li class=md-nav__item> <a href=../../coding/langgraph/ class=md-nav__link> <span class=md-ellipsis> LangGraph </span> </a> </li> <li class=md-nav__item> <a href=../../coding/llama-index/ class=md-nav__link> <span class=md-ellipsis> LlamaIndex </span> </a> </li> <li class=md-nav__item> <a href=../../coding/pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../../coding/pytorch/ class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../../coding/sklearn/ class=md-nav__link> <span class=md-ellipsis> Scikit-learn </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> <span class=md-ellipsis> Solutions </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Solutions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../solutions/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../techno/players_to_look/ class=md-nav__link> <span class=md-ellipsis> Key AI startups </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#perceptron class=md-nav__link> <span class=md-ellipsis> Perceptron </span> </a> </li> <li class=md-nav__item> <a href=#adaline class=md-nav__link> <span class=md-ellipsis> Adaline </span> </a> </li> <li class=md-nav__item> <a href=#logistic-regression class=md-nav__link> <span class=md-ellipsis> Logistic regression </span> </a> </li> <li class=md-nav__item> <a href=#maximum-margin-classification-with-support-vector-machines-svm class=md-nav__link> <span class=md-ellipsis> Maximum margin classification with support vector machines (SVM) </span> </a> </li> <li class=md-nav__item> <a href=#decision-trees class=md-nav__link> <span class=md-ellipsis> Decision Trees </span> </a> </li> <li class=md-nav__item> <a href=#combining-weak-to-strong-learners-via-random-forests class=md-nav__link> <span class=md-ellipsis> Combining weak to strong learners via random forests </span> </a> </li> <li class=md-nav__item> <a href=#k-nearest-neighbor-classifier-knn class=md-nav__link> <span class=md-ellipsis> k-nearest neighbor classifier (KNN) </span> </a> </li> <li class=md-nav__item> <a href=#see-also class=md-nav__link> <span class=md-ellipsis> See also </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=classifiers>Classifiers<a class=headerlink href=#classifiers title="Permanent link">&para;</a></h1> <p><strong>Classification</strong> problem is when we are trying to predict one of a small number of discrete-valued outputs. The class labels are defined as multiple classes or binary classification task, where the machine learning algorithm learns a set of rules in order to distinguish between the possible classes.</p> <p>Below is a python example of using the iris flower NIST dataset: 4 features, three potential classes:</p> <div class=highlight><pre><span></span><code><span class=n>feature_names</span><span class=o>=</span> <span class=p>[</span><span class=s1>&#39;sepal length (cm)&#39;</span><span class=p>,</span> <span class=s1>&#39;sepal width (cm)&#39;</span><span class=p>,</span> <span class=s1>&#39;petal length (cm)&#39;</span><span class=p>,</span> <span class=s1>&#39;petal width (cm)&#39;</span><span class=p>]</span> 
<span class=n>target_names</span><span class=o>=</span> <span class=p>[</span><span class=s1>&#39;setosa&#39;</span><span class=p>,</span> <span class=s1>&#39;versicolor&#39;</span><span class=p>,</span> <span class=s1>&#39;virginica&#39;</span><span class=p>]</span>
<span class=n>data</span><span class=o>=</span> <span class=p>[</span> <span class=mf>5.1</span><span class=p>,</span>  <span class=mf>3.5</span><span class=p>,</span>  <span class=mf>1.4</span><span class=p>,</span>  <span class=mf>0.2</span><span class=p>],</span> <span class=p>[</span> <span class=mf>4.9</span><span class=p>,</span>  <span class=mf>3.</span> <span class=p>,</span>  <span class=mf>1.4</span><span class=p>,</span>  <span class=mf>0.2</span><span class=p>]</span>

<span class=n>target</span><span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span><span class=err>…</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=err>…</span> <span class=mi>2</span><span class=p>,</span><span class=mi>2</span><span class=p>,</span><span class=mi>2</span><span class=p>]</span>
</code></pre></div> <p>Three majors components to have for each machine learning classifier:</p> <ul> <li><strong>Representation</strong>: define what is the classifier: a rule, a decision tree, a neural network...</li> <li><strong>Evaluation</strong>: how to know if a given classifier is giving good or bad results: how to asses result rule. Could be the <code># of errors</code> on some test set, <code># of recalls</code>, squared error, likelihood?... We may compute the coverage of a rule: <code># of data points</code> that satisfy the conditions and the <code>accuracy = # of correct predictions / coverage</code>.</li> <li><strong>Optimization</strong>: how to search among all the alternatives, greedy search or gradient descent? One idea is to build a set of rules by finding the conditions that maximize accuracy.</li> </ul> <p>For each dataset, try to humanly inspect the data, and do some plotting diagrams with some attributes over others. Then to select a naive class, look at attribute, where we can derive some basic rules. This will build a first hypothesis. To assess an hypothesis build a <strong>confusion matrix</strong>: a square matrix where column and rows are the different class label of an outcome. The cells count the number of time the rules classified the dataset. Assess the <strong>accuracy</strong> number: sum good results/ total results.</p> <details class=notes> <summary>Code execution</summary> <p>All the Classifier Python apps execute well from the python environment in docker. See <a href=../../coding/#environments>environement note.</a></p> </details> <h2 id=perceptron>Perceptron<a class=headerlink href=#perceptron title="Permanent link">&para;</a></h2> <p>Based on the human neuron model, Frank Rosenblatt proposed an algorithm that would automatically learn the optimal weight coefficients that are then multiplied with the input features in order to make the decision of whether a neuron fires or not. In the context of supervised learning and classification, such an algorithm could then be used to predict if a sample belonged to one class or the other.</p> <p>The problem is reduced to a binary classification (-1,1), and an activation function that takes a linear combination of input X, with corresponding weights vector W, to compute the net input as:</p> <div class=highlight><pre><span></span><code><span class=n>z</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>w</span><span class=p>(</span><span class=n>i</span><span class=p>)</span> <span class=o>*</span> <span class=n>x</span><span class=p>(</span><span class=n>i</span><span class=p>))</span> <span class=n>i</span> <span class=kn>from</span><span class=w> </span><span class=mi>1</span> <span class=n>to</span> <span class=n>n</span>
</code></pre></div> <p>If the value is greater than a threshold the output is 1, -1 otherwise. The function is called <code>unit step</code> function. </p> <p>If w0 is set to be -threshold and x0=1 then the equation becomes:</p> <p><img alt src="https://latex.codecogs.com/svg.latex?h(x)=\sum_{i} \theta_{i} * x_{i}= \theta^{T}*x" width=300></p> <p>The following python functions in a Perceptron class, use numpy library to compute the matrix dot product wT*x:</p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>netInput</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>X</span><span class=p>):</span>
     <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=bp>self</span><span class=o>.</span><span class=n>weights</span><span class=p>[</span><span class=mi>1</span><span class=p>:])</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>weights</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
   
<span class=k>def</span><span class=w> </span><span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>X</span><span class=p>):</span>
   <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>netInput</span><span class=p>(</span><span class=n>X</span><span class=p>)</span><span class=o>&gt;=</span><span class=mf>0.0</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</code></pre></div> <p>The weights are computed using the training set. The value of delta, which is used to update the weight , is calculated by the perceptron learning rule:</p> <p><img alt src="https://latex.codecogs.com/svg.latex?\Delta(\theta_{j})= \eta*(y_{i} - mean(y_{i}))* x_{i}^j" width=300></p> <p>eta is the learning rate, Y(i) is the known answer or target for i th sample. The weight update is proportional to the value of X(i)   It is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small.</p> <p><img alt src=../images/perceptron.png width=800></p> <p>The <a href=https://github.com/jbcodeforce/ML-studies/blob/6073cbb4560386dde09d833878ad0724172ded4b/ml-python/classifiers/Perceptron.py#L24>fit function</a> implements the weights update algorithm.</p> <p>Test the python Perceptron implementation, uisnf NIST iris dataset. The way to use the perceptron: Create an instance by specifying the eta coefficient and the number of epochs (passes over the training set) to perform</p> <div class=highlight><pre><span></span><code><span class=c1>#under ml-python/classifiers folder</span>
python<span class=w> </span>TestPerceptron.py
</code></pre></div> <p>The test loads the dataset, fit the Perceptron with a training set, plots some sample of the two types of Iris. Then displays the decision boundary to classify an Iris in one of the two classes: setosa, versicolor.</p> <h2 id=adaline>Adaline<a class=headerlink href=#adaline title="Permanent link">&para;</a></h2> <p>In ADAptive LInear NEuron classifier, the weights are updated based on a linear activation function (the <code>Identity</code> function) rather than a unit step function like in the Perceptron.</p> <p><img alt src=../images/adaline.png width=800></p> <div class=highlight><pre><span></span><code><span class=c1># Start python docker</span>
<span class=c1># under ml-python/classifiers folder</span>
python<span class=w> </span>TestAdaline.py
</code></pre></div> <p>The test works on the Iris dataset too, when we choose a learning rate that is too large, we have an error rate that becomes larger in every epoch because we overshoot the global minimum.</p> <p><img alt src=../images/ada-learning-rate-1.png></p> <p>When the features are standardized (each feature value is reduced by the mean and divided by the standard deviation) the ADALine algorithm converges more quickly.</p> <p>The two regions illustrates the two classes, with good results: </p> <p><img alt src=../images/ada-iris-classes.png></p> <p>The following curve shows the cost function results per iteration or epoch</p> <p><img alt src=../images/adaline-learning-rate.png></p> <p><div class=highlight><pre><span></span><code><span class=n>X_std</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>copy</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
<span class=n>X_std</span><span class=p>[:,</span><span class=mi>0</span><span class=p>]</span><span class=o>=</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span><span class=mi>0</span><span class=p>]</span><span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span><span class=mi>0</span><span class=p>]))</span><span class=o>/</span><span class=n>np</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span><span class=mi>0</span><span class=p>])</span>
<span class=n>X_std</span><span class=p>[:,</span><span class=mi>1</span><span class=p>]</span><span class=o>=</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span><span class=mi>1</span><span class=p>]</span><span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span><span class=mi>1</span><span class=p>]))</span><span class=o>/</span><span class=n>np</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span><span class=mi>1</span><span class=p>])</span>
</code></pre></div>   The previous approach can take a lot of time when the dataset includes millions of records. A more efficient approach is to take the <strong>stochastic gradient descent</strong> approach. It is used with online training, where the algorithm is trained on-the-fly, while new training set arrives.</p> <p>The weights are computed with: </p> <p><img alt src="https://latex.codecogs.com/svg.latex?w_{i}=\eta*(y_{i}%20-%20\phi%20(z_{i}))*%20x_{i}" width=300></p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>updateWeights</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>xi</span><span class=p>,</span><span class=n>target</span><span class=p>):</span>
        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>netInput</span><span class=p>(</span> <span class=n>xi</span><span class=p>)</span>
        <span class=n>error</span> <span class=o>=</span> <span class=p>(</span><span class=n>target</span> <span class=o>-</span> <span class=n>output</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>weights</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span> <span class=o>+=</span> <span class=bp>self</span><span class=o>.</span><span class=n>eta</span> <span class=o>*</span> <span class=n>xi</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span> <span class=n>error</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>weights</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+=</span> <span class=bp>self</span><span class=o>.</span><span class=n>eta</span> <span class=o>*</span> <span class=n>error</span>
        <span class=n>cost</span> <span class=o>=</span> <span class=p>(</span><span class=n>error</span><span class=o>**</span> <span class=mi>2</span><span class=p>)</span><span class=o>/</span> <span class=mf>2.0</span>
        <span class=k>return</span> <span class=n>cost</span>
</code></pre></div> <p>To obtain accurate results via stochastic gradient descent, it is important to present it with data in a random order, which is why we want to shuffle the training set for every epoch to prevent cycles.   <img alt src=../images/ada-iris-boundaries.png></p> <h2 id=logistic-regression>Logistic regression<a class=headerlink href=#logistic-regression title="Permanent link">&para;</a></h2> <p>Another classification approach is to use ‘Logistic Regression’ which performs very well on linearly separable set:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LogisticRegression</span>
<span class=n>lr</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span> <span class=n>C</span> <span class=o>=</span> <span class=mf>1000.0</span><span class=p>,</span> <span class=n>random_state</span> <span class=o>=</span> <span class=mi>0</span><span class=p>)</span>
<span class=n>lr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span> <span class=n>X_train_std</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>lr</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test_std</span><span class=p>[</span><span class=mi>0</span><span class=p>,:])</span>
</code></pre></div> <p>For C=1000 we have the following results:</p> <p><img alt src=../images/iris-log-reg-1.png width=500></p> <p>Logistic regression uses the odds-ratio <code>P/(1-P)</code>, P being the probability to have event e: in our case P could be the probability that a set of values for the feature X leads that the sample is part of a class 1. </p> <p>In fact, the mathematical model uses the <code>log (P/(1-P))</code> as function in the model. It takes input values in the range 0 to 1 and transforms them to values over the entire real number range, which we can use to express a linear relationship between feature values and the log-odds:</p> <p><img alt src="https://latex.codecogs.com/svg.latex?logit(p(%20y%20=%201%20|%20x))=\sum_{i} \theta_{i} * x_{i}= \theta^{T}*x" width=400></p> <p>For logistic regression, the hypothesis function is used to predict the probability of having a certain sample X being of class y=1. This is the sigmoid function:</p> <p><img alt src="https://latex.codecogs.com/svg.latex?\phi(z)=\frac{1}{(1+e^{-z})}" width=300></p> <p>Here, z is the net input, that is, the linear combination of weights and sample features= W’.x </p> <p>The sigmoid function is used as activation function in the classifier:</p> <p><img alt src=../images/sigmoid-fct-werror.png width=600></p> <p>The output of the sigmoid function is then interpreted as the probability of particular sample belonging to class 1 </p> <p><img alt src="https://latex.codecogs.com/svg.latex?\phi(z)=P(y=1 | x;w)" width=300></p> <p>given its features X parameterized by the weights W.</p> <p>Logistic regression can be used to predict the chance that a patient has a particular disease given certain symptoms. As seen before to find the weights W, we need to minimize a cost function, which in the case of logistic regression is:</p> <p><img alt src="https://latex.codecogs.com/svg.latex?J(w)=C\left [ \sum_{i}^{n} (-y^{i} log(\phi(z^{i})) - (1 - y^{i}))log(1-\phi(z^{i})) \right ] + \frac{1}{2}\left| w \right|^2" width=500></p> <p>The C=1/lambda parameter used in logistic regression api is the factor to control overfitting.</p> <p><img alt src="https://latex.codecogs.com/svg.latex?\frac{1}{2} ||W||^2"> is the regularization bias to penalize extreme parameter weights.</p> <p>Logistic regression is a useful model for online learning via stochastic gradient descent, but also allows us to predict the probability of a particular event. </p> <h2 id=maximum-margin-classification-with-support-vector-machines-svm>Maximum margin classification with support vector machines (SVM)<a class=headerlink href=#maximum-margin-classification-with-support-vector-machines-svm title="Permanent link">&para;</a></h2> <p>In SVM, the goal is to maximize the margin: the distance between the decision boundary and the training samples.</p> <p><img alt src=../images/svn-1.png width=600></p> <p>The rationale behind having decision boundaries with large margins is that they tend to have a lower generalization error whereas models with small margins are more prone to overfitting.</p> <p>To prepare the data here is the standard code that is using SciKit <code>model_selection</code> to split the input data set into training and test sets and then a standardScaler to normalize values</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn</span><span class=w> </span><span class=kn>import</span> <span class=n>model_selection</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span>

<span class=n>iris</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_iris</span><span class=p>()</span>
<span class=n>X</span> <span class=o>=</span> <span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>[:,</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]]</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>iris</span><span class=o>.</span><span class=n>target</span>

<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>model_selection</span><span class=o>.</span><span class=n>train_test_split</span><span class=p>(</span>
    <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.3</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span>
<span class=p>)</span>

<span class=n>sc</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>sc</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
<span class=n>X_train_std</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
<span class=n>X_test_std</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <p>To train a SVM model using sklearn:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.svm</span><span class=w> </span><span class=kn>import</span> <span class=n>SVC</span>
<span class=n>svm</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;linear&#39;</span><span class=p>,</span><span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span><span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=n>svm</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_std</span><span class=p>,</span><span class=n>y_train</span><span class=p>)</span>
</code></pre></div> <p>See <a href=https://github.com/jbcodeforce/ML-studies/blob/master/ml-python/classifiers/SVM-IRIS.py>code in SVM-IRIS.py</a></p> <p>The SVMs mostly care about the points that are closest to the decision boundary (support vectors).</p> <p><img alt src=../images/svm-results.png></p> <p>The SVM can use Radial Basis Function kernel, to create nonlinear combinations of the original features to project them onto a higher dimensional space via a mapping function phi() where it becomes linearly separable. </p> <div class=highlight><pre><span></span><code><span class=n>svm</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;rbf&#39;</span><span class=p>,</span><span class=n>C</span><span class=o>=</span><span class=mf>10.0</span><span class=p>,</span><span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.10</span><span class=p>)</span>
<span class=n>svm</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_std</span><span class=p>,</span><span class=n>y_train</span><span class=p>)</span>
</code></pre></div> <p>Gamma is a cut-off parameter for the Gaussian sphere. If we increase the value for gamma, we increase the influence or reach of the training samples, which leads to a softer decision boundary. Gamma at 0.1. Optimizing Gamma is important to avoid overfitting.</p> <h2 id=decision-trees>Decision Trees<a class=headerlink href=#decision-trees title="Permanent link">&para;</a></h2> <p>The decision tree model learns a series of questions to infer the class labels of the samples. </p> <p>The algorithm is to start at the tree root and to split the data on the feature that results in the largest information gain (IG). In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are pure. This means that the samples at each node all belong to the same class. In practice, this can result in a very deep tree with many nodes, which can easily lead to overfitting. Thus, we typically want to prune the tree by setting a limit for the maximal depth of the tree.</p> <p>In order to split the nodes at the most informative features, we need to define an objective function that we want to optimize via the tree learning algorithm. In binary decision trees there are 3 commonly used impurity function: Gini_impurity(), entropy(), and the classification_error().</p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>gini</span><span class=p>(</span><span class=n>p</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>p</span> <span class=o>*</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>p</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>p</span><span class=p>)</span><span class=o>*</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>p</span><span class=p>))</span>

<span class=k>def</span><span class=w> </span><span class=nf>entropy</span><span class=p>(</span> <span class=n>p</span><span class=p>):</span>
    <span class=k>return</span> <span class=o>-</span> <span class=n>p</span><span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log2</span><span class=p>(</span> <span class=n>p</span><span class=p>)</span> <span class=o>-</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>p</span><span class=p>)</span><span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log2</span><span class=p>((</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>p</span><span class=p>))</span>

<span class=k>def</span><span class=w> </span><span class=nf>error</span><span class=p>(</span> <span class=n>p</span><span class=p>):</span>
    <span class=k>return</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>([</span> <span class=n>p</span><span class=p>,</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>p</span><span class=p>])</span>
</code></pre></div> <p>Decision trees are particularly attractive if we care about interpretability. </p> <p>See <a href=https://github.com/jbcodeforce/ML-studies/blob/master/ml-python/classifiers/DecisionTreeIRIS.py>DecisionTreeIRIS.py code</a> and <a href=https://github.com/jbcodeforce/ML-studies/blob/master/notebooks/DecisionTree.ipynb>this DecisionTree notebook</a>.</p> <h2 id=combining-weak-to-strong-learners-via-random-forests>Combining weak to strong learners via random forests<a class=headerlink href=#combining-weak-to-strong-learners-via-random-forests title="Permanent link">&para;</a></h2> <p>Random forests have gained huge popularity in applications of machine learning in 2010s due to their good classification performance, scalability, and ease of use. Intuitively, a random forest can be considered as an ensemble of decision trees. The idea behind ensemble learning is to combine weak learners to build a more robust model, that has a better generalization error and is less susceptible to overfitting.</p> <p>The only parameter to play with is the number of trees, and the max depth of each tree. The larger the number of trees, the better the performance of the random forest classifier at the expense of an increased computational cost. Scikit-learn provides tools to automatically find the best parameter combinations (via cross-validation)</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>buildTrainingSet</span><span class=p>()</span>
<span class=n>forest</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>criterion</span> <span class=o>=</span><span class=s1>&#39;entropy&#39;</span><span class=p>,</span> <span class=n>n_estimators</span> <span class=o>=</span> <span class=mi>10</span><span class=p>,</span> <span class=n>random_state</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span> <span class=n>n_jobs</span> <span class=o>=</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>forest</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span> <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=o>...</span>
</code></pre></div> <p>the sample size of the bootstrap sample is chosen to be equal to the number of samples in the original training set.</p> <h2 id=k-nearest-neighbor-classifier-knn>k-nearest neighbor classifier (KNN)<a class=headerlink href=#k-nearest-neighbor-classifier-knn title="Permanent link">&para;</a></h2> <p>For KNN, we define some distance metric between the items in our dataset, and find the K closest items.</p> <p>Machine learning algorithms can be grouped into parametric and nonparametric models. Using parametric models, we estimate parameters from the training dataset to learn a function that can classify new data points without requiring the original training dataset anymore.</p> <p>With nonparametric models there is no fixed set of parameters, and the number of parameters grows with the training data (decision tree, random forest and kernel SVM). </p> <p>KNN belongs to a subcategory of nonparametric models that is described as instance-based learning which are characterized by memorizing the training dataset, and lazy learning is a special case of instance-based learning that is associated with no (zero) cost during the learning process.</p> <p>The KNN algorithm is fairly straightforward and can be summarized by the following steps:</p> <ol> <li>Choose the number of k and a distance metric function.</li> <li>Find the k nearest neighbors of the sample that we want to classify.</li> <li>Assign the class label by majority vote.</li> </ol> <p>In the case of a tie, the scikit-learn implementation of the KNN algorithm will prefer the neighbors with a closer distance to the sample.</p> <p>See the <a href=https://github.com/jbcodeforce/ML-studies/blob/master/notebooks/KNN.ipynb>KNN notebook</a>.</p> <p>It is important to mention that KNN is very susceptible to overfitting due to the curse of dimensionality. The curse of dimensionality describes the phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset.</p> <p>The K-nearest neighbor classifier offers lazy learning that allows us to make predictions without any model training but with a more computationally expensive prediction step.</p> <h2 id=see-also>See also<a class=headerlink href=#see-also title="Permanent link">&para;</a></h2> <p>See also <a href=../../coding/pytorch/ >classifiers done with PyTorch as neural network</a>.</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../ class="md-footer__link md-footer__link--prev" aria-label="Previous: Introduction"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Introduction </div> </div> </a> <a href=../unsupervised/ class="md-footer__link md-footer__link--next" aria-label="Next: Unsupervised Learning"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Unsupervised Learning </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2018 - 2024 Jerome Boyer </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/jbcodeforce target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://linkedin.com/in/jeromeboyer target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M536.4-26.3c9.8-3.5 20.6-1 28 6.3s9.8 18.2 6.3 28l-178 496.9c-5 13.9-18.1 23.1-32.8 23.1-14.2 0-27-8.6-32.3-21.7l-64.2-158c-4.5-11-2.5-23.6 5.2-32.6l94.5-112.4c5.1-6.1 4.7-15-.9-20.6s-14.6-6-20.6-.9l-112.4 94.3c-9.1 7.6-21.6 9.6-32.6 5.2L38.1 216.8c-13.1-5.3-21.7-18.1-21.7-32.3 0-14.7 9.2-27.8 23.1-32.8z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "content.code.copy", "search.highlight", "navigation.instant", "navigation.instant.progress", "navigation.tabs", "navigation.tabs.sticky", "navigation.instant", "navigation.tracking", "navigation.top", "navigation.footer"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.79ae519e.min.js></script> </body> </html>