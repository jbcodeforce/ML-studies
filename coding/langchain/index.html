<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://jeromeboyer.net/ML-studies/coding/langchain/ rel=canonical><link href=../haystack/ rel=prev><link href=../langgraph/ rel=next><link rel=icon href=../../assets/logo.drawio.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>LangChain - Machine Learning Studies - Jerome Boyer</title><link rel=stylesheet href=../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#langchain-study class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Machine Learning Studies - Jerome Boyer" class="md-header__button md-logo" aria-label="Machine Learning Studies - Jerome Boyer" data-md-component=logo> <img src=../../assets/logo.drawio.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Machine Learning Studies - Jerome Boyer </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> LangChain </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/jbcodeforce/ML-studies title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> About </a> </li> <li class=md-tabs__item> <a href=../../ml/ class=md-tabs__link> Machine Learning </a> </li> <li class=md-tabs__item> <a href=../../ml/deep-learning/ class=md-tabs__link> Deep Learning </a> </li> <li class=md-tabs__item> <a href=../../genAI/ class=md-tabs__link> Generative AI </a> </li> <li class=md-tabs__item> <a href=../../techno/airflow/ class=md-tabs__link> Techno </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../ class=md-tabs__link> Coding </a> </li> <li class=md-tabs__item> <a href=../../solutions/ class=md-tabs__link> Solutions </a> </li> <li class=md-tabs__item> <a href=https://jbcodeforce.github.io class=md-tabs__link> Home </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Machine Learning Studies - Jerome Boyer" class="md-nav__button md-logo" aria-label="Machine Learning Studies - Jerome Boyer" data-md-component=logo> <img src=../../assets/logo.drawio.png alt=logo> </a> Machine Learning Studies - Jerome Boyer </label> <div class=md-nav__source> <a href=https://github.com/jbcodeforce/ML-studies title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <label class=md-nav__link for=__nav_1 id=__nav_1_label tabindex=0> <span class=md-ellipsis> About </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> About </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../guide_for_ai/ class=md-nav__link> <span class=md-ellipsis> Guide for AI/ML </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_3> <label class=md-nav__link for=__nav_1_3 id=__nav_1_3_label tabindex=0> <span class=md-ellipsis> Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_1_3> <span class="md-nav__icon md-icon"></span> Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../concepts/ class=md-nav__link> <span class=md-ellipsis> Core concepts </span> </a> </li> <li class=md-nav__item> <a href=../../concepts/maths/ class=md-nav__link> <span class=md-ellipsis> Math summary </span> </a> </li> <li class=md-nav__item> <a href=../../concepts/skill/ class=md-nav__link> <span class=md-ellipsis> Data scientist skill </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Architecture </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Architecture </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../architecture/sol-design/ class=md-nav__link> <span class=md-ellipsis> Solution Design </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_5> <label class=md-nav__link for=__nav_1_5 id=__nav_1_5_label tabindex=0> <span class=md-ellipsis> Data management </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_5_label aria-expanded=false> <label class=md-nav__title for=__nav_1_5> <span class="md-nav__icon md-icon"></span> Data management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../data/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../data/features/ class=md-nav__link> <span class=md-ellipsis> Feature Engineering </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Machine Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Machine Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../ml/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../ml/classifier/ class=md-nav__link> <span class=md-ellipsis> Classifier </span> </a> </li> <li class=md-nav__item> <a href=../../ml/unsupervised/ class=md-nav__link> <span class=md-ellipsis> Unsupervised Learning </span> </a> </li> <li class=md-nav__item> <a href=../../anomaly/ class=md-nav__link> <span class=md-ellipsis> Anomaly detection </span> </a> </li> <li class=md-nav__item> <a href=../../neuro-symbolic/ class=md-nav__link> <span class=md-ellipsis> Hybrid AI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Deep Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Deep Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../ml/deep-learning/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../ml/nlp/ class=md-nav__link> <span class=md-ellipsis> NLP </span> </a> </li> <li class=md-nav__item> <a href=../ddp/ class=md-nav__link> <span class=md-ellipsis> Distributed Data Parallel </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Generative AI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Generative AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../genAI/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/review/ class=md-nav__link> <span class=md-ellipsis> Skill set </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/prompt-eng/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> </a> </li> <li class=md-nav__item> <a href=../../architecture/sol-design/ class=md-nav__link> <span class=md-ellipsis> Solution Design </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/rag/ class=md-nav__link> <span class=md-ellipsis> RAG </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/agentic/ class=md-nav__link> <span class=md-ellipsis> Agentic AI </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/anthropic/ class=md-nav__link> <span class=md-ellipsis> Anthropic </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/openai/ class=md-nav__link> <span class=md-ellipsis> OpenAI </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/mcp/ class=md-nav__link> <span class=md-ellipsis> MCP </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/mistral/ class=md-nav__link> <span class=md-ellipsis> Mistral </span> </a> </li> <li class=md-nav__item> <a href=../../genAI/cohere/ class=md-nav__link> <span class=md-ellipsis> Cohere </span> </a> </li> <li class=md-nav__item> <a href=../../techno/watsonx/ class=md-nav__link> <span class=md-ellipsis> WatsonX.ai </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Techno </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Techno </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../techno/airflow/ class=md-nav__link> <span class=md-ellipsis> Airflow </span> </a> </li> <li class=md-nav__item> <a href=../../techno/feature_store/ class=md-nav__link> <span class=md-ellipsis> Feature Store </span> </a> </li> <li class=md-nav__item> <a href=../../kaggle/ class=md-nav__link> <span class=md-ellipsis> Kaggle </span> </a> </li> <li class=md-nav__item> <a href=../../techno/opensearch/ class=md-nav__link> <span class=md-ellipsis> OpenSearch </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/python-code/ class=md-nav__link> <span class=md-ellipsis> Python studies </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/yarfba/ai-ml/sagemaker class=md-nav__link> <span class=md-ellipsis> SageMaker </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/spark-studies/ class=md-nav__link> <span class=md-ellipsis> Spark studies </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_8> <label class=md-nav__link for=__nav_5_8 id=__nav_5_8_label tabindex=0> <span class=md-ellipsis> UI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_8_label aria-expanded=false> <label class=md-nav__title for=__nav_5_8> <span class="md-nav__icon md-icon"></span> UI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../techno/gradio/ class=md-nav__link> <span class=md-ellipsis> Gradio </span> </a> </li> <li class=md-nav__item> <a href=../../techno/streamlit/ class=md-nav__link> <span class=md-ellipsis> Streamlit </span> </a> </li> <li class=md-nav__item> <a href=../../techno/taipy/ class=md-nav__link> <span class=md-ellipsis> TaiPy </span> </a> </li> <li class=md-nav__item> <a href=../../techno/nicegui/ class=md-nav__link> <span class=md-ellipsis> NiceGUI </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../techno/gcp/ class=md-nav__link> <span class=md-ellipsis> GCP </span> </a> </li> <li class=md-nav__item> <a href=../../techno/watsonx/ class=md-nav__link> <span class=md-ellipsis> WatsonX.ai </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6 checked> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex> <span class=md-ellipsis> Coding </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=true> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Coding </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ class=md-nav__link> <span class=md-ellipsis> Coding getting started </span> </a> </li> <li class=md-nav__item> <a href=../visualization/ class=md-nav__link> <span class=md-ellipsis> Data Visualization </span> </a> </li> <li class=md-nav__item> <a href=../haystack/ class=md-nav__link> <span class=md-ellipsis> Haystack.ai </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> LangChain </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> LangChain </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#value-propositions class=md-nav__link> <span class=md-ellipsis> Value propositions </span> </a> <nav class=md-nav aria-label="Value propositions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sources class=md-nav__link> <span class=md-ellipsis> Sources </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#langchain-libraries class=md-nav__link> <span class=md-ellipsis> LangChain libraries </span> </a> <nav class=md-nav aria-label="LangChain libraries"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#getting-started class=md-nav__link> <span class=md-ellipsis> Getting started </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#main-concepts class=md-nav__link> <span class=md-ellipsis> Main Concepts </span> </a> <nav class=md-nav aria-label="Main Concepts"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#model-io class=md-nav__link> <span class=md-ellipsis> Model I/O </span> </a> </li> <li class=md-nav__item> <a href=#chain class=md-nav__link> <span class=md-ellipsis> Chain </span> </a> </li> <li class=md-nav__item> <a href=#runnable class=md-nav__link> <span class=md-ellipsis> Runnable </span> </a> </li> <li class=md-nav__item> <a href=#memory class=md-nav__link> <span class=md-ellipsis> Memory </span> </a> </li> <li class=md-nav__item> <a href=#retrieval-augmented-generation class=md-nav__link> <span class=md-ellipsis> Retrieval Augmented Generation </span> </a> </li> <li class=md-nav__item> <a href=#qa-app class=md-nav__link> <span class=md-ellipsis> Q&amp;A app </span> </a> </li> <li class=md-nav__item> <a href=#chatbot class=md-nav__link> <span class=md-ellipsis> ChatBot </span> </a> </li> <li class=md-nav__item> <a href=#text-generation-examples class=md-nav__link> <span class=md-ellipsis> Text Generation Examples </span> </a> </li> <li class=md-nav__item> <a href=#summarization-chain class=md-nav__link> <span class=md-ellipsis> Summarization chain </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-results class=md-nav__link> <span class=md-ellipsis> Evaluating results </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#agent class=md-nav__link> <span class=md-ellipsis> Agent </span> </a> <nav class=md-nav aria-label=Agent> <ul class=md-nav__list> <li class=md-nav__item> <a href=#tool-calling class=md-nav__link> <span class=md-ellipsis> Tool Calling </span> </a> </li> <li class=md-nav__item> <a href=#interesting-tools class=md-nav__link> <span class=md-ellipsis> Interesting tools </span> </a> </li> <li class=md-nav__item> <a href=#how-tos class=md-nav__link> <span class=md-ellipsis> How Tos </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#langchain-expression-language-lcel class=md-nav__link> <span class=md-ellipsis> LangChain Expression Language (LCEL) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../langgraph/ class=md-nav__link> <span class=md-ellipsis> LangGraph </span> </a> </li> <li class=md-nav__item> <a href=../llama-index/ class=md-nav__link> <span class=md-ellipsis> LlamaIndex </span> </a> </li> <li class=md-nav__item> <a href=../pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../pytorch/ class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../sklearn/ class=md-nav__link> <span class=md-ellipsis> Scikit-learn </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> <span class=md-ellipsis> Solutions </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Solutions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../solutions/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../techno/players_to_look/ class=md-nav__link> <span class=md-ellipsis> Key AI startups </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#value-propositions class=md-nav__link> <span class=md-ellipsis> Value propositions </span> </a> <nav class=md-nav aria-label="Value propositions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sources class=md-nav__link> <span class=md-ellipsis> Sources </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#langchain-libraries class=md-nav__link> <span class=md-ellipsis> LangChain libraries </span> </a> <nav class=md-nav aria-label="LangChain libraries"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#getting-started class=md-nav__link> <span class=md-ellipsis> Getting started </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#main-concepts class=md-nav__link> <span class=md-ellipsis> Main Concepts </span> </a> <nav class=md-nav aria-label="Main Concepts"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#model-io class=md-nav__link> <span class=md-ellipsis> Model I/O </span> </a> </li> <li class=md-nav__item> <a href=#chain class=md-nav__link> <span class=md-ellipsis> Chain </span> </a> </li> <li class=md-nav__item> <a href=#runnable class=md-nav__link> <span class=md-ellipsis> Runnable </span> </a> </li> <li class=md-nav__item> <a href=#memory class=md-nav__link> <span class=md-ellipsis> Memory </span> </a> </li> <li class=md-nav__item> <a href=#retrieval-augmented-generation class=md-nav__link> <span class=md-ellipsis> Retrieval Augmented Generation </span> </a> </li> <li class=md-nav__item> <a href=#qa-app class=md-nav__link> <span class=md-ellipsis> Q&amp;A app </span> </a> </li> <li class=md-nav__item> <a href=#chatbot class=md-nav__link> <span class=md-ellipsis> ChatBot </span> </a> </li> <li class=md-nav__item> <a href=#text-generation-examples class=md-nav__link> <span class=md-ellipsis> Text Generation Examples </span> </a> </li> <li class=md-nav__item> <a href=#summarization-chain class=md-nav__link> <span class=md-ellipsis> Summarization chain </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-results class=md-nav__link> <span class=md-ellipsis> Evaluating results </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#agent class=md-nav__link> <span class=md-ellipsis> Agent </span> </a> <nav class=md-nav aria-label=Agent> <ul class=md-nav__list> <li class=md-nav__item> <a href=#tool-calling class=md-nav__link> <span class=md-ellipsis> Tool Calling </span> </a> </li> <li class=md-nav__item> <a href=#interesting-tools class=md-nav__link> <span class=md-ellipsis> Interesting tools </span> </a> </li> <li class=md-nav__item> <a href=#how-tos class=md-nav__link> <span class=md-ellipsis> How Tos </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#langchain-expression-language-lcel class=md-nav__link> <span class=md-ellipsis> LangChain Expression Language (LCEL) </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=langchain-study>LangChain Study<a class=headerlink href=#langchain-study title="Permanent link">&para;</a></h1> <p>In LLM application there are a lot of steps to do, trying different prompting, integrating different LLMs, implementing conversation history, at the end there is a lot of glue code to implement.</p> <p><a href=https://python.langchain.com/docs/get_started/introduction>LangChain</a> is a open-source framework for developing applications powered by large language models, connecting them to external data sources, and manage conversation with human. </p> <h2 id=value-propositions>Value propositions<a class=headerlink href=#value-propositions title="Permanent link">&para;</a></h2> <p>Develop apps with context awareness, and that can reason using LLMs. It includes Python and Typescript packages, and a Java one under construction.</p> <p>It focuses on composition and modularity. The components defined by the framework can be combined to address specific use cases, and developers can add new components.</p> <ul> <li><strong>LangChain</strong>: Python and Javascript libraries</li> <li><strong>LangServe:</strong> a library for deploying LangChain chains as a REST API.</li> <li><strong>LangSmith:</strong> a platform that lets developers debug, test, evaluate, and monitor chains</li> <li>Predefined prompt template from langChain Hub.</li> </ul> <p>They are adding new products to their portfolio quickly like LangSmith (get visibility on LLMs execution), and LangServe (server API for LangChain apps).</p> <h3 id=sources>Sources<a class=headerlink href=#sources title="Permanent link">&para;</a></h3> <p>The content comes from different sources:</p> <ul> <li><a href=https://python.langchain.com/docs/ >Excellent product documentation</a>, should be the go to place.</li> <li><a href=https://learn.deeplearning.ai>deeplearning.ai LangChain introduction by Harisson Chase and Andrew Ng</a></li> <li><a href=https://lilianweng.github.io/posts/2023-06-23-agent/ >LLM Powered Autonomous Agents</a></li> <li><a href=https://blog.langchain.dev/retrieval/ >Retrieval and RAG blog.</a></li> </ul> <h2 id=langchain-libraries>LangChain libraries<a class=headerlink href=#langchain-libraries title="Permanent link">&para;</a></h2> <p>The core building block of LangChain applications is the LLMChain:</p> <ul> <li>A LLM</li> <li>Prompt templates</li> <li>Output parsers</li> </ul> <p><a href=https://python.langchain.com/docs/modules/model_io/prompts>PromptTemplate</a> helps to structure the prompt and facilitate reuse by creating model agnostic templates. The library includes output parsers to get content extracted from the keyword defined in the prompt. Example is the chain of thought keywords of <strong>Thought, Action, Observation.</strong></p> <h3 id=getting-started>Getting started<a class=headerlink href=#getting-started title="Permanent link">&para;</a></h3> <p>The <a href=https://python.langchain.com/docs/get_started/quickstart/ >LangChain documentation</a> is excellent so no need to write more. All my study codes with LangChain and LLM are in different folders of this repo:</p> <table> <thead> <tr> <th>Backend</th> <th>Type of chains</th> </tr> </thead> <tbody> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/openAI>openAI</a></td> <td>The implementation of the quickstart examples, RAG, chatbot, agent</td> </tr> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/ollama>Ollama</a></td> <td>run a simple query to Ollama (running Llama 3.2) locally</td> </tr> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/anthropic>Anthropic Claude</a></td> <td></td> </tr> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/mistral>Mistral LLM</a></td> <td></td> </tr> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/watsonX>IBM WatsonX</a></td> <td></td> </tr> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/bedrock>AWS Bedrock</a></td> <td>zero_shot generation</td> </tr> </tbody> </table> <p>Each code needs to define only the needed LangChain modules to keep the executable size low. </p> <h2 id=main-concepts>Main Concepts<a class=headerlink href=#main-concepts title="Permanent link">&para;</a></h2> <h3 id=model-io>Model I/O<a class=headerlink href=#model-io title="Permanent link">&para;</a></h3> <p><a href=https://python.langchain.com/docs/modules/model_io/ >Model I/O</a> are building blocks to interface with any language model. It facilitates the interface of model input (prompts) with the LLM model to produce the model output.</p> <ul> <li>LangChain supports two types of language: LLM (for pure text completion models) or ChatModel (conversation on top of LLM using constructs of AIMessage, HumanMessage)</li> <li> <p>LangChain uses <a href=https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/ >Prompt templates</a> to control LLM behavior.</p> <ul> <li>Two common prompt templates: <a href=https://api.python.langchain.com/en/latest/prompts/langchain.prompts.base.StringPromptTemplate.html>string prompt</a> templates and <a href=https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html>chat prompt</a> templates.</li> </ul> <p><div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>langchain_core.prompts</span><span class=w> </span><span class=kn>import</span> <span class=n>ChatPromptTemplate</span><span class=p>,</span> <span class=n>MessagesPlaceholder</span>
<span class=n>prompt</span> <span class=o>=</span> <span class=n>ChatPromptTemplate</span><span class=o>.</span><span class=n>from_messages</span><span class=p>([</span>
    <span class=p>(</span><span class=s2>&quot;system&quot;</span><span class=p>,</span> <span class=s2>&quot;Answer the user&#39;s questions based on the below context:</span><span class=se>\n\n</span><span class=si>{context}</span><span class=s2>&quot;</span><span class=p>),</span>
    <span class=n>MessagesPlaceholder</span><span class=p>(</span><span class=n>variable_name</span><span class=o>=</span><span class=s2>&quot;chat_history&quot;</span><span class=p>),</span>
    <span class=p>(</span><span class=s2>&quot;user&quot;</span><span class=p>,</span> <span class=s2>&quot;</span><span class=si>{input}</span><span class=s2>&quot;</span><span class=p>),</span>
<span class=p>])</span>
</code></pre></div> * We can build custom prompt by extending existing default templates. An example is a 'few-shot-examples' in a chat prompt using <a href=https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples_chat>FewShotChatMessagePromptTemplate</a>. * LangChain offers a <a href=https://smith.langchain.com/hub>prompt hub</a> to get predefined prompts easily loadable:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>langchain</span><span class=w> </span><span class=kn>import</span> <span class=n>hub</span>
<span class=n>prompt</span> <span class=o>=</span> <span class=n>hub</span><span class=o>.</span><span class=n>pull</span><span class=p>(</span><span class=s2>&quot;hwchase17/openai-functions-agent&quot;</span><span class=p>)</span>
</code></pre></div> </li> <li> <p><a href=https://python.langchain.com/docs/modules/chains/ >Chains</a> allow developers to combine multiple components together (or to combine other chains) to create a single, coherent application. </p> </li> <li> <p><strong>OutputParsers</strong> convert the raw output of a language model into a format that can be used downstream</p> </li> </ul> <p><em>Feature stores, like <a href=https://github.com/feast-dev/feast>Feast</a>, can be a great way to keep information about the user conversation or query, and LangChain provides an easy way to combine data from Feast with LLMs.</em></p> <h3 id=chain>Chain<a class=headerlink href=#chain title="Permanent link">&para;</a></h3> <p>Chains are <a href=#runnable>runnable</a>, observable and composable. The LangChain framework uses the Runnable class to encapsulate operations that can be run synchronously or asynchronously. </p> <ul> <li> <p><a href=https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html>LLMChain</a> class is the basic chain to integrate with any LLM.</p> <div class=highlight><pre><span></span><code><span class=c1># Basic chain</span>
<span class=n>chain</span> <span class=o>=</span> <span class=n>LLMChain</span><span class=p>(</span><span class=n>llm</span><span class=o>=</span><span class=n>model</span><span class=p>,</span> <span class=n>prompt</span> <span class=o>=</span> <span class=n>prompt</span><span class=p>)</span>
<span class=n>chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span><span class=s2>&quot;a query&quot;</span><span class=p>)</span>
</code></pre></div> </li> <li> <p>Sequential chain combines chains in sequence with single input and output (SimpleSequentialChain)</p> <div class=highlight><pre><span></span><code><span class=n>overall_simple_chain</span> <span class=o>=</span> <span class=n>SimpleSequentialChain</span><span class=p>(</span><span class=n>chains</span><span class=o>=</span><span class=p>[</span><span class=n>chain_one</span><span class=p>,</span> <span class=n>chain_two</span><span class=p>],</span>
                                            <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span>
                                            <span class=p>)</span>
</code></pre></div> <p>or multiple inputs and outputs with prompt using the different environment variables (see <a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/multi_chains.py>this code</a>).</p> </li> <li> <p><a href=https://api.python.langchain.com/en/latest/chains/langchain.chains.router.llm_router.LLMRouterChain.html#langchain.chains.router.llm_router.LLMRouterChain>LLMRouterChain</a> is a chain that outputs the name of a destination chain and the inputs to it.</p> </li> <li> <p><a href=https://python.langchain.com/docs/expression_language/ >LangChain Expression Language</a> is a declarative way to define chains. It looks similar to Unix shell pipe: input for one runnable comes from the output of predecessor (This is why prompt below is a runnable). </p> <div class=highlight><pre><span></span><code><span class=c1># a chain definition using Langchain expression language</span>
<span class=n>chain</span> <span class=o>=</span> <span class=n>prompt</span> <span class=o>|</span> <span class=n>model</span> <span class=o>|</span> <span class=n>output_parser</span>
</code></pre></div> </li> <li> <p>Chain can be executed asynchronously in its own Thread using the <code>ainvoke</code> method.</p> </li> </ul> <h3 id=runnable>Runnable<a class=headerlink href=#runnable title="Permanent link">&para;</a></h3> <p><a href=https://python.langchain.com/v0.1/docs/expression_language/interface/ >Runnable interface</a> is a protocol to define custom chains and invoke them. Each Runnable exposes methods to get input, output and config schemas. Each implements synchronous and async invoke methods and batch. Runnable can run in parallel or in sequence.</p> <p>To pass data to a Runnable there is the <code>RunnablePassthrough</code> class. This is used in conjunction with RunnableParallel to assign data to key in a map.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>langchain.schema.runnable</span><span class=w> </span><span class=kn>import</span> <span class=n>RunnableParallel</span><span class=p>,</span> <span class=n>RunnablePassthrough</span>

<span class=n>runnable</span> <span class=o>=</span> <span class=n>RunnableParallel</span><span class=p>(</span>
   <span class=n>passed</span> <span class=o>=</span> <span class=n>RunnablePassthrough</span><span class=p>(),</span>
   <span class=n>extra</span><span class=o>=</span> <span class=n>RunnablePassthrough</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=n>mult</span><span class=o>=</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span><span class=n>x</span><span class=p>[</span><span class=s2>&quot;num&quot;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>3</span><span class=p>),</span>
   <span class=n>modified</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span><span class=n>x</span><span class=p>[</span><span class=s2>&quot;num&quot;</span><span class=p>]</span> <span class=o>+</span><span class=mi>1</span>   
<span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=n>runnable</span><span class=o>.</span><span class=n>invoke</span><span class=p>({</span><span class=s2>&quot;num&quot;</span><span class=p>:</span> <span class=mi>6</span><span class=p>}))</span>
<span class=p>{</span><span class=s1>&#39;passed&#39;</span><span class=p>:</span> <span class=p>{</span><span class=s1>&#39;num&#39;</span><span class=p>:</span> <span class=mi>6</span><span class=p>},</span> <span class=s1>&#39;extra&#39;</span><span class=p>:</span> <span class=p>{</span><span class=s1>&#39;num&#39;</span><span class=p>:</span> <span class=mi>6</span><span class=p>,</span> <span class=s1>&#39;mult&#39;</span><span class=p>:</span> <span class=mi>18</span><span class=p>},</span> <span class=s1>&#39;modified&#39;</span><span class=p>:</span> <span class=mi>7</span><span class=p>}</span>
</code></pre></div> <ul> <li><a href=https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/base.py#L3572>RunnableLambda</a> is a type of Runnable that wraps a callable function. </li> </ul> <div class=highlight><pre><span></span><code><span class=n>sequence</span> <span class=o>=</span> <span class=n>RunnableLambda</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>|</span> <span class=p>{</span>
    <span class=s1>&#39;mul_2&#39;</span><span class=p>:</span> <span class=n>RunnableLambda</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>*</span> <span class=mi>2</span><span class=p>),</span>
    <span class=s1>&#39;mul_5&#39;</span><span class=p>:</span> <span class=n>RunnableLambda</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>*</span> <span class=mi>5</span><span class=p>)</span>
<span class=p>}</span>
<span class=n>sequence</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</code></pre></div> <p>The <a href=https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/passthrough.py>RunnablePassthrough.assign</a> method is used to create a Runnable that passes the input through while adding some keys to the output.</p> <p>We can use <code>Runnable.bind()</code> to pass arguments as constants accessible within a runnable sequence (a chain) where argument is not part of the output of preceding runnables in the sequence.</p> <p>See some code <a href=https://github.com/jbcodeforce/ML-studies/tree/master/e2e-demos/ollama-mistral/RunnableExamples.py>RunnableExamples</a></p> <h3 id=memory>Memory<a class=headerlink href=#memory title="Permanent link">&para;</a></h3> <p>Large Language Models are stateless and do not remember anything. Chatbot seems to have memory, because conversation is kept in the context. </p> <p>With a simple conversation like the following code, the conversation is added as string into the context:</p> <div class=highlight><pre><span></span><code><span class=n>llm</span> <span class=o>=</span> <span class=n>ChatOpenAI</span><span class=p>(</span><span class=n>temperature</span> <span class=o>=</span> <span class=mi>0</span><span class=p>)</span>
<span class=n>memory</span> <span class=o>=</span> <span class=n>ConversationBufferMemory</span><span class=p>()</span>
<span class=n>conversation</span> <span class=o>=</span> <span class=n>ConversationChain</span><span class=p>(</span>
    <span class=n>llm</span><span class=o>=</span> <span class=n>llm</span><span class=p>,</span>
    <span class=n>memory</span><span class=o>=</span><span class=n>memory</span><span class=p>,</span>
    <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span>   <span class=c1># trace the chain</span>
<span class=p>)</span>
</code></pre></div> <p>The memory is just a container in which we can save {"input:""} and {"output": ""} content.</p> <p>But as the conversation goes, the size of the context grows, and so the cost of operating this chatbot, as API are charged by the size of the token. Using <code>ConversationBufferWindowMemory(k=1)</code> with a k necessary to keep enough context, we can limit cost. Same with <code>ConversationTokenBufferMemory</code> to limit the token in memory.</p> <p><a href=https://python.langchain.com/docs/modules/memory/conversational_customization/ >ConversationChain</a> is a predefined chain to have a conversation and load context from memory.</p> <p>As part of memory component there is the <a href=https://python.langchain.com/docs/modules/memory/types/summary/ >ConversationSummaryMemory</a> to get the conversation summary so far.</p> <p>The other important memory is <a href=https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory/ >Vector Data memory</a> and entity memory or <a href=https://python.langchain.com/docs/modules/memory/types/kg/ >knowledgeGraph</a></p> <p>See related code <a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/conversation_with_memory.py>conversation_with_memory.py</a></p> <h3 id=retrieval-augmented-generation>Retrieval Augmented Generation<a class=headerlink href=#retrieval-augmented-generation title="Permanent link">&para;</a></h3> <p>The goal for <strong>Retrieval Augmented Generation</strong> (RAG) is to add custom dataset not already part of a trained model and use the dataset as input sent to the LLM. RAG is illustrated in figure below:</p> <p><img alt src=../diagrams/rag-process.drawio.png></p> <p>Embed is the vector representation of a chunk of text. Different embedding can be used.</p> <details class=code open=open> <summary>Embeddings</summary> <p>The classical Embedding is the <a href=https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.openai.OpenAIEmbeddings.htm>OpenAIEmbeddings</a> but Hugging Face offers an open source version: the <a href><strong>SentenceTransformers</strong></a>https://huggingface.co/sentence-transformers which is a Python framework for state-of-the-art sentence, text and image embeddings.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>langchain_openai</span><span class=w> </span><span class=kn>import</span> <span class=n>OpenAIEmbeddings</span>
<span class=n>vectorstore</span> <span class=o>=</span>  <span class=n>Chroma</span><span class=o>.</span><span class=n>from_documents</span><span class=p>(</span><span class=n>documents</span><span class=o>=</span><span class=n>splits</span><span class=p>,</span> <span class=n>embedding</span><span class=o>=</span><span class=n>OpenAIEmbeddings</span><span class=p>(),</span>
                                     <span class=n>persist_directory</span><span class=o>=</span><span class=n>DOMAIN_VS_PATH</span><span class=p>)</span>
<span class=c1># With HuggingFace</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sentence_transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>SentenceTransformer</span>
<span class=k>def</span><span class=w> </span><span class=nf>build_embedding</span><span class=p>(</span><span class=n>docs</span><span class=p>):</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>SentenceTransformer</span><span class=p>(</span><span class=s2>&quot;all-MiniLM-L6-v2&quot;</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>docs</span><span class=p>)</span>
<span class=c1># With AWS embedding</span>
<span class=kn>from</span><span class=w> </span><span class=nn>langchain.embeddings</span><span class=w> </span><span class=kn>import</span> <span class=n>BedrockEmbeddings</span>
</code></pre></div> </details> <p>Different code that implement RAG</p> <table> <thead> <tr> <th>Code</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/blob/master/RAG/build_agent_domain_rag.py/ >build_agent_domain_rag.py</a></td> <td>Read Lilian Weng blog and create a ChromeDB vector store with OpenAIEmbeddings</td> </tr> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/blob/master/RAG/query_agent_domain_store.py/ >query_agent_domain_store.py</a></td> <td>Query the persisted vector store for similarity search</td> </tr> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/Q&A/prepareVectorStore.py>prepareVectorStore.py</a></td> <td>Use AWS Bedrock Embeddings</td> </tr> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/blob/master/RAG/embeddings_hf.py>embeddings_hf.py</a></td> <td>Use Hunggingface embeddings with splitting a markdown file and use FAISS vector store</td> </tr> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/blob/master/RAG/rag_HyDE.py>rag_HyDE.py</a></td> <td>Hypothetical Document Embedding (HyDE) the first prompt create an hypothetical document</td> </tr> </tbody> </table> <p>Creating chunks is necessary because language models generally have a limit to the amount of token they can deal with. It also improve the similarity search based on vector.</p> <details class="- code"> <summary>Split docs and save in vector store</summary> <p><a href=https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html>RecursiveCharacterTextSplitter</a> splits text by recursively look at characters. <code>text_splitter.split_documents(documents)</code> return a list of <a href=https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html>Document</a> which is a wrapper to page content and some metadata for the indexes from the source document.</p> <div class=highlight><pre><span></span><code><span class=c1># ...</span>
<span class=kn>from</span><span class=w> </span><span class=nn>langchain.text_splitter</span><span class=w> </span><span class=kn>import</span> <span class=n>RecursiveCharacterTextSplitter</span>
<span class=kn>from</span><span class=w> </span><span class=nn>langchain.vectorstores</span><span class=w> </span><span class=kn>import</span> <span class=n>FAISS</span>
<span class=kn>from</span><span class=w> </span><span class=nn>langchain.indexes.vectorstore</span><span class=w> </span><span class=kn>import</span> <span class=n>VectorStoreIndexWrapper</span>

<span class=n>loader</span> <span class=o>=</span> <span class=n>PyPDFDirectoryLoader</span><span class=p>(</span><span class=s2>&quot;./data/&quot;</span><span class=p>)</span>
<span class=n>documents</span> <span class=o>=</span> <span class=n>loader</span><span class=o>.</span><span class=n>load</span><span class=p>()</span>
<span class=n>text_splitter</span> <span class=o>=</span> <span class=n>RecursiveCharacterTextSplitter</span><span class=p>(</span>
    <span class=n>chunk_size</span> <span class=o>=</span> <span class=mi>1000</span><span class=p>,</span>
    <span class=n>chunk_overlap</span>  <span class=o>=</span> <span class=mi>100</span><span class=p>,</span>
<span class=p>)</span>
<span class=n>docs</span> <span class=o>=</span> <span class=n>text_splitter</span><span class=o>.</span><span class=n>split_documents</span><span class=p>(</span><span class=n>documents</span><span class=p>)</span>

<span class=n>vectorstore_faiss</span> <span class=o>=</span> <span class=n>FAISS</span><span class=o>.</span><span class=n>from_documents</span><span class=p>(</span>
    <span class=n>docs</span><span class=p>,</span>
    <span class=n>embeddings</span><span class=p>,</span>
<span class=p>)</span>
<span class=n>vectorstore_faiss</span><span class=o>.</span><span class=n>save_local</span><span class=p>(</span><span class=s2>&quot;faiss_index&quot;</span><span class=p>)</span>
</code></pre></div> </details> <details class="- code"> <summary>Search similarity in vector DB</summary> <p><a href=https://python.langchain.com/docs/integrations/text_embedding/openai/ >OpenAIEmbeddings</a> <div class=highlight><pre><span></span><code><span class=n>embeddings</span> <span class=o>=</span> <span class=n>OpenAIEmbeddings</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&quot;text-embedding-3-large&quot;</span><span class=p>,</span> <span class=n>dimensions</span><span class=o>=</span><span class=mi>1024</span><span class=p>)</span>
<span class=n>query</span> <span class=o>=</span> <span class=s2>&quot;&quot;&quot;Is it possible that ...?&quot;&quot;&quot;</span>
<span class=n>query_embedding</span> <span class=o>=</span> <span class=n>embeddings</span><span class=o>.</span><span class=n>embed_query</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>
<span class=n>relevant_documents</span> <span class=o>=</span> <span class=n>vectorstore_faiss</span><span class=o>.</span><span class=n>similarity_search_by_vector</span><span class=p>(</span><span class=n>query_embedding</span><span class=p>)</span>
</code></pre></div></p> </details> <p>During the interaction with the end-user, the system (a chain in LangChain) retrieves the most relevant data to the question asked, and passes it to LLM in the generation step.</p> <ul> <li>Embeddings capture the semantic meaning of the text to help do similarity search</li> <li>Persist the embeddings into a Vector store. Faiss and ChromaDB are common vector stores to use, but OpenSearch, Postgresql can also being used.</li> <li>Retriever includes semantic search and efficient algorithm to prepare the prompt. To improve on vector similarity search we can generate variants of the input question.</li> </ul> <p>See <a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/Q&A/qa-faiss-store.py>Q&amp;A with FAISS store qa-faiss-store.py</a>.</p> <ul> <li> <p><a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/feast/feast-prompt.py>Another example of LLM Chain with AWS Bedrock llm and Feast as feature store</a></p> </li> <li> <p><strong><a href=https://python.langchain.com/docs/use_cases/web_scraping>Web scraping</a></strong> for LLM based web research. It uses the same process: document/page loading, transformation with tool like BeautifulSoup, to HTML2Text.</p> </li> </ul> <details class="- info"> <summary>Getting started with Feast</summary> <p>Use <code>pip install feast</code> then the <code>feast</code> CLI with <code>feast init my_feature_repo</code> to create a Feature Store then <code>feast apply</code> to create entity, feature views, and services. Then <code>feast ui</code> + <a href=http://localhost:8888>http://localhost:8888</a> to act on the store. </p> </details> <details class="- info"> <summary>LLM and FeatureForm</summary> <p>See <a href=https://docs.featureform.com/ >FeatureForm</a> as another open-source feature store solution and the LangChain sample with <a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/featureform/ff-langchain-prompt.py>Claude LLM</a></p> </details> <h3 id=qa-app>Q&amp;A app<a class=headerlink href=#qa-app title="Permanent link">&para;</a></h3> <p>For <strong>Q&amp;A</strong> the pipeline will most likely integrate with existing documents as illustrated in the figure below:</p> <p><img alt src=../diagrams/lg-pipeline.drawio.png width=700></p> <p><strong>Embeddings</strong> capture the semantic meaning of the text, which helps to do similarity search. <strong>Vector store</strong> supports storage and searching of these embeddings. Retrievers use <a href=https://python.langchain.com/docs/modules/data_connection/retrievers/ >different algorithms</a> for the semantic search to load vectors. </p> <details class="- code"> <summary>Use RAG with Q&amp;A</summary> <p><a href>chains.RetrievalQA</a></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>langchain.chains</span><span class=w> </span><span class=kn>import</span> <span class=n>RetrievalQA</span>
<span class=kn>from</span><span class=w> </span><span class=nn>langchain.prompts</span><span class=w> </span><span class=kn>import</span> <span class=n>PromptTemplate</span>

<span class=n>prompt_template</span> <span class=o>=</span> <span class=s2>&quot;&quot;&quot;Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don&#39;t know the answer, just say that you don&#39;t know, don&#39;t try to make up an answer.</span>

<span class=si>{context}</span>

<span class=s2>Question: </span><span class=si>{question}</span>
<span class=s2>Assistant:&quot;&quot;&quot;</span>

<span class=n>PROMPT</span> <span class=o>=</span> <span class=n>PromptTemplate</span><span class=p>(</span>
    <span class=n>template</span><span class=o>=</span><span class=n>prompt_template</span><span class=p>,</span> <span class=n>input_variables</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;context&quot;</span><span class=p>,</span> <span class=s2>&quot;question&quot;</span><span class=p>]</span>
<span class=p>)</span>

<span class=n>qa</span> <span class=o>=</span> <span class=n>RetrievalQA</span><span class=o>.</span><span class=n>from_chain_type</span><span class=p>(</span>
    <span class=n>llm</span><span class=o>=</span><span class=n>llm</span><span class=p>,</span>
    <span class=n>chain_type</span><span class=o>=</span><span class=s2>&quot;stuff&quot;</span><span class=p>,</span>
    <span class=n>retriever</span><span class=o>=</span><span class=n>vectorstore_faiss</span><span class=o>.</span><span class=n>as_retriever</span><span class=p>(</span>
        <span class=n>search_type</span><span class=o>=</span><span class=s2>&quot;similarity&quot;</span><span class=p>,</span> <span class=n>search_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&quot;k&quot;</span><span class=p>:</span> <span class=mi>3</span><span class=p>}</span>
    <span class=p>),</span>
    <span class=n>return_source_documents</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>chain_type_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&quot;prompt&quot;</span><span class=p>:</span> <span class=n>PROMPT</span><span class=p>}</span>
<span class=p>)</span>
<span class=n>query</span> <span class=o>=</span> <span class=s2>&quot;Is it possible that I get sentenced to jail due to failure in filings?&quot;</span>
<span class=n>result</span> <span class=o>=</span> <span class=n>qa</span><span class=p>({</span><span class=s2>&quot;query&quot;</span><span class=p>:</span> <span class=n>query</span><span class=p>})</span>
<span class=n>print_ww</span><span class=p>(</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;result&#39;</span><span class=p>])</span>
</code></pre></div> </details> <h3 id=chatbot>ChatBot<a class=headerlink href=#chatbot title="Permanent link">&para;</a></h3> <p><strong><a href=https://python.langchain.com/docs/use_cases/chatbots/ >Chatbots</a></strong> is the most common app for LLM: Aside from basic prompting and LLMs call, chatbots have <strong>memory</strong> and retrievers:</p> <p><img alt src=../diagrams/chatbot.drawio.png></p> <h3 id=text-generation-examples>Text Generation Examples<a class=headerlink href=#text-generation-examples title="Permanent link">&para;</a></h3> <ul> <li><a href=ttps://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/openAI/1st_openAI_lc.py>Basic query with unknown content to generate hallucination: 1st_openAI_lc.py </a></li> <li><a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/bedrock/TestBedrockWithLangchain.py>Simple test to call Bedrock with Langchain</a> using on zero_shot generation.</li> <li>Response to an email of unhappy customer using Claude 2 and PromptTemplate. <code>PromptTemplates</code> allow us to create generic shells which can be populated with information and get model outputs based on different scenarios. See the <a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/text_generation/ResponseToUnhappyCustomer.py>text_generation/ResponseToUnhappyCustomer.py code.</a></li> </ul> <h3 id=summarization-chain>Summarization chain<a class=headerlink href=#summarization-chain title="Permanent link">&para;</a></h3> <p>Always assess the size of the content to send, as the approach can be different: for big document, we need to split the doc in chunks.</p> <ul> <li>Small text summary with OpenAI. </li> <li>Small text to summarize, with <a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/bedrock/utils/bedrock.py>bedrock client</a> and the invoke_model on the client see the code in <a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/summarization/SmallTextSummarization.py>llm-langchain/summarization/SmallTextSummarization.py</a></li> <li>For big document, langchain provides the load_summarize_chain to summarize by chunks and get the summary of the summaries. See code with 'manual' extraction of the summaries as insights and then creating a summary of insights in <a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/bedrock/summarization/long-text-summarization.py>summarization/long-text-summarization.py</a> or using a LangChain summarization with map-reduce in <a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/bedrock/summarization/long-text-summarization-mr.py>summarization/long-text-summarization-mr.py</a>.</li> </ul> <details class="- code"> <summary>Using langchain summarize chain</summary> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>langchain.text_splitter</span><span class=w> </span><span class=kn>import</span> <span class=n>RecursiveCharacterTextSplitter</span>
<span class=kn>from</span><span class=w> </span><span class=nn>langchain.llms.bedrock</span><span class=w> </span><span class=kn>import</span> <span class=n>Bedrock</span>
<span class=kn>from</span><span class=w> </span><span class=nn>langchain.chains.summarize</span><span class=w> </span><span class=kn>import</span> <span class=n>load_summarize_chain</span>

<span class=n>llm</span> <span class=o>=</span> <span class=n>Bedrock</span><span class=p>(</span>
    <span class=n>model_id</span><span class=o>=</span><span class=n>modelId</span><span class=p>,</span>
    <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span>
        <span class=s2>&quot;max_tokens_to_sample&quot;</span><span class=p>:</span> <span class=mi>1000</span><span class=p>,</span>
    <span class=p>},</span>
    <span class=n>client</span><span class=o>=</span><span class=n>boto3_bedrock</span><span class=p>,</span>
<span class=p>)</span> 

<span class=n>text_splitter</span> <span class=o>=</span> <span class=n>RecursiveCharacterTextSplitter</span><span class=p>(</span>
    <span class=n>separators</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;</span><span class=se>\n\n</span><span class=s2>&quot;</span><span class=p>,</span> <span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span><span class=p>],</span> <span class=n>chunk_size</span><span class=o>=</span><span class=mi>4000</span><span class=p>,</span> <span class=n>chunk_overlap</span><span class=o>=</span><span class=mi>100</span>
<span class=p>)</span>
<span class=n>docs</span> <span class=o>=</span> <span class=n>text_splitter</span><span class=o>.</span><span class=n>create_documents</span><span class=p>([</span><span class=n>letter</span><span class=p>])</span>

<span class=n>summary_chain</span> <span class=o>=</span> <span class=n>load_summarize_chain</span><span class=p>(</span><span class=n>llm</span><span class=o>=</span><span class=n>llm</span><span class=p>,</span> <span class=n>chain_type</span><span class=o>=</span><span class=s2>&quot;map_reduce&quot;</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>output</span> <span class=o>=</span> <span class=n>summary_chain</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>docs</span><span class=p>)</span>
</code></pre></div> </details> <h3 id=evaluating-results>Evaluating results<a class=headerlink href=#evaluating-results title="Permanent link">&para;</a></h3> <p>The evaluation of a chain, we need to define the data points to be measured. Building Questions and accepted answers is a classical approach.</p> <p>We can use LLM and a special chain (<a href=https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.generate_chain.QAGenerateChain.html>QAGenerateChain</a>) to build Q&amp;A from a document.</p> <h2 id=agent>Agent<a class=headerlink href=#agent title="Permanent link">&para;</a></h2> <p><a href=https://python.langchain.com/v0.2/docs/concepts/#agents>Agent</a> is an orchestrator pattern where the LLM decides what actions to take from the current query and context. With chain, developer code the sequence of tasks, with agent the LLM decides. <a href=../langgraph/ >LangGraph</a> is an extension of LangChain specifically aimed at creating highly controllable and customizable agents.</p> <p><strong>Chains</strong> let create a pre-defined sequence of tool usage(s), while <strong>Agents</strong> let the model uses tools in a loop, so that it can decide how many times to use its defined tools.</p> <details class=warning open=open> <summary>AgentExecutor is deprecated</summary> <p>Use LangGraph to implement agent.</p> <p>This content is then from v0.1</p> <p>There are <a href=https://python.langchain.com/docs/modules/agents/agent_types/ >different types</a> of agent: Intended Model, Supports Chat, Supports Multi-Input Tools, Supports Parallel Function Calling, Required Model Params.</p> <p>LangChain uses a specific <a href=https://python.langchain.com/docs/modules/agents/concepts/#schema>Schema model</a> to define: <strong>AgentAction</strong>, with tool and tool_input and <strong>AgentFinish</strong>.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>langchain.agents</span><span class=w> </span><span class=kn>import</span> <span class=n>create_tool_calling_agent</span>
<span class=kn>from</span><span class=w> </span><span class=nn>langchain.agents</span><span class=w> </span><span class=kn>import</span> <span class=n>AgentExecutor</span>
<span class=kn>from</span><span class=w> </span><span class=nn>langchain.tools.retriever</span><span class=w> </span><span class=kn>import</span> <span class=n>create_retriever_tool</span>
<span class=kn>from</span><span class=w> </span><span class=nn>langchain_community.tools.tavily_search</span><span class=w> </span><span class=kn>import</span> <span class=n>TavilySearchResults</span>

<span class=o>...</span>
<span class=n>tools</span> <span class=o>=</span> <span class=p>[</span><span class=n>retriever_tool</span><span class=p>,</span> <span class=n>search</span><span class=p>,</span> <span class=n>llm_math</span><span class=p>,</span> <span class=n>wikipedia</span><span class=p>]</span>

<span class=n>agent</span> <span class=o>=</span> <span class=n>create_tool_calling_agent</span><span class=p>(</span><span class=n>llm</span><span class=p>,</span> <span class=n>tools</span><span class=p>,</span> <span class=n>prompt</span><span class=p>)</span>
<span class=n>agent_executor</span> <span class=o>=</span> <span class=n>AgentExecutor</span><span class=p>(</span><span class=n>agent</span><span class=o>=</span><span class=n>agent</span><span class=p>,</span> <span class=n>tools</span><span class=o>=</span><span class=n>tools</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <ul> <li>To create agents use one of the constructor methods such as: <code>create_react_agent, create_json_agent, create_structured_chat_agent</code>, <a href=https://api.python.langchain.com/en/latest/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html#langchain-agents-tool-calling-agent-base-create-tool-calling-agent>create_tool_calling_agent</a> etc. Those methods return a Runnable.</li> <li>The <strong>Agent</strong> loops on user input until it returns <code>AgentFinish</code> action. If the Agent returns an <code>AgentAction</code>, then use that to call a tool and get an <code>Observation</code>. Agent has input and output and <code>intermediate steps</code>. AgentAction is a response that consists of action and action_input. </li> <li>See the existing predefined <a href=https://python.langchain.com/docs/modules/agents/agent_types/ >agent types</a>.</li> <li> <p><a href=https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.AgentExecutor.html>AgentExecutor</a> is the runtime for an agent.</p> </li> <li> <p><strong>Tools</strong> are functions that an agent can invoke. It defines the input schema for the tool and the function to run. Parameters of the tool should be sensibly named and described.</p> </li> </ul> </details> <h3 id=tool-calling>Tool Calling<a class=headerlink href=#tool-calling title="Permanent link">&para;</a></h3> <p>With Tool Calling we can define function or tool to be referenced as part of the LLM response, and LLM will prepare the arguments for the function. It is used to generate tool invocations, not to execute it. </p> <p>Tool calling allows a model to detect when one or more tools should be called and responds with the inputs that should be passed to those tools. The inputs match a defined schema. Below is an example structured answer from OpenAI LLM: "tool_calls" is the key to get the list of function names and arguments the orchestrator needs to call.</p> <div class=highlight><pre><span></span><code><span class=w>    </span><span class=nt>&quot;tool_calls&quot;</span><span class=p>:</span><span class=w> </span><span class=p>[</span>
<span class=w>            </span><span class=p>{</span>
<span class=w>            </span><span class=nt>&quot;name&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;tavily_search_results_json&quot;</span><span class=p>,</span>
<span class=w>            </span><span class=nt>&quot;args&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
<span class=w>                </span><span class=nt>&quot;query&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;weather in San Francisco&quot;</span>
<span class=w>            </span><span class=p>},</span>
<span class=w>            </span><span class=nt>&quot;id&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;call_Vg6JRaaz8d06OXbG5Gv7Ea5J&quot;</span>
<span class=w>            </span><span class=p>}</span>
</code></pre></div> <p>Prompt defines placeholders to get tools parameters. The following <a href=https://smith.langchain.com/hub/hwchase17/openai-tools-agent>langchain prompt</a> for OpenAI uses <code>agent_scratchpad</code> variable, which is a <code>MessagesPlaceholder</code>. Intermediate agent actions and tool output messages, will be passed in here. </p> <p>LangChain has a lot of <a href=https://python.langchain.com/docs/integrations/tools/ >predefined tool definitions to be reused</a>.</p> <p>We can use tool calling in chain (to use tools in sequence) or in <a href=https://python.langchain.com/docs/modules/agents/agent_types/tool_calling/ >agent</a> (to use tools in loop).</p> <p>LangChain offers an API to the LLM called <code>bind_tools</code> to pass the definition of the tool, as part of each call to the model, so that the application can invoke the tool when appropriate.</p> <p>See also <a href=https://api.python.langchain.com/en/latest/agents/langchain.agents.load_tools.load_tools.html#langchain.agents.load_tools.load_tools>the load tools api with a list of predefined tools</a>.</p> <p>Below is the classical application flow using tool calling. The exposed function wraps a remote microservice.</p> <p><img alt src=../diagrams/tool_calling.drawio.png></p> <p>When developing a solution based on agent, consider the tools, the services, the agent needs to access. See a code example <a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/openAI/openAI_agent.py>openAI_agent.py</a>.</p> <p>Many LLM providers support for tool calling, including Anthropic, Cohere, Google, Mistral, OpenAI, see the <a href=https://python.langchain.com/docs/integrations/tools/ >existing LangChain tools</a>.</p> <h3 id=interesting-tools>Interesting tools<a class=headerlink href=#interesting-tools title="Permanent link">&para;</a></h3> <h4 id=search-recent-news>Search recent news<a class=headerlink href=#search-recent-news title="Permanent link">&para;</a></h4> <p>A common tool integrated in agent, is the <a href=https://tavily.com/ >Tavily</a> search API, used to get the last trusted News, so the most recent information created after the cutoff date of the LLM.</p> <div class=highlight><pre><span></span><code><span class=n>retriever_tool</span> <span class=o>=</span> <span class=n>create_retriever_tool</span><span class=p>(</span>
    <span class=n>retriever</span><span class=p>,</span>
    <span class=s2>&quot;langsmith_search&quot;</span><span class=p>,</span>
    <span class=s2>&quot;Search for information about LangSmith. For any questions about LangSmith, you must use this tool!&quot;</span><span class=p>,</span>
<span class=p>)</span>
<span class=n>search</span> <span class=o>=</span> <span class=n>TavilySearchResults</span><span class=p>()</span>
<span class=n>tools</span> <span class=o>=</span> <span class=p>[</span><span class=n>retriever_tool</span><span class=p>,</span> <span class=n>search</span><span class=p>]</span>
</code></pre></div> <details class="- info"> <summary>Tavily</summary> <p><a href=https://docs.tavily.com/ >Tavily</a> is the leading search engine optimized for LLMs. It provides factual, explicit and objective answers. It is a GPT researcher which queries, filters and aggregates over 20+ web sources per a single research task. It focuses on optimizing search for AI developers and autonomous AI agents. See <a href=https://github.com/assafelovic/gpt-researcher.git>this git repo</a></p> </details> <h4 id=python-repltool>Python REPLtool<a class=headerlink href=#python-repltool title="Permanent link">&para;</a></h4> <p><a href=https://api.python.langchain.com/en/latest/tools/langchain_experimental.tools.python.tool.PythonREPLTool.html#langchain_experimental.tools.python.tool.PythonREPLTool>PythonREPLTool</a> is a tool for running python code in REPL (look like a jupiter notebook).</p> <h4 id=a-base-model>A base model<a class=headerlink href=#a-base-model title="Permanent link">&para;</a></h4> <p>It is possible to bind a BaseModel class as below, where a LLM is used to create prompt, so the prompt instruction entity is a json used as tool. (Tool definition are structured system prompt for the LLMs as they just understand text)</p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>PromptInstructions</span><span class=p>(</span><span class=n>BaseModel</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Instructions on how to prompt the LLM.&quot;&quot;&quot;</span>
    <span class=n>objective</span><span class=p>:</span> <span class=nb>str</span>
    <span class=n>variables</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span>
    <span class=n>constraints</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span>
    <span class=n>requirements</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span>

<span class=n>llm_with_tool</span> <span class=o>=</span> <span class=n>llm</span><span class=o>.</span><span class=n>bind_tools</span><span class=p>([</span><span class=n>PromptInstructions</span><span class=p>])</span>
</code></pre></div> <p><a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/langgraph/prompt_builder_graph.py>See the LangGraph sample: prompt_builder_graph.py</a>.</p> <h4 id=our-own-tools>Our own tools<a class=headerlink href=#our-own-tools title="Permanent link">&para;</a></h4> <p><a href=https://python.langchain.com/docs/modules/tools/custom_tools/ >Define custom tool</a> using the <code>@tool</code> annotation on a function to expose it as a tool. It uses the function name as the tool name and the functions docstring as the tools description. </p> <p>A second approach is to subclass the langchain.<code>pydantic_v1.BaseModel</code> class. </p> <p>Finally the last possible approach is to use <code>StructuredTool</code> dataclass. </p> <p>When doing agent we need to manage exception and implement handle_tool_error. </p> <p>To map the tools to OpenAI function call there is a module called: <code>from langchain_core.utils.function_calling import convert_to_openai_function</code>.</p> <p>It may be interesting to use embeddings to do tool selection before calling LLM. <a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/agent_wt_tool_retrieval.py>See this code agent_wt_tool_retrieval.py</a> The approach is to dynamically select the N tools we want at run time, without having to pass all the tool definitions within the context window. It uses a vector store to create embeddings for each tool description.</p> <h3 id=how-tos>How Tos<a class=headerlink href=#how-tos title="Permanent link">&para;</a></h3> <details class="- question"> <summary>How to trace the agent execution?</summary> <p><div class=highlight><pre><span></span><code>import langchain
langchain.debug = True
</code></pre></div> Or use LangSmith</p> </details> <details class="- question"> <summary>Defining an agent with tool calling, and the concept of scratchpad</summary> <p>Define an agent with 1/ a user input, 2/ a component for formatting intermediate steps (agent action, tool output pairs) (<code>format_to_openai_tool_messages</code>: convert (AgentAction, tool output) tuples into FunctionMessages), and 3/ a component for converting the output message into an agent action/agent finish:</p> <div class=highlight><pre><span></span><code><span class=c1># x is the response from LLM </span>
<span class=n>agent</span> <span class=o>=</span> <span class=p>(</span>
        <span class=p>{</span>
            <span class=s2>&quot;input&quot;</span><span class=p>:</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>[</span><span class=s2>&quot;input&quot;</span><span class=p>],</span>
            <span class=s2>&quot;agent_scratchpad&quot;</span><span class=p>:</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>format_to_openai_tool_messages</span><span class=p>(</span>
                <span class=n>x</span><span class=p>[</span><span class=s2>&quot;intermediate_steps&quot;</span><span class=p>]</span>
            <span class=p>),</span>
             <span class=s2>&quot;chat_history&quot;</span><span class=p>:</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>[</span><span class=s2>&quot;chat_history&quot;</span><span class=p>],</span>
        <span class=p>}</span>
        <span class=o>|</span> <span class=n>prompt</span>
        <span class=o>|</span> <span class=n>llm_with_tools</span>
        <span class=o>|</span> <span class=n>OpenAIToolsAgentOutputParser</span><span class=p>()</span>
    <span class=p>)</span>
</code></pre></div> <p><a href=https://api.python.langchain.com/en/latest/_modules/langchain/agents/output_parsers/openai_tools.html>OpenAIToolsAgentOutputParser</a> used with OpenAI models, as it relies on the specific tool_calls parameter from OpenAI to convey what tools to use.</p> </details> <details class="- question"> <summary>How to support streaming the LLM's output?</summary> <p><a href=https://python.langchain.com/docs/expression_language/streaming/ >LangChain streaming </a> is needed to make the app more responsive for end-users. All <a href=https://python.langchain.com/v0.1/docs/expression_language/interface/ >Runnable objects</a> implement a sync method called <code>stream</code> and an <code>async</code> variant called <code>astream</code>. They cut output into chunks and yield them. Recall yield is a generator of data and acts as <code>return</code>. The main demo code is <a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/web_server_wt_streaming.py>web_server_wt_streaming</a> with the client_stream.py</p> </details> <details class="- question"> <summary>Example of Intended Model</summary> <p>to be done</p> </details> <details class="- question"> <summary>Example of Supports Multi-Input Tools</summary> <p>to be done</p> </details> <details class="- question"> <summary>Use a vector store to keep the list of agent and description</summary> <p>As we cannot put the description of all the tools in the prompt (because of context length issues) so instead we dynamically select the N tools we do want to consider using, at run time. See the code in <a href=https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/agent_wt_tool_retrieval.py>agent_wt_tool_retrieval.py</a>. </p> </details> <h2 id=langchain-expression-language-lcel><a href=https://python.langchain.com/docs/expression_language>LangChain Expression Language (LCEL)</a><a class=headerlink href=#langchain-expression-language-lcel title="Permanent link">&para;</a></h2> <p>LCEL supports streaming the LLM results, use async communication, run in parallel, retries and fallbacks, access intermediate results.</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../haystack/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Haystack.ai"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Haystack.ai </div> </div> </a> <a href=../langgraph/ class="md-footer__link md-footer__link--next" aria-label="Next: LangGraph"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> LangGraph </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2018 - 2024 Jerome Boyer </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/jbcodeforce target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://linkedin.com/in/jeromeboyer target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M536.4-26.3c9.8-3.5 20.6-1 28 6.3s9.8 18.2 6.3 28l-178 496.9c-5 13.9-18.1 23.1-32.8 23.1-14.2 0-27-8.6-32.3-21.7l-64.2-158c-4.5-11-2.5-23.6 5.2-32.6l94.5-112.4c5.1-6.1 4.7-15-.9-20.6s-14.6-6-20.6-.9l-112.4 94.3c-9.1 7.6-21.6 9.6-32.6 5.2L38.1 216.8c-13.1-5.3-21.7-18.1-21.7-32.3 0-14.7 9.2-27.8 23.1-32.8z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "content.code.copy", "search.highlight", "navigation.instant", "navigation.instant.progress", "navigation.tabs", "navigation.tabs.sticky", "navigation.instant", "navigation.tracking", "navigation.top", "navigation.footer"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.79ae519e.min.js></script> </body> </html>