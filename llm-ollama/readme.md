# LLM with core API like Ollama or OpenAI API

This folder includes different examples using pure lower level APIs without langchain or langgraph.

## Chat with ollama API

The first example is to use terminal to chat with ollama mistral model. The code is chat_with_mistral.py.

## Asynch chat with ollama

## Chat with a llm running within Ollama using OpenAI SDK

```
uv run ollama_openai_chat.py
```

## Combine with vector store for RAG
