{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI and Machine Learning Studies","text":"<p>Update</p> <p>Created 2017 - Updated 8/05/2024</p> <p>Welcome to this repository for machine learning using Python and other cutting-edge technologies! Here, you will find a treasure trove of notes, code samples, and Jupyter notebooks, carefully curated from various reputable sources such as IBM labs, Google, AWS, Kaggle, Coursera, Udemy courses, books, and insightful websites.</p> <p>This becoming, almost, like a virtual book  </p> A Body of Knowledge about AI driven solutions <p>This web site contents articles, summaries, notebooks and python codes to validate and play with most of the concepts addressed here. </p>"},{"location":"#three-ai-flavors","title":"Three AI flavors","text":"<p>With the deployment of Generative AI, AI term needs to be more specifics, and all three flavors are useful to address a business problem:</p> <ol> <li>Symbolic AI: Expert System AI and knowledge graph to represent the human knowledge with rules and relationship semantic. For understanding business rule systems, I recommend reading Agile Business Rules Development book, from Hafedh Mili and J. Boyer with this summary of the methodology.</li> <li>Analytical AI: the machine learned model from data, analytical algorithms, used to solve analytical tasks such as classification, clustering, predictive scoring, or evaluation. The first neuron-network were used to support better classification on unstructured data like image, and text.</li> <li>Generative AI: generate new content (text, image, audio) from human existing large corpus of unstructured data. It uses deep learning, NLP, the transformer architecture, image recognition, voice recognition...</li> </ol>"},{"location":"#the-aiml-market","title":"The AI/ML Market","text":"<p>AI/ML by 2030 will be a $B300 market. Every company is using AI/ML already or consider using it in very short term. 2023 illustrated that AI is part of the world with the arrival of the Generative AI. Some on the main business drivers include:</p> <ul> <li>Make faster decisions by extracting and analyzing data from unstructured documents, voice, video records, transcripts...</li> <li>Generate and operationalize predictive and prescriptive insights to make decision at the right time.</li> <li>Create new content, ideas, conversations, stories, summaries, images, videos or music from question or suggestions (Generative AI).</li> <li>Code decision with inference rule to express domain and expert knowledge</li> <li>ML helps when developers do not have knowledge of all the rule conditions. Decision trees can be discovered from data.</li> <li>Deep learning can adapt to change in the environment and address new scenarios.</li> <li>Discover analytic insights within large data set.</li> </ul> <p>The stakeholders interested by AI/ML are CTOs, CPOs, Data Scientists, business analysts who want to derive decision from data and improve their business processes.</p>"},{"location":"#why-hybrid-cloud","title":"Why Hybrid Cloud?","text":"<p>Enterprises are using data as the main asset to derive empirical decisions and for that, they are adopting big data techniques which means high volume, high variation and high velocity.</p> <p>In most enterprise data are about customers' behaviors and come from different sources like click stream, shopping cart content,\u00a0transaction history, historical analytics, IoT sensors,...</p> <p>Big data need elastic storage and distributed, elastic computing resources. The cloud adoption is really driven by the access to elastic resources, pay as you go, with value-added managed services. </p> <p>Private data can stay on-premises servers close to the applications reading the data. Analytics data pre-processing can anonymize data and remove PII data, before uploading to cloud storage. Cloud Storages are used to store large datasets required for training machine learning models, taking advantage of the storage's scalability and performance. So an hybrid cloud strategy is key to support growing adoption of AI and ML solutions.</p> <p>Model trainings, run as batch processing, for few minutes to few days, and resources can then be released. It is less relevant to buy expensive hardware as CAPEX to do machine learning, when cloud computing can be used.</p> <p>Most Generative AI, LLMs are deployed as SaaS with API access.</p>"},{"location":"#data-science-major-concepts","title":"Data Science major concepts","text":"<p>There are three types of task, data scientists do: </p> <ol> <li>Preparing data to run a model (gathering, cleaning, integrating, transforming, filtering, combining, extracting, shaping...).</li> <li>Running the machine learning model, tuning it and assessing its quality.</li> <li>Communicate the results.</li> </ol> <p>With the adoption of Feature Store technologies, data scientists also prepare the features for reusability and governance.</p>"},{"location":"#analytics","title":"Analytics","text":"<p>The concept of statistical inference is to draw conclusions about a population from sample data using one of the two key methods:</p> <ul> <li>Hypothesis tests.</li> <li>Confidence intervals.</li> </ul> <p>But the truth wears off: previous analysis done on statistical data are less true overtime. Analytics need to be a continuous processing.</p>"},{"location":"#hypothesis-tests","title":"Hypothesis tests","text":"<p>The goal of hypothesis test is to compare an experimental group to a control group. There are two types of result:</p> <ul> <li>H0 for null hypothesis: this happens when there is no difference between the groups.</li> <li>Ha for alternative hypothesis: happens when there is statistically significant difference between the groups.</li> </ul> <p>The bigger the number of cases (named study size) the more statistical power we have, and better we are to get better results.</p> <p>We do not know if the difference in two treatments is not just due to chance. But we can calculate the odds that it is. Which is named the p-value.</p> <p>Statistics does not apply well to large-scale inference problems that big data brings. Big data is giving more spurious results than small data set.</p> <p>The curse of big data is the fact that when we search for patterns in very, very large data sets with billions or trillions of data points and thousands of metrics,  we are bound to identify coincidences that have no predictive power.</p>"},{"location":"#big-data-processing-with-map-reduce","title":"Big data processing with Map - Reduce","text":"<p>One of the classical approach to run analytics on big data is to use the map-reduce algorithm, which can be summarized as:</p> <ul> <li>Split the dataset into chunks and process each chunk on a different computer: chunk is typically 64Mb.</li> <li>Each chunk is replicated several times on different racks for fault tolerance.</li> <li>When processing a huge dataset, the first processing step is to read from distributed file systems and to split data into chunk files.</li> <li>Then a record reader reads records from files, then runs the <code>map</code> function which is customized for each different problem to solve.</li> <li>The combine operation identifies  with the same key and applies a combine function which should have the associative and commutative properties. <li>The output of map function are saved to local storage, then <code>reduce</code> task pulls the record per key from the local storage to sort the value and then call the last custom function: reduce</li> <p></p> <ul> <li>System architecture is based on shared nothing, in opposite of sharing file system or sharing memory approach.</li> <li>Massive parallelism on thousand of computers where jobs run for many hours. The % of failure of such job is high, so the algorithm should tolerate failure.</li> <li>For a given server, a mean time between failure is 1 year then for 10000 servers, we have a likelihood of failure around one failure / hour.</li> <li>Distributed FS: very large files TB and PB. Different implementations: Google FS or Hadoop DFS.</li> </ul> <p>Hadoop used to be the map-reduce platform, now Apache Spark is used for that or Apache Flink.</p> <p>Read my own  Sparck studies and Flink.</p>"},{"location":"#books-and-other-sources","title":"Books and other sources","text":"<p>Content is based of the following different sources:</p> <ul> <li>Python Machine learning - Sebastian Raschka's book.</li> <li>Collective intelligence - Toby Segaran's book.</li> <li>Stanford Machine learning training - Andrew Ng.</li> <li>Machine Learning University</li> <li>arxiv.org academic paper on science subjects</li> <li>Dive into deep learning book</li> <li>Amazon Sagemaker Python SDK</li> <li>Kaggle</li> <li>Papers with code - trends</li> <li>Introduction to Data Sciences - University of Washington.</li> <li>Jeff Heaton - Applications of Deep Neural Networks.</li> <li>Medium articles on generative AI, ML Analytics...</li> <li>poe.com to search content using LLM</li> <li>Made with ML from Goku Mohandas.</li> <li>Vision Transformer github from lucidrains</li> </ul>"},{"location":"anomaly/","title":"Anomaly detection","text":"<p>The goal for anomaly detection is, given a dataset <code>{x(1), ... x(m)}</code> and a Xtest dataset, the goal is to compute the probability that a X is anomalous: <code>P(X test) &lt; epsilon</code>.</p> <p>It is used in user behavioral, like fraud detection, or manufactoring test, or computer monitoring in data center, but it can be also used by data scientist when doing data analysis to verify the data quality and to understand why there are some outliers.</p> <p>In anomaly detection, we fit a model <code>P(X)</code> to a set of negative (y=0) examples, without using any positive examples we may have collected of previously observed anomalies. There is a large number of normal examples, and a relatively small number of anomalous examples.</p> <p><code>P(X)</code> is following a gaussian distribution. On the dataset we will fit a Gaussian distribution and then find values that have very low probability and hence can be considered anomalies. The red circles in the figure below are anomalies or outliers.</p> <p></p> <p>Note</p> <pre><code>Remember that the gaussian distribution is giving the probability of X using the mean and the variance sigma. The area under the bell curve is always equals to 1\n</code></pre>"},{"location":"anomaly/#standard-deviation","title":"Standard deviation","text":"<p>A simple approach, on a unique numberical feature is to use the standard deviation:  In statistical data distribution is approximately normal then about 68% of the data values lie within one standard deviation of the mean and about 95% are within two standard deviations, and about 99.7% lie within three standard deviations. See the code in [ml-python/anomaly-detection/StdDeviation.py].</p> <p>When choosing features for an anomaly detection system, it is a good idea to look for features that take on unusually large or small values for (mainly the) anomalous examples.</p>"},{"location":"anomaly/#box-plot","title":"Box plot","text":"<p>Box plots are a graphical depiction of numerical data through their quantiles. It is a very simple but effective way to visualize outliers.</p>"},{"location":"anomaly/#when-to-use-it","title":"When to use it?","text":"<p>use anomaly detection:</p> <ul> <li>When there is a very small number of positive example (y=1)</li> <li>Many different types of anomalies, it is hard for any algorithm to learn from positive examples what the anomalies look like</li> <li>Future anomalies may look nothing like any of the anomalous examples we've seen so far</li> <li>Fraud detection, monitoring machines in a data center, manufacturing</li> </ul> <p>use supervised learning</p> <ul> <li>When both positive and negative examples are large</li> <li>Enough positive examples for algorithm to get a sense of what positive examples are like</li> <li>Future positive examples likely to be similar to ones in training set</li> <li>Spam, weather prediction, cancer classification</li> </ul>"},{"location":"anomaly/#what-feature-to-use-for-anomaly-detection","title":"What feature to use for anomaly detection?","text":"<ul> <li>historical data plot shows a bell curve like gaussian</li> <li>it is possible to transform a non-gaussian feature to a gaussian using square root, log, </li> </ul>"},{"location":"kaggle/","title":"Kaggle and ML tutorial","text":""},{"location":"kaggle/#getting-started-with-titanic-data-set","title":"Getting started with Titanic data set","text":"<p>The goal is to find patterns in <code>train.csv</code> that help us predict whether the passengers in <code>test.csv</code> survived or not.</p> <p>Example of minimum code for a random forest with 100 decision trees</p> <pre><code>import numpy as np \nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n# build test and train sets\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\n</code></pre>"},{"location":"kaggle/#generic-approach","title":"Generic Approach","text":"<ul> <li>Define notebook with the following imports</li> </ul> <pre><code>import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n</code></pre> <ul> <li>Read csv file and see statistics like min, max, mean, std deviation, and 25,50,75%</li> </ul> <pre><code>home_data = pd.read_csv(a_file_path)\n# Print summary statistics\nhome_data.describe()\n# See column names\nhome_data.columns()\n# Select the features using the column names\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\nX = melbourne_data[melbourne_features]\n</code></pre> <ul> <li>Build the model</li> </ul> <pre><code># import model from sklearn\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\n\n# split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state = 0)\n# Define model\nmelbourne_model = DecisionTreeRegressor()\n# Fit model\nmelbourne_model.fit(train_X, train_y)\n</code></pre> <ul> <li>Make prediction</li> </ul> <pre><code>val_predictions=melbourne_model.predict(val_X)\n</code></pre> <ul> <li>Validate prediction accuracy</li> </ul> <p>One metric to use is the Mean Absolute Error (MAE).</p> <pre><code>from sklearn.metrics import mean_absolute_error\nprint(mean_absolute_error(val_y, val_predictions))\n</code></pre> <p>Below is a function to get MAE for a decision tree by changing the depth of the tree:</p> <pre><code>def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n</code></pre> <ul> <li>Persist solution</li> </ul> <pre><code>output = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)\n</code></pre> <p>Then submit to the competition.</p>"},{"location":"kaggle/#decision-tree","title":"Decision Tree","text":"<p>Use the <code>DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)</code> method.</p> <p>A deep tree with lots of leaves will overfit because each prediction is coming from  historical data from only the few records at its leaf. But a shallow tree with few leaves  will perform poorly because it fails to capture as many distinctions in the raw data.</p>"},{"location":"kaggle/#random-forest","title":"Random Forest","text":"<p>The random forest uses many trees, and it makes a prediction by averaging the predictions  of each component tree. It generally has much better predictive accuracy than a single  decision tree and it works well with default parameters:</p> <pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n# create y and X from input file and then,...\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(train_X, train_y)\npredictions = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, predictions))\n</code></pre> <p>Here is an example of running different random forest models:</p> <pre><code>model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\nmodel_2 = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\nmodel_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\nmodel_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n\nmodels = [model_1, model_2, model_3, model_4, model_5]\n</code></pre>"},{"location":"kaggle/#feature-engineering","title":"Feature engineering","text":"<p>See separate note</p>"},{"location":"kaggle/#pipelines","title":"Pipelines","text":"<p>To organize pre-processing and model fitting and prediction.</p> <pre><code># Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Define model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\n# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid)\n</code></pre>"},{"location":"kaggle/#cross-validation","title":"Cross-validation","text":"<p>With cross-validation, we run our modeling process on different subsets of the data to get  multiple measures of model quality.</p> <p>For small datasets, where extra computational burden isn't a big deal, we should run cross-validation. For larger datasets, a single validation set is sufficient. Our code will run faster, and we may have enough data that there's little need to re-use some of it for holdout.</p> <p>The approach is to use sklearn cross_val_score:</p> <pre><code>from sklearn.model_selection import cross_val_score\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n</code></pre>"},{"location":"kaggle/#gradient-boosting","title":"Gradient boosting","text":"<p>Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble. An ensemble combines the predictions of several models.</p> <p>XGBoost,  (extreme gradient boosting) is an implementation of gradient boosting with several additional features focused on performance and speed.</p> <pre><code>from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(X_train, y_train,\n            early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],)\n</code></pre> <p>XGBoost has a few parameters that can dramatically affect accuracy and training speed:</p> <ul> <li>n_estimators: # of model in the ensemble</li> <li>early_stopping_rounds: stop iterating when the validation score stops improving</li> <li>learning_rate: multiply the predictions from each model by a small number (known as the learning rate) before adding them in.</li> <li>n_jobs: equal to the number of cores on your machine, to run in parallel.</li> </ul>"},{"location":"kaggle/#data-leakage","title":"Data Leakage","text":"<p>Data leakage (or leakage) happens when your training data contains information  about the target, but similar data will not be available when the model is used for  prediction.  This leads to high performance on the training set (and possibly even the validation data),  but the model will perform poorly in production.</p> <p>There are two main types of leakage: target leakage and train-test contamination:</p> <ul> <li>Target leakage occurs when your predictors include data that will not be available  at the time you make predictions. It is important to think about target leakage in terms  of the timing or chronological order that data becomes available, not merely whether a  feature helps make good predictions.</li> <li>train-test contamination, when we aren't careful to distinguish training data from validation data. Like running a preprocessing (like fitting an imputer for missing values) before calling <code>train_test_split()</code>.</li> </ul> <p>Examples of data leakage:</p> <ul> <li>to forecast the number of shoelace, every month, the leather used may be a good feature, but it depends if the value is provided at the beginning of the month as a prediction, or close to the end of the month as real consumption of the leather used to build the shoes.</li> <li>Now if leather represents what the company order to make shoes in the month, then the number of showlace may be accurate, except if we order them before the leather.</li> <li>To predict which patients from a rare surgery are at risk of infection.  If we take all surgeries by each surgeon and calculate the infection rate among those surgeons. And then, for each patient in the data, find out who the surgeon was and plug in that surgeon's  average infection rate as a feature, we will create target leakage if a given patient's outcome  contributes to the infection rate for his surgeon, which is then plugged back into the  prediction model for whether that patient becomes infected. We can avoid target leakage  if we calculate the surgeon's infection rate by using only the surgeries before  the patient we are predicting for. Calculating this for each surgery in our training  data may be a little tricky. We also have a train-test contamination problem if we  calculate this using all surgeries a surgeon performed, including those from the test-set.</li> </ul>"},{"location":"kaggle/#other-studies","title":"Other Studies","text":"<ul> <li>Parsing Dates from our Data Cleaning course.</li> <li>Geospatial Analysis course.</li> <li>Natural Language Processing</li> </ul>"},{"location":"architecture/po-processing/","title":"Purchase Order Processing with AI and Hybrid Cloud","text":"<p>This example is a, scope reduced, purchase order processing for a manufacturing company working in complex pipes, pumps and valves, product production. The existing solution includes a web application and a product configurator, based on rule based software. The idea is to improve the process by automating content extraction and interpretation from the unstructured document </p> <p></p> <p>The application flow looks like:</p> <ol> <li> <p>POs are uploaded to cloud storage in buckets organized with some business dimension, like geography, or customer name. </p> <ul> <li>Google Cloud Storage is a highly scalable and durable object storage service. 11 9s availability.</li> <li>It offers different storage classes to meet various performance and cost requirements.</li> <li>Replicated between locations, high availability and low latency access from anywhere in the world.</li> <li>Secured with server-side encryption, IAM-based access control.</li> <li>Support object versioning, and restore to previous version.</li> </ul> </li> <li> <p>Once a file is uploaded, and event can be propagated to a Pub/Sub service responsible to keep the event for some time and authorize asynchronous event processing. It helps to scale but also to use function execution, really paying for what resources are consumed. Adding a pub/sub topic helps to add more subscriber than just a function.</p> <ul> <li>Google Pub/Sub is a fully managed service, highly scalable, distributed messaging system that can handle high volumes of data with low latency. It processes millions of messages per second.</li> <li>It provides at-most-once, at-least-once delivery and guaranteed message ordering.</li> <li>Asynchronous, topic based pub/sub mechanism with long term persistence, replicated for no data lost. Topics help categorize messages and send them to specific subscribers.</li> <li>Designed for real-time event ingestion.</li> <li>Automatically scales to handle changes in message traffic, to handle sudden spikes in data without any manual intervention.</li> <li>Secured with IAM-based access control, encryption encryption at rest and in transit, and different authentication methods.</li> <li>Pay-as-you-go pricing model.</li> </ul> </li> <li> <p>The subscriber to the new-file-upload event is a function doing the document parsing, splitting, and encoding to extract key values in a more structure way. It uses API to call a first AI service for Document understanding</p> <p>Google Cloud Functions:</p> <ul> <li>Cloud Functions allows to run code without having to manage any servers or infrastructure.</li> <li>Automatic scaling up and down to zero.</li> <li>May be triggered by various events, such as HTTP requests, Cloud Storage events, Pub/Sub messages. </li> <li>Supports different programming languages: Node.js, Python, Go, Java, and .NET .</li> <li>Priced based on the number of invocations, the duration of each invocation, and the amount of memory used.</li> <li>Integrated with monitoring ang logging services.</li> </ul> <p>Google Document AI is used to process and understand documents:</p> <ul> <li>Extracts and understands structured information from a wide variety of document types, including PDFs, images, and scanned documents.</li> <li>It leverages advanced natural language processing (NLP) and computer vision technologies to deliver high-accuracy document extraction and understanding. </li> <li>Trained to extract data from invoices, receipts, and contracts.</li> <li>It allows to create custom document models to extract information from specialized or domain-specific documents.</li> <li>Different natural languages are supported.</li> <li>Scale up and down ensuring high throughput and low latency.</li> <li>No-code tools to let developers quickly set up and configure document processing pipelines without writing any code.</li> <li>Integrated with monitoring and logging services.</li> <li>It is possible to combine with custom MM models to do better entity extraction.</li> </ul> </li> <li> <p>Custom development for product configuration flow automation. This will be a LangGraph implementation to support conversation management, integration with Large Language Model, like Gemini, and doing function calling to interact with the expert system.</p> <ul> <li>The approach is to define a configuration tree, to search for the next data to extract from the parsed purchase order, and drive the interactions with the expert system.</li> </ul> </li> <li> <p>If we need to process unstructured user's requests we can add Google Gemini to support entity extraction, and agentic application. </p> <ul> <li>Gemini offers multi-modal capabilities: it is designed to understand, operate, and combine different types of information, including text, images, audio, video, and code.</li> <li>Code assistant</li> <li>Available in three sizes: Nano, Pro, and Ultra - each optimized for different user needs.</li> <li>Excellent performance: 90.0% score on the Massive Multitask Language Understanding (MMLU) benchmark</li> <li>Gemini API enables developers to adjust safety settings on the following 4 dimensions to quickly assess if the application requires more or less restrictive configuration: 1/ Harassment, 2/ Hate speech, 3/ Sexually explicit, 4/ Dangerous</li> <li>Pay-as-you-go</li> <li>Gemini Pro demonstrates improved capabilities in understanding complex concepts and solving logical problems. This makes it suitable for tasks that require critical thinking.</li> <li>Still under development and continuous improvement.</li> </ul> </li> </ol>"},{"location":"architecture/sol-design/","title":"AI deployment solution designs","text":"<p>This section presents examples of solution design using Gen AI, Hybrid AI, Agentic AI, with links to external demonstrations when available</p>"},{"location":"architecture/sol-design/#1-basic-private-chatbot-deployed-on-a-cloud-provider","title":"1- Basic private chatbot deployed on a cloud provider","text":""},{"location":"architecture/sol-design/#needs","title":"Needs","text":"<ul> <li>End users are deployed world wide and are part of a B2B partnership, so not a consumer app</li> <li>Access via login and Single Sign On</li> <li>Highly available</li> <li>Trace the conversation in lake house to be able to do quality assurance and AI accuracy enhancement</li> </ul>"},{"location":"architecture/sol-design/#solution","title":"Solution","text":"<p>A solution need to address the following subjects:</p> <ul> <li>User authentication and authorization</li> <li>DNS record - DNS Routing to Load Balancers in each AZ and even in different regions</li> <li>Web socket connection to API server and conversation server</li> <li>May be API Gateway</li> <li>Static content on S3 with cloudfront</li> <li>Auto-scaling of conversation server</li> <li>Avoid single point of failure for each components</li> <li>Address backup/ restore</li> <li>Failover and recovery for DR</li> <li>GitOps</li> </ul> <p>At the system context level, we have the following high-level components, as seen by an end-user.</p> <p></p>"},{"location":"architecture/sol-design/#deeper-dive","title":"Deeper dive","text":""},{"location":"architecture/sol-design/#chatbot-client-app","title":"ChatBot client app","text":"<p>The application may be a mobile app or a modern single page reactive webapp. As the user interface should be simple wth login/authentication mechanism then a chatbot interface, it will be easy to support it with a reactive HTML/ javascript webapp. The most important decision is how to maintain the communication with the conversation server.</p>"},{"location":"architecture/sol-design/#_1","title":"Solution Design","text":"<p>Amazon Route 53 is Domain Name System (DNS) web service for DNS routing. It is used to route internet traffic to a website or web application. It is highly available, scalable, and fully managed, global service. It is used to load balance between regions, and then between AZs. </p> <p></p> <p>When using an EC2 auto-scaling group to host the conversation server, we need to add elastic load balancers to manage load balancing and routing within the region and cross AZs.</p> <p>Route 53 uses the concept of <code>hosted zone</code> which is a \"container\" that holds information about how to route traffic for a domain or subdomain. The zone can be public (internet facing) or private (inside a VPC). All resource record sets within a hosted zone must have the hosted zone\u2019s domain name as a suffix.</p> <p>EC2 DNS name could not be a target of DNS alias. Routing policies helps to define how Route 53 responds to DNS queries. </p> <p>When using API Gateway, you create a Route 53 alias record that routes traffic to the regional API endpoint. The API Gateway routes traffic to the Lambda function backend, to SQS, or to Fargate tasks running in ECS as illustrated in the following figure:</p> <p></p> <p>See this nice repository for terraform definitions for a microservice deployment on Fargate with API Gateway, VPC private endpoints, and NLB.</p>"},{"location":"architecture/sol-design/#2-rag-solution-ha-and-scalable","title":"2- RAG solution HA and scalable","text":"<p>RAG solution adds document management and vector store management on top of the ChatBot solution. Most likely those chatbot will be for internal staff to the enterprise but could be for B2B and even some B2C (even as of mid 2024 it is very risky to expose LLM to consumer for enterprises running in regulated business).</p> <p>The basic RAG high level architecture </p> <p></p>"},{"location":"architecture/sol-design/#3-ml-flow","title":"3- ML Flow","text":""},{"location":"architecture/sol-design/#4-stateful-agentic-application","title":"4- Stateful Agentic application","text":""},{"location":"architecture/sol-design/#5-purchase-order-processing","title":"5- Purchase Order Processing","text":"<p>An neuro-symbolic solution to partially automate purchase order processing and manufactured product configuration. See dedicated note.</p>"},{"location":"blogs/lg_drools_kg/","title":"Agentic solution with decision engines and knowledge graph","text":""},{"location":"coding/","title":"Coding","text":"Update <p>05/02/2023 Move to python 3.10 in docker, retest docker env with all code. See samples section below.</p> <p>09/10/2023: Add PyTorch</p> <p>12/2023: Clean Jupyter</p>"},{"location":"coding/#environments","title":"Environments","text":"<p>To avoid impacting the laptop (Mac) python core installation, use virtual environment:</p> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre> <p>Then in each main folder there is a <code>requirements.txt</code> to get the necessary modules.</p> <p>There are a lot of other solutions we can use, like the Amazon scikit-learn image. The SageMaker team uses this repository to build its official Scikit-learn image.  we can build an image via:</p> <pre><code>docker build -t sklearn-base:1.2-1 -f https://raw.githubusercontent.com/aws/sagemaker-scikit-learn-container/master/docker/1.2-1/base/Dockerfile.cpu .\n</code></pre>"},{"location":"coding/#vscode","title":"VSCode","text":""},{"location":"coding/#jupyter-notebook","title":"Jupyter Notebook","text":"<ul> <li>To select an environment, use the <code>Python: Select Interpreter</code> command from the Command Palette (\u21e7\u2318P)</li> <li>Use <code>Create: New Jupyter Notebook</code> from command Palette</li> <li>Select a kernel using the kernel picker in the top right.</li> <li>Within a Python Notebook, it's possible to view, inspect, sort, and filter the variables within the current Jupyter session, using <code>Variables</code> in toolbar.</li> <li>We can offload intensive computation in a Jupyter Notebook to other computers by connecting to a remote Jupyter server. Use server URL with security token.</li> </ul>"},{"location":"coding/#run-kaggle-image","title":"Run Kaggle image","text":"<p>As an alternate Kaggle has a more complete docker image to start with. </p> <pre><code># CPU based\ndocker run --rm -v $(pwd):/home -it gcr.io/kaggle-images/python /bin/bash\n# GPU based\ndocker run -v $(pwd):/home --runtime nvidia --rm -it gcr.io/kaggle-gpu-images/python /bin/bash\n</code></pre>"},{"location":"coding/#conda","title":"Conda","text":"<p>Conda provides package, dependency, and environment management for many languages. </p> <p>On Mac M1 we need ARM64 architecture. </p> <ol> <li>Install miniconda: projects/miniconda</li> <li>To create a conda environment named \"torch\", in miniconda3 folder do: <code>conda env create -f torch-conda-nightly.yml -n torch</code></li> <li> <p>Activate conda environment: </p> <pre><code>conda activate torch\n</code></pre> </li> <li> <p>Register environment: <code>python -m ipykernel install --user --name pytorch --display-name \"Python 3.9 (pytorch)\"</code></p> </li> <li>Install the following: <code>conda install pytorch pandas scikit-learn</code></li> <li>Start Jupiter: <code>jupiter notebook</code></li> <li>Execute the notebook in to test test-env.ipynb</li> </ol> <p>Install Conda on Windows.</p>"},{"location":"coding/#run-jupyter-notebooks","title":"Run Jupyter notebooks","text":"<ul> <li> <p>We can use jupyter lab (see installation options) or conda and miniconda.</p> <pre><code>conda install -y jupyter\n</code></pre> </li> </ul>"},{"location":"coding/#jupyterlab","title":"JupyterLab","text":"<p>The following works as of April 2023:</p> <pre><code>pip3 install jupyterlab\n# build the assets\njupyter-lab build\n# The path is something like\n# /opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/share/jupyter/lab\n# Start the server\njupyter-lab\n</code></pre> <p>Once started, in VScode select a remote Python kernel and Jupiter extension to run the notebook inside it. </p>"},{"location":"coding/#important-python-libraries","title":"Important Python Libraries","text":""},{"location":"coding/#numpy","title":"numpy","text":"<ul> <li>Array computing in Python. Getting started</li> <li>NumPy dimensions are called axes.</li> </ul> <p><pre><code>import numpy as np\na = np.array([2, 3, 4])\nb = np.ones((2, 3, 4), dtype=np.int16)\nc = np.zeros((3, 4))\n</code></pre> * Create a sequence of number: <code>np.arange(10, 30, 5)</code> * Matrix product: using .dot or @</p> <pre><code>```python\nA = np.array([[1, 1], [0, 1]])\nB = np.array([[2, 0], [3, 4]])\nA @ B\nA.dot(B)\n```\n</code></pre>"},{"location":"coding/#scipy","title":"scipy","text":"<p>SciPy is a collection of mathematical algorithms and convenience functions built on NumPy .</p> <ul> <li>Get a normal distribution function: use the probability density function (pdf)</li> </ul> <pre><code>from scipy.stats import norm\nx = np.arange(-3, 3, 0.01)\ny=norm.pdf(x)\n</code></pre>"},{"location":"coding/#matplotlib","title":"MatPlotLib","text":"<p>Persent figure among multiple axes, from our data.</p> <ul> <li>Classic import</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nimport matplotlib as mpl\n</code></pre> <ul> <li>See Notebook</li> </ul>"},{"location":"coding/#seaborn","title":"Seaborn","text":"<p>Seaborn provides a high-level interface for drawing attractive and informative statistical graphics. Based on top of MatPlotLib and integrated with Pandas.</p> <p>See the introduction for different examples</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\nsns.relplot(\n    data=masses_data,\n    x=\"age\", y=\"shape\", \n    hue=\"density\", size=\"density\"\n)\nplt.show()\n</code></pre>"},{"location":"coding/#pytorch","title":"PyTorch","text":"<p>Via conda or pip, install <code>pytorch torchvision torchaudio</code>.</p> <p>Example of getting started code in deep-neural-net folder. </p> <p>Summary of the library and deeper studies</p>"},{"location":"coding/#code-samples","title":"Code samples","text":"Link Description Perceptron To classify the iris flowers. Use identity activation function Adaline ADAptive LInear NEuron with weights updated based on a linear activation function Fischer Fisher classification for sentences"},{"location":"coding/ddp/","title":"DDP","text":""},{"location":"coding/ddp/#distributed-data-parallel-with-pytorch","title":"Distributed Data Parallel with PyTorch","text":"Updates <p>Created 01/2024</p> <p>The goal of Distributed Data Parallel is to train model in distributed computers but keep model integrity. PyTorch offers a DDP library (<code>torch.distributed</code>) to facilitate this complex processing on multiple GPU hosts or using multiple machines.</p> <p>On one host, the model is trained on CPU/GPU, from the complete data set. It processes the forward pass to compute weights, computes the lost, performs the backward propagation for the gradients, then optimizes the gradients. </p> <p>Using more hosts, we can split the dataset and send those data to different hosts, which have the same initial model and optimizer function.</p> <p></p> <p>Sending different data set to train the different models, leads to different gradients, so different models.</p> <p>DDP adds a synchronization step before optimizing the gradients so each model has the same weights:</p> <p></p> <p>Each gradients from all the replicas are aggregated between model using the bucketed Ring AllReduce algorithm. DDP overlaps gradient computation with communication to synch them between models. The synchronization step does not need to wait for all gradient within one model to be computed, it can start communication along the ring while the backward pass is still running, this ensures the GPUs are always working.</p> <p>On a computer with multiple GPUs, each GPU will run on process, which communicates between each other. A process group helps to discover those processes and manage the communication. One host is the master to coordinate the processes across GPUs and machines. </p> <p></p> <p>The backend is Nvidia communication library, <code>nccl</code>. On a single machine with one GPU, it is not needed to use DDP, but to test the code, we can use the <code>gloo</code> backend.</p> <pre><code>def ddp_setup(rank, world_size):\n    \"\"\"\n    Args:\n        rank: Unique identifier of each process\n        world_size: Total number of processes\n    \"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n</code></pre> <p>A model in the trainer is now a DDP using the gpu_id</p> <pre><code> self.model = DDP(model, device_ids=[gpu_id])\n</code></pre> <p>Each process has its own instance of the trainer class. Only one process will perform the checkpoint save:</p> <pre><code> if self.gpu_id == 0 and epoch % self.save\n</code></pre> <p>The <code>torch.multiprocessing</code> is responsible to take a function and spawns it to all processes in the distributed group. The rank, for each trainer, is assigned by multiprocessing.</p> <pre><code>mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)\n</code></pre>"},{"location":"coding/ddp/#multi-gpus-with-torchrun","title":"Multi-GPUs with torchrun","text":"<p><code>torchrun</code> provides a superset of the functionality as torch.distributed.launch to manage worker failures by restarting all workers from last snapshots, and continue training. The number of nodes may change overtime.</p> <p>A snapshot includes the model states, and any other parameters like, state of the optimizer, the epoch...</p> <p><code>torchrun</code> manages the environment for us. LOCAL_RANK for example is used to identify the GPU id. The code is then a little bit simpler as illustrated in multi_gpu_torchrun.py.</p> <p>Example of execution for testing</p> <pre><code>torchrun --standalone --nproc_per_node=gpu multi_gpu_torchrun.py 50 10\n</code></pre> <p>When using multiple machines we can use <code>torchrun</code> on each machine, with one master machine to coordinate the communication. LOCAL_RANK is used at the machine level, in case there are multiple GPUs on this machine, the GPU will be indexed from 0. A global rank is needed, cross machine. For the backend coordinator, select a machine with high level bandwidth. On AWS be sure to set the network security policy to let nodes communicating over tcp. </p> <p>Below is the command to run on each machine, with different node_rank on each machine:</p> <pre><code>export NCCL_DEBUG=INFO\nexport NCCL_SOCKET_INAME=eth0\ntorchrun --nproc_per_node=gpu --nnodes=3 --node_rank=0 --rdzv_id=345 --rdzv_backend=c10d --rdzv_endpoint=1720.30.23.101:29603 multi_gpu_torchrun.py 50 10\n</code></pre> <p>Each machine will have their own snapshot. </p> <p>Recall that splitting data processing between machine over network is less efficient, than on  ane machine is more GPUs. So prefer bigger machine. The constraint, then, becomes the memory.</p>"},{"location":"coding/ddp/#slurm-as-orchestrator","title":"Slurm as orchestrator","text":"<p>Andrej Karathy's minGPT repository is a PyTorch re-implementation of the GPT with a small size, to learn the GPT architecture. It includes small projects for sandboxing. And nanoGPT to reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training.</p>"},{"location":"coding/ddp/#code-samples","title":"Code samples","text":"<ul> <li>multi_gpu_ddp.py demonstrates the basic DDP code to train a model using multiple GPUs machine, and the <code>from torch.utils.data.distributed import DistributedSampler</code> the <code>torch.nn.parallel.DistributedDataParallel</code> and <code>torch.distributed</code> modules. </li> </ul> <pre><code>def prepare_dataloader(dataset: Dataset, batch_size: int):\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        pin_memory=True,\n        shuffle=False,\n        sampler=DistributedSampler(dataset)\n    )\n</code></pre> <p>To run this example with 50 epochs saved every 10 epochs: <code>python multi_gpu_ddp.py 50 10</code></p> <ul> <li>multi_gpu_torchrun.py to run a training on multiple GPUs on the same machine with <code>torchrun</code></li> <li>multinode.py to demonstrate global rank.</li> </ul>"},{"location":"coding/ddp/#source-of-information","title":"Source of information","text":"<ul> <li>DDP Tutorial Series code repository. and YouTube videos by Suraj Subramanian.</li> <li>See minGPT git repository for a chatGPT model and trainer to do a fine tuning.</li> </ul>"},{"location":"coding/haystack/","title":"Haystack AI Framework","text":""},{"location":"coding/langchain/","title":"LangChain Study","text":"<p>In LLM application there are a lot of steps to do, trying different prompting, integrating different LLMs, implementing conversation history, at the end there is a lot of glue code to implement.</p> <p>LangChain is a open-source framework for developing applications powered by large language models, connecting them to external data sources, and manage conversation with human. </p>"},{"location":"coding/langchain/#value-propositions","title":"Value propositions","text":"<p>Develop apps with context awareness, and that can reason using LLMs. It includes Python and Typescript packages, and a Java one under construction.</p> <p>It focuses on composition and modularity. The components defined by the framework can be combined to address specific use cases, and developers can add new components.</p> <ul> <li>LangChain: Python and Javascript libraries</li> <li>LangServe: a library for deploying LangChain chains as a REST API.</li> <li>LangSmith: a platform that lets developers debug, test, evaluate, and monitor chains</li> <li>Predefined prompt template from langChain Hub.</li> </ul> <p>They are adding new products to their portfolio quickly like LangSmith (get visibility on LLMs execution), and LangServe (server API for LangChain apps).</p>"},{"location":"coding/langchain/#sources","title":"Sources","text":"<p>The content comes from different sources:</p> <ul> <li>Excellent product documentation, should be the go to place.</li> <li>deeplearning.ai LangChain introduction by Harisson Chase and Andrew Ng</li> <li>LLM Powered Autonomous Agents</li> <li>Retrieval and RAG blog.</li> </ul>"},{"location":"coding/langchain/#langchain-libraries","title":"LangChain libraries","text":"<p>The core building block of LangChain applications is the LLMChain:</p> <ul> <li>A LLM</li> <li>Prompt templates</li> <li>Output parsers</li> </ul> <p>PromptTemplate helps to structure the prompt and facilitate reuse by creating model agnostic templates. The library includes output parsers to get content extracted from the keyword defined in the prompt. Example is the chain of thought keywords of Thought, Action, Observation.</p>"},{"location":"coding/langchain/#getting-started","title":"Getting started","text":"<p>The LangChain documentation is excellent so no need to write more. All my study codes with LangChain and LLM are in different folders of this repo:</p> Backend Type of chains openAI The implementation of the quickstart examples, RAG, chatbot, agent Ollama run a simple query to Ollama (running Llama 3.2) locally Anthropic Claude Mistral LLM IBM WatsonX AWS Bedrock zero_shot generation <p>Each code needs to define only the needed LangChain modules to keep the executable size low. </p>"},{"location":"coding/langchain/#main-concepts","title":"Main Concepts","text":""},{"location":"coding/langchain/#model-io","title":"Model I/O","text":"<p>Model I/O are building blocks to interface with any language model. It facilitates the interface of model input (prompts) with the LLM model to produce the model output.</p> <ul> <li>LangChain supports two types of language: LLM (for pure text completion models) or ChatModel (conversation on top of LLM using constructs of AIMessage, HumanMessage)</li> <li> <p>LangChain uses Prompt templates to control LLM behavior.</p> <ul> <li>Two common prompt templates: string prompt templates and chat prompt templates.</li> </ul> <p><pre><code>from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"user\", \"{input}\"),\n])\n</code></pre> * We can build custom prompt by extending existing default templates. An example is a 'few-shot-examples' in a chat prompt using FewShotChatMessagePromptTemplate. * LangChain offers a prompt hub to get predefined prompts easily loadable:</p> <pre><code>from langchain import hub\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\n</code></pre> </li> <li> <p>Chains allow developers to combine multiple components together (or to combine other chains) to create a single, coherent application. </p> </li> <li> <p>OutputParsers convert the raw output of a language model into a format that can be used downstream</p> </li> </ul> <p>Feature stores, like Feast, can be a great way to keep information about the user conversation or query, and LangChain provides an easy way to combine data from Feast with LLMs.</p>"},{"location":"coding/langchain/#chain","title":"Chain","text":"<p>Chains are runnable, observable and composable. The LangChain framework uses the Runnable class to encapsulate operations that can be run synchronously or asynchronously. </p> <ul> <li> <p>LLMChain class is the basic chain to integrate with any LLM.</p> <pre><code># Basic chain\nchain = LLMChain(llm=model, prompt = prompt)\nchain.invoke(\"a query\")\n</code></pre> </li> <li> <p>Sequential chain combines chains in sequence with single input and output (SimpleSequentialChain)</p> <pre><code>overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n                                            verbose=True\n                                            )\n</code></pre> <p>or multiple inputs and outputs with prompt using the different environment variables (see this code).</p> </li> <li> <p>LLMRouterChain is a chain that outputs the name of a destination chain and the inputs to it.</p> </li> <li> <p>LangChain Expression Language is a declarative way to define chains. It looks similar to Unix shell pipe: input for one runnable comes from the output of predecessor (This is why prompt below is a runnable). </p> <pre><code># a chain definition using Langchain expression language\nchain = prompt | model | output_parser\n</code></pre> </li> <li> <p>Chain can be executed asynchronously in its own Thread using the <code>ainvoke</code> method.</p> </li> </ul>"},{"location":"coding/langchain/#runnable","title":"Runnable","text":"<p>Runnable interface is a protocol to define custom chains and invoke them. Each Runnable exposes methods to get input, output and config schemas. Each implements synchronous and async invoke methods and batch. Runnable can run in parallel or in sequence.</p> <p>To pass data to a Runnable there is the <code>RunnablePassthrough</code> class. This is used in conjunction with RunnableParallel to assign data to key in a map.</p> <pre><code>from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n\nrunnable = RunnableParallel(\n   passed = RunnablePassthrough(),\n   extra= RunnablePassthrough.assign(mult= lambda x:x[\"num\"] * 3),\n   modified=lambda x:x[\"num\"] +1   \n)\n\nprint(runnable.invoke({\"num\": 6}))\n{'passed': {'num': 6}, 'extra': {'num': 6, 'mult': 18}, 'modified': 7}\n</code></pre> <ul> <li>RunnableLambda is a type of Runnable that wraps a callable function. </li> </ul> <pre><code>sequence = RunnableLambda(lambda x: x + 1) | {\n    'mul_2': RunnableLambda(lambda x: x * 2),\n    'mul_5': RunnableLambda(lambda x: x * 5)\n}\nsequence.invoke(1)\n</code></pre> <p>The RunnablePassthrough.assign method is used to create a Runnable that passes the input through while adding some keys to the output.</p> <p>We can use <code>Runnable.bind()</code> to pass arguments as constants accessible within a runnable sequence (a chain) where argument is not part of the output of preceding runnables in the sequence.</p> <p>See some code RunnableExamples</p>"},{"location":"coding/langchain/#memory","title":"Memory","text":"<p>Large Language Models are stateless and do not remember anything. Chatbot seems to have memory, because conversation is kept in the context. </p> <p>With a simple conversation like the following code, the conversation is added as string into the context:</p> <pre><code>llm = ChatOpenAI(temperature = 0)\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(\n    llm= llm,\n    memory=memory,\n    verbose=True   # trace the chain\n)\n</code></pre> <p>The memory is just a container in which we can save {\"input:\"\"} and {\"output\": \"\"} content.</p> <p>But as the conversation goes, the size of the context grows, and so the cost of operating this chatbot, as API are charged by the size of the token. Using <code>ConversationBufferWindowMemory(k=1)</code> with a k necessary to keep enough context, we can limit cost. Same with <code>ConversationTokenBufferMemory</code> to limit the token in memory.</p> <p>ConversationChain is a predefined chain to have a conversation and load context from memory.</p> <p>As part of memory component there is the ConversationSummaryMemory to get the conversation summary so far.</p> <p>The other important memory is Vector Data memory and entity memory or knowledgeGraph</p> <p>See related code conversation_with_memory.py</p>"},{"location":"coding/langchain/#retrieval-augmented-generation","title":"Retrieval Augmented Generation","text":"<p>The goal for Retrieval Augmented Generation (RAG) is to add custom dataset not already part of a trained model and use the dataset as input sent to the LLM. RAG is illustrated in figure below:</p> <p></p> <p>Embed is the vector representation of a chunk of text. Different embedding can be used.</p> Embeddings <p>The classical Embedding is the OpenAIEmbeddings but Hugging Face offers an open source version: the SentenceTransformershttps://huggingface.co/sentence-transformers which is a Python framework for state-of-the-art sentence, text and image embeddings.</p> <pre><code>from langchain_openai import OpenAIEmbeddings\nvectorstore =  Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(),\n                                     persist_directory=DOMAIN_VS_PATH)\n# With HuggingFace\nfrom sentence_transformers import SentenceTransformer\ndef build_embedding(docs):\n    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    return model.encode(docs)\n# With AWS embedding\nfrom langchain.embeddings import BedrockEmbeddings\n</code></pre> <p>Different code that implement RAG</p> Code Notes build_agent_domain_rag.py Read Lilian Weng blog and  create a ChromeDB vector store with OpenAIEmbeddings query_agent_domain_store.py Query the persisted vector store for similarity search prepareVectorStore.py Use AWS Bedrock Embeddings embeddings_hf.py Use Hunggingface embeddings with splitting a markdown file and use FAISS vector store rag_HyDE.py Hypothetical Document Embedding (HyDE) the first prompt create an hypothetical document <p>Creating chunks is necessary because language models generally have a limit to the amount of token they can deal with. It also improve the similarity search based on vector.</p> Split docs and save in vector store <p>RecursiveCharacterTextSplitter splits text by recursively look at characters. <code>text_splitter.split_documents(documents)</code> return a list of Document which is a wrapper to page content and some metadata for the indexes from the source document.</p> <pre><code># ...\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.indexes.vectorstore import VectorStoreIndexWrapper\n\nloader = PyPDFDirectoryLoader(\"./data/\")\ndocuments = loader.load()\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 1000,\n    chunk_overlap  = 100,\n)\ndocs = text_splitter.split_documents(documents)\n\nvectorstore_faiss = FAISS.from_documents(\n    docs,\n    embeddings,\n)\nvectorstore_faiss.save_local(\"faiss_index\")\n</code></pre> Search similarity in vector DB <p>OpenAIEmbeddings <pre><code>embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=1024)\nquery = \"\"\"Is it possible that ...?\"\"\"\nquery_embedding = embeddings.embed_query(query)\nrelevant_documents = vectorstore_faiss.similarity_search_by_vector(query_embedding)\n</code></pre></p> <p>During the interaction with the end-user, the system (a chain in LangChain) retrieves the most relevant data to the question asked, and passes it to LLM in the generation step.</p> <ul> <li>Embeddings capture the semantic meaning of the text to help do similarity search</li> <li>Persist the embeddings into a Vector store. Faiss and ChromaDB are common vector stores to use, but OpenSearch, Postgresql can also being used.</li> <li>Retriever includes semantic search and efficient algorithm to prepare the prompt. To improve on vector similarity search we can generate variants of the input question.</li> </ul> <p>See Q&amp;A with FAISS store qa-faiss-store.py.</p> <ul> <li> <p>Another example of LLM Chain with AWS Bedrock llm and Feast as feature store</p> </li> <li> <p>Web scraping for LLM based web research. It uses the same process: document/page loading, transformation with tool like BeautifulSoup, to HTML2Text.</p> </li> </ul> Getting started with Feast <p>Use <code>pip install feast</code> then the <code>feast</code> CLI with <code>feast init my_feature_repo</code> to create a Feature Store then <code>feast apply</code> to create entity, feature views, and services. Then <code>feast ui</code> + http://localhost:8888 to act on the store. </p> LLM and FeatureForm <p>See FeatureForm as another open-source feature store solution and the LangChain sample with Claude LLM</p>"},{"location":"coding/langchain/#qa-app","title":"Q&amp;A app","text":"<p>For Q&amp;A the pipeline will most likely integrate with existing documents as illustrated in the figure below:</p> <p></p> <p>Embeddings capture the semantic meaning of the text, which helps to do similarity search. Vector store supports storage and searching of these embeddings. Retrievers use different algorithms for the semantic search to load vectors. </p> Use RAG with Q&amp;A <p>chains.RetrievalQA</p> <pre><code>from langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\nprompt_template = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nAssistant:\"\"\"\n\nPROMPT = PromptTemplate(\n    template=prompt_template, input_variables=[\"context\", \"question\"]\n)\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore_faiss.as_retriever(\n        search_type=\"similarity\", search_kwargs={\"k\": 3}\n    ),\n    return_source_documents=True,\n    chain_type_kwargs={\"prompt\": PROMPT}\n)\nquery = \"Is it possible that I get sentenced to jail due to failure in filings?\"\nresult = qa({\"query\": query})\nprint_ww(result['result'])\n</code></pre>"},{"location":"coding/langchain/#chatbot","title":"ChatBot","text":"<p>Chatbots is the most common app for LLM: Aside from basic prompting and LLMs call, chatbots have memory and retrievers:</p> <p></p>"},{"location":"coding/langchain/#text-generation-examples","title":"Text Generation Examples","text":"<ul> <li>Basic query with unknown content to generate hallucination: 1st_openAI_lc.py </li> <li>Simple test to call Bedrock with Langchain using on zero_shot generation.</li> <li>Response to an email of unhappy customer using Claude 2 and PromptTemplate. <code>PromptTemplates</code> allow us to create generic shells which can be populated with information and get model outputs based on different scenarios. See the text_generation/ResponseToUnhappyCustomer.py code.</li> </ul>"},{"location":"coding/langchain/#summarization-chain","title":"Summarization chain","text":"<p>Always assess the size of the content to send, as the approach can be different: for big document, we need to split the doc in chunks.</p> <ul> <li>Small text summary with OpenAI. </li> <li>Small text to summarize, with bedrock client and the invoke_model on the client see the code in llm-langchain/summarization/SmallTextSummarization.py</li> <li>For big document, langchain provides the load_summarize_chain to summarize by chunks and get the summary of the summaries. See code with 'manual' extraction of the summaries as insights and then creating a summary of insights in summarization/long-text-summarization.py or using a LangChain summarization with map-reduce in summarization/long-text-summarization-mr.py.</li> </ul> Using langchain summarize chain <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.llms.bedrock import Bedrock\nfrom langchain.chains.summarize import load_summarize_chain\n\nllm = Bedrock(\n    model_id=modelId,\n    model_kwargs={\n        \"max_tokens_to_sample\": 1000,\n    },\n    client=boto3_bedrock,\n) \n\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\"\\n\\n\", \"\\n\"], chunk_size=4000, chunk_overlap=100\n)\ndocs = text_splitter.create_documents([letter])\n\nsummary_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=True)\noutput = summary_chain.run(docs)\n</code></pre>"},{"location":"coding/langchain/#evaluating-results","title":"Evaluating results","text":"<p>The evaluation of a chain, we need to define the data points to be measured. Building Questions and accepted answers is a classical approach.</p> <p>We can use LLM and a special chain (QAGenerateChain) to build Q&amp;A from a document.</p>"},{"location":"coding/langchain/#agent","title":"Agent","text":"<p>Agent is an orchestrator pattern where the LLM decides what actions to take from the current query and context. With chain, developer code the sequence of tasks, with agent the LLM decides. LangGraph is an extension of LangChain specifically aimed at creating highly controllable and customizable agents.</p> <p>Chains let create a pre-defined sequence of tool usage(s), while Agents let the model uses tools in a loop, so that it can decide how many times to use its defined tools.</p> AgentExecutor is deprecated <p>Use LangGraph to implement agent.</p> <p>This content is then from v0.1</p> <p>There are different types of agent: Intended Model, Supports Chat, Supports Multi-Input Tools, Supports Parallel Function Calling, Required Model Params.</p> <p>LangChain uses a specific Schema model to define: AgentAction, with tool and tool_input and AgentFinish.</p> <pre><code>from langchain.agents import create_tool_calling_agent\nfrom langchain.agents import AgentExecutor\nfrom langchain.tools.retriever import create_retriever_tool\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\n...\ntools = [retriever_tool, search, llm_math, wikipedia]\n\nagent = create_tool_calling_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n</code></pre> <ul> <li>To create agents use one of the constructor methods such as: <code>create_react_agent, create_json_agent, create_structured_chat_agent</code>, create_tool_calling_agent etc. Those methods return a Runnable.</li> <li>The Agent loops on user input until it returns <code>AgentFinish</code> action. If the Agent returns an <code>AgentAction</code>, then use that to call a tool and get an <code>Observation</code>. Agent has input and output and <code>intermediate steps</code>. AgentAction is a response that consists of action and action_input. </li> <li>See the existing predefined agent types.</li> <li> <p>AgentExecutor is the runtime for an agent.</p> </li> <li> <p>Tools are functions that an agent can invoke. It defines the input schema for the tool and the function to run. Parameters of the tool should be sensibly named and described.</p> </li> </ul>"},{"location":"coding/langchain/#tool-calling","title":"Tool Calling","text":"<p>With Tool Calling we can define function or tool to be referenced as part of the LLM response, and LLM will prepare the arguments for the function. It is used to generate tool invocations, not to execute it. </p> <p>Tool calling allows a model to detect when one or more tools should be called and responds with the inputs that should be passed to those tools. The inputs match a defined schema. Below is an example structured answer from OpenAI LLM: \"tool_calls\" is the key to get the list of function names and arguments the orchestrator needs to call.</p> <pre><code>    \"tool_calls\": [\n            {\n            \"name\": \"tavily_search_results_json\",\n            \"args\": {\n                \"query\": \"weather in San Francisco\"\n            },\n            \"id\": \"call_Vg6JRaaz8d06OXbG5Gv7Ea5J\"\n            }\n</code></pre> <p>Prompt defines placeholders to get tools parameters. The following langchain prompt for OpenAI uses <code>agent_scratchpad</code> variable, which is a <code>MessagesPlaceholder</code>. Intermediate agent actions and tool output messages, will be passed in here. </p> <p>LangChain has a lot of predefined tool definitions to be reused.</p> <p>We can use tool calling in chain (to use tools in sequence) or in agent (to use tools in loop).</p> <p>LangChain offers an API to the LLM called <code>bind_tools</code> to pass the definition of the tool, as part of each call to the model, so that the application can invoke the tool when appropriate.</p> <p>See also the load tools api with a list of predefined tools.</p> <p>Below is the classical application flow using tool calling. The exposed function wraps a remote microservice.</p> <p></p> <p>When developing a solution based on agent, consider the tools, the services, the agent needs to access. See a code example openAI_agent.py.</p> <p>Many LLM providers support for tool calling, including Anthropic, Cohere, Google, Mistral, OpenAI, see the existing LangChain tools.</p>"},{"location":"coding/langchain/#interesting-tools","title":"Interesting tools","text":""},{"location":"coding/langchain/#search-recent-news","title":"Search recent news","text":"<p>A common tool integrated in agent, is the Tavily search API, used to get the last trusted News, so the most recent information created after the cutoff date of the LLM.</p> <pre><code>retriever_tool = create_retriever_tool(\n    retriever,\n    \"langsmith_search\",\n    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n)\nsearch = TavilySearchResults()\ntools = [retriever_tool, search]\n</code></pre> Tavily <p>Tavily is the leading search engine optimized for LLMs. It provides factual, explicit and objective answers. It is a GPT researcher which queries, filters and aggregates over 20+ web sources per a single research task. It focuses on optimizing search for AI developers and autonomous AI agents. See this git repo</p>"},{"location":"coding/langchain/#python-repltool","title":"Python REPLtool","text":"<p>PythonREPLTool is a tool for running python code in REPL (look like a jupiter notebook).</p>"},{"location":"coding/langchain/#a-base-model","title":"A base model","text":"<p>It is possible to bind a BaseModel class as below, where a LLM is used to create prompt, so the prompt instruction entity is a json used as tool. (Tool definition are structured system prompt for the LLMs as they just understand text)</p> <pre><code>class PromptInstructions(BaseModel):\n    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n    objective: str\n    variables: List[str]\n    constraints: List[str]\n    requirements: List[str]\n\nllm_with_tool = llm.bind_tools([PromptInstructions])\n</code></pre> <p>See the LangGraph sample: prompt_builder_graph.py.</p>"},{"location":"coding/langchain/#our-own-tools","title":"Our own tools","text":"<p>Define custom tool using the <code>@tool</code> annotation on a function to expose it as a tool. It uses the function name as the tool name and the function\u2019s docstring as the tool\u2019s description. </p> <p>A second approach is to subclass the langchain.<code>pydantic_v1.BaseModel</code> class. </p> <p>Finally the last possible approach is to use <code>StructuredTool</code> dataclass. </p> <p>When doing agent we need to manage exception and implement handle_tool_error. </p> <p>To map the tools to OpenAI function call there is a module called: <code>from langchain_core.utils.function_calling import convert_to_openai_function</code>.</p> <p>It may be interesting to use embeddings to do tool selection before calling LLM. See this code agent_wt_tool_retrieval.py The approach is to dynamically select the N tools we want at run time, without having to pass all the tool definitions within the context window. It uses a vector store to create embeddings for each tool description.</p>"},{"location":"coding/langchain/#how-tos","title":"How Tos","text":"How to trace the agent execution? <p><pre><code>import langchain\nlangchain.debug = True\n</code></pre> Or use LangSmith</p> Defining an agent with tool calling, and the concept of scratchpad <p>Define  an agent with 1/ a user input, 2/ a component for formatting intermediate steps (agent action, tool output pairs) (<code>format_to_openai_tool_messages</code>: convert (AgentAction, tool output) tuples into FunctionMessages), and 3/ a component for converting the output message into an agent action/agent finish:</p> <pre><code># x is the response from LLM \nagent = (\n        {\n            \"input\": lambda x: x[\"input\"],\n            \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n                x[\"intermediate_steps\"]\n            ),\n             \"chat_history\": lambda x: x[\"chat_history\"],\n        }\n        | prompt\n        | llm_with_tools\n        | OpenAIToolsAgentOutputParser()\n    )\n</code></pre> <p>OpenAIToolsAgentOutputParser used with OpenAI models, as it relies on the specific tool_calls parameter from OpenAI to convey what tools to use.</p> How to support streaming the LLM's output? <p>LangChain streaming  is needed to make the app more responsive for end-users. All Runnable objects implement a sync method called <code>stream</code> and an <code>async</code> variant called <code>astream</code>. They cut output into chunks and yield them. Recall yield is a generator of data and acts as <code>return</code>. The main demo code is web_server_wt_streaming with the client_stream.py</p> Example of Intended Model <p>to be done</p> Example of Supports Multi-Input Tools <p>to be done</p> Use a vector store to keep the list of agent and description <p>As we cannot put the description of all the tools in the prompt (because of context length issues)  so instead we dynamically select the N tools we do want to consider using, at run time. See the code in agent_wt_tool_retrieval.py. </p>"},{"location":"coding/langchain/#langchain-expression-language-lcel","title":"LangChain Expression Language (LCEL)","text":"<p>LCEL supports streaming the LLM results, use async communication, run in parallel, retries and fallbacks, access intermediate results.</p>"},{"location":"coding/langgraph/","title":"LangGraph","text":"<p>Updates</p> <p>Created 04/2024 - Update 09/21/2024</p> <p>LangGraph is a library for building stateful, multi-actor applications, and being able to add cycles to LLM app. It is not a DAG. </p> <p>Single and multi-agent flows are described and represented as graphs.</p>"},{"location":"coding/langgraph/#value-propositions","title":"Value propositions","text":"<ul> <li>Build stateful, multi-actor applications with LLMs</li> <li>Coordinate multiple chains or actors across multiple steps of computation in a cyclic manner</li> <li>Build plan of the actions to take</li> <li>Take the actions</li> <li>Observe the effects</li> <li>Support persistence to save state after each step in the graph. This allows human in the loop pattern</li> <li>Support Streaming agent tokens and node transitions</li> </ul>"},{"location":"coding/langgraph/#concepts","title":"Concepts","text":"<p>States may be a collection of messages or custom states as defined by a TypedDict schema. States are passed between nodes of the graph.  MessageState is a predefined state to include the list of messages.</p> <p><code>Nodes</code> represent units of work.  It can be either a function or a runnable. Each node updates the internal graph state and returns it after execution.</p> <p><code>Graph</code> defines the organization of the node workflow. Graphs are immutable so are compiled once defined:</p> A simple call LLM graph<pre><code>graph = MessageGraph()\n\ngraph.add_node(\"chatbot\", chatbot_func)  # (1)\ngraph.add_edge(\"chatbot\", END)\n\ngraph.set_entry_point(\"chatbot\")\n\nrunnable = graph.compile()\n</code></pre> <ol> <li>chatbot_func is a function to call a LLM.  <code>add_node()</code> takes a function or runnable, with the input is the entire current state:</li> </ol> <p>See FirstGraphOnlyLLM.py</p> <pre><code>def call_tool(state):  # (1)\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    #...\n</code></pre> <ol> <li>The State of the graph, in this case, includes a list of messages</li> </ol> <p>Conditional edge between nodes, helps to build more flexible workflow: based on the output of a node, one of several paths may be taken. Conditional edge use function to decide where to route according to the last message content.</p> <pre><code>workflow.add_conditional_edges(\n                \"agent\",\n                self.should_continue,\n                {\n                    \"continue\": \"action\",\n                    \"end\": END,\n                },\n            )\n</code></pre>"},{"location":"coding/langgraph/#agents","title":"Agents","text":"<p>Graphs helps implementing Agents as AgentExecutor is a deprecated API. They most likely use tools. The graph development approach is:</p> <ol> <li>Define the tools to be used</li> <li>Define the state and what needs to be persisted</li> <li> <p>Define the workflow as a graph, and the persistence mechanism to use when needed, compile the graph into a LangChain Runnable. Once the graph is compiled, the application can interact with the graph via stream or invoke methods.</p> <pre><code>app = workflow.compile(checkpointer=checkpointer)\n</code></pre> </li> <li> <p>Invoke the graph as part of an API, an integrated ChatBot, using a dict including the parameter of the State...</p> </li> </ol> <p>Graphs such as StateGraph's naturally can be composed. Creating subgraphs lets developers build things like multi-agent teams, where each team can track its own separate state.</p> <p>LangGraph comes with built-in persistence, allowing developers to save the state of the graph at a given point and resume from there MemorySaver, Postgresql, SqliteSaver.</p> <pre><code>from langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\napp = workflow.compile(checkpointer=memory, interrupt_before=[\"action\"])\n</code></pre> <p>When using checkpointing the invoke method needs to get a configuration parameter with a unique thread_id to group messages and checkpoints in the context of this thread_id:</p> <pre><code>thread = {\"configurable\": {\"thread_id\": \"2\"}}\nfor event in app.stream({\"messages\": inputs}, thread, stream_mode=\"values\"):  # (1)\n</code></pre> <ol> <li>Call the graph using streaming do event are yielded.</li> </ol> <p>See other checkpointer ways to persist state, AsyncSqliteSaver is an asynchronous checkpoint saver that stores checkpoints in a SQLite database or SqliteSaver for synchronous storage is SQLlite.</p> <pre><code>memory = AsyncSqliteSaver.from_conn_string(\"checkpoints.sqlite\")\n</code></pre> <ul> <li>See first basic program or the one with tool to call Tavily tool for searching recent information about the weather in San Francisco using OpenAI LLM. (it is based on the tutorial). It does not use any prompt, and the call_method function invokes OpenAI model directly.</li> </ul>"},{"location":"coding/langgraph/#invocation-and-chat-history","title":"Invocation and chat history","text":"<p>The LangGraph's <code>MessageState</code> keeps an array of messages. So the input is a dict with the \"messages\" key and then a HumanMessage, ToolMessage or AIMessage. As graphs are stateful, it is important to pass a thread_id, which should be unique per user's chat conversation.</p> <pre><code>app.invoke(\n    {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n    config={\"configurable\": {\"thread_id\": \"42\"}}, debug=True\n)\n</code></pre> <p>Some code using chat_history:</p> <ul> <li> <p>A simple version with tool and memory using prebuilt LangGraph constructs FirstGraphWithToolAndMemory.py</p> </li> <li> <p>Close Question with a node creating a close question and then processes the outcome with llm.</p> </li> </ul> <p></p> <p>The LLM execution trace presents the following content:</p> <pre><code> Entering LLM run with input:\n{\n  \"prompts\": [\n    \"Human: what is the weather in sf\"\n  ]\n}\n</code></pre> <p>The LLM is generating some statements that tool calling is needed by matching to the tool name specified (e.g. <code>tavily_search_results_json</code>) during LLM creation (with the args coming from the function signature or for a schema definition as part of the tool definition). Below is an example of OpenAI tool_calls response. Most LLMs support this schema:</p> <pre><code>\"generations\": [\n    [\n      {\n        \"text\": \"\",\n        \"generation_info\": {\n          \"finish_reason\": \"tool_calls\",\n          ...\n        \"tool_calls\": [\n            {\n            \"name\": \"tavily_search_results_json\",\n            \"args\": {\n                \"query\": \"weather in San Francisco\"\n            },\n            \"id\": \"call_Vg6JRaaz8d06OXbG5Gv7Ea5J\"\n            }\n</code></pre> <p>Graph cycles the steps until there are no more <code>tool_calls</code> within the AIMessage: 1/ If AIMessage has tool_calls, \"tools\" node executes the matching function, 2/ the \"agent\" node executes again and returns AIMessage. Execution progresses to the special <code>END</code> value and outputs the final state</p> <p>Adding a \"chat memory\" to the graph with LangGraph's checkpointer to retain the chat context between interactions.</p>"},{"location":"coding/langgraph/#tool-calling","title":"Tool Calling","text":"<p>The Graph must include <code>ToolNode</code> to call the slected function or tool which can be called via conditions on edge. The following declaration uses the predefined langchain tool definition of TavilySearch. The <code>TavilySearchResults</code> has function name, argument schema and tool definition so the prompt sent to LLM has information about the tool like: \"name\": \"tavily_search_results_json\"</p> <pre><code>from langchain_community.tools.tavily_search import TavilySearchResults\ntools = [TavilySearchResults(max_results=2)]\ntool_node = ToolNode(tools)\n</code></pre>"},{"location":"coding/langgraph/#tool-calling-with-mistral","title":"Tool calling with Mistral","text":"<p>See this product documentation adapted to langgraph in this code and this new LangGraph API with ToolNode, and ChatMistral with bind_tools.</p>"},{"location":"coding/langgraph/#use-cases","title":"Use cases","text":"<p>The interesting use cases for LangGraph are:</p> <ul> <li>workflow with cycles and conditional output</li> <li>planning agent for plan and execute pattern</li> <li>using reflection and self critique</li> <li>multi agent collaboration, with or without supervisor</li> <li>human in the loop (by adding an \"interrupt\" before a node is executed.)</li> <li>controlled tool calling with or without LLM output</li> </ul>"},{"location":"coding/langgraph/#reasoning-and-acting-react-implementation","title":"Reasoning and Acting (ReAct) implementation","text":"<p>See this paper: A simple Python implementation of the ReAct pattern for LLMs from Simon Willison, and the raw code implementation using openAI API code: ReAct.py. LangGraph uses a prebuilt implementation of ReAct that can be tested by PreBuilt_ReAct_lg.py  or the implementation of ReAct pattern using LangGraph.</p> <p>An interesting prompt to use in the ReAct implementation is hwchase17/react.</p> <p>It is possible to interrupt before or after a node.</p> <pre><code>graph = create_react_agent(model, tools=tools, interrupt_before=[\"tools\"],  checkpointer=memory)\nprint_stream(graph, inputs, thread)\n# the graph ended because of the interrupt\nsnapshot = graph.get_state(thread)  # got where it was stopped\n</code></pre> <p>See The most simple ReAct with Mistral Model</p>"},{"location":"coding/langgraph/#adaptive-rag","title":"Adaptive RAG","text":"<p>The code adaptive_rag.py is implementing the following graph as documented in this sample from product documentation: </p> <p></p> <p>The documents processing and vector store creation is done in separate function outside of the graph.</p> <p>Some interesting patterns from this sample:</p> <ul> <li>One Agent to route the query, with dedicated prompt </li> <li>retriever from vector store with an agent to grade the retrieved documents</li> <li>graph state includes question, retrieved documents </li> </ul>"},{"location":"coding/langgraph/#human-in-the-loop","title":"Human in the loop","text":"<p>The human is the loop can be implemented in different ways:</p> <ul> <li>Add a confirmation before invoking a tool, using the the interrupt_before the names of the tool. See human_in_loop.py</li> <li>Implementing a human node before which the graph will always stop ask_human_graph.py</li> </ul> <p>See prompt_builder_graph which is also integrated with Taipy UI in Taipy UI with a langgraph graph</p>"},{"location":"coding/langgraph/#other-code","title":"Other Code","text":"<p>See Langgraph code samples with interesting patterns, but with some code not following the last updates in the APIs and my own sample folder. </p> <p>See the owl agent framework open source project to manage assistant, agents, tools, prompts..</p>"},{"location":"coding/langgraph/#code-faq","title":"Code FAQ","text":"prompt variables to be integrated in LangGraph <p>The graph state should include variables used in the prompts used in the agents.</p> Streaming the output? <p>Each node with LLM agent needs to call an async function.</p> <pre><code>async def call_agent(state: State)\n  # ... process the state\n  response = await model.ainvoke(messages)\n</code></pre> <p>Once the graph is created, the application needs to invoke it with an <code>async for</code></p> <pre><code>async def text_chat(graph):\nconfig = {\"configurable\": {\"thread_id\": 1}}\n\nwhile True:\n    user_msg = input(\"User (q/Q to quit): \")\n    if user_msg in {\"q\", \"Q\"}:\n        print(\"AI: Byebye\")\n        break\n    async for event in graph.astream_events({\"messages\": [HumanMessage(content=user_msg)]}, version=\"v1\"):\n        ... process the event payload\n</code></pre> <p>See stream_agent_node.py and the one with a simple UI with websocket and langgraph</p> How to do close question? How to do classification of intent? <p>Use a system prompt with possible classification values, and one agent in one node of the graph. Then in the conditional edge function, test to the different value to branch in different paths.</p>"},{"location":"coding/langgraph/#deeper-dive","title":"Deeper dive","text":"<ul> <li>LangGraph product reference documentation.</li> <li>LangGraph git repository</li> <li>LangGraph API reference guide</li> <li>Deeplearning.ai AI Agents in LangGraph with matching code </li> <li>A simple Python implementation of the ReAct pattern for LLMs</li> </ul>"},{"location":"coding/llama-index/","title":"LlamaIndex library","text":"<p>LlamaIndex is a framework for building context-augmented LLM applications, like Q&amp;A, chatbot, Document understanding and extraction, and agentic apps.</p> <p>It includes data connectors, indexes, NL engines, Agents, and a set of tools for observabiltiy and evaluation.</p> <p>LlamaParse is an interesting document parsing solution. LlamaHub</p>"},{"location":"coding/llama-index/#getting-started","title":"Getting Started","text":"<ul> <li>Routing queries example using RAG router_engine.py.</li> </ul>"},{"location":"coding/oldnotes/","title":"Oldnotes","text":""},{"location":"coding/oldnotes/#run-my-python-development-shell","title":"Run my python development shell","text":"<p>As some of the python codes are using matplotlib and graphics, it is possible to use the MAC display  with docker and X11 display (see this blog) for details, which can be summarized as:</p> <ul> <li>By default XQuartz will listen on a UNIX socket, which is private and only exists on local filesystem. Install <code>socat</code> to create a two bidirectional streams between Xquartz and a client endpoints: <code>brew install socat</code>.</li> <li>Install XQuartz with <code>brew install xquartz</code>. Then start Xquartz from the application or using: <code>open -a Xquartz</code>. A white terminal window will pop up.  </li> <li>The first time Xquartz is started, within the X11 Preferences window, select the Security tab and ensure the <code>allow connections from network clients</code> is ticked.</li> <li>Set the display using the ipaddress pod the host en0 interface: <code>ifconfig en0 | grep \"inet \" | awk '{print $2}</code> </li> </ul> <p>Then run the following command to open a two bi-directional streams between the docker container and the X window system of Xquartz.</p> <pre><code>source ./setDisplay.sh\n# If needed install Socket Cat: socat with:  brew install socat\nsocat TCP-LISTEN:6000,reuseaddr,fork UNIX-CLIENT:\\\"$DISPLAY\\\"\n</code></pre> <p>which is what the <code>socatStart.sh</code> script does.</p> <p>Start a docker container with the command from the root folder:</p> <pre><code>./startPythonDocker.sh\nroot@...$ cd /app/ml-python\n</code></pre> <p>Then navigate to the python code from the current <code>/app</code> folder, and call python</p> <pre><code>$ python deep-net-keras.py\n</code></pre>"},{"location":"coding/pandas/","title":"Pandas","text":"<p>Pandas is the primary tool data scientists use for exploring and manipulating data.</p> <p>Pandas uses <code>DataFrame</code> to hold the type of data a table which contains an array of individual entries, each of which has a certain value. Each entry corresponds to a row (or record) and a column.</p> <p>Pandas supports the integration with many file formats or data sources out of the box (csv, excel, sql, json, parquet,\u2026)</p> <pre><code>import pandas as pd\nmasses_data=pd.read_csv('mammographic_masses.data.txt',na_values=['?'],names= ['BI-RADS','age','shape','margin','density','severity'])\nmasses_data.head()\n</code></pre>"},{"location":"coding/pandas/#how-to","title":"How to","text":""},{"location":"coding/pandas/#work-on-the-data","title":"Work on the data","text":"<pre><code>masses_data.describe()\n# Search for row with no data in one of the column\nmasses_data.loc[masses_data['age'].isnull()]\n# remove such rows\nmasses_data.dropna(inplace=True)\n</code></pre>"},{"location":"coding/pandas/#transform-data-for-sklearn","title":"Transform data for Sklearn","text":"<pre><code>all_features = masses_data[['age', 'shape',\n                             'margin', 'density']].values\n\n\nall_classes = masses_data['severity'].values\n\nfeature_names = ['age', 'shape', 'margin', 'density']\n</code></pre>"},{"location":"coding/pandas/#sources","title":"Sources","text":"<ul> <li>Pandas getting started</li> <li>Kaggle's training on pandas.</li> </ul>"},{"location":"coding/pytorch/","title":"Pytorch library","text":"<p>The most popular Python ML and deep learning library to implement ML workflow and deep learning solution. It is open-source project. It helps to run code on GPU/TPU. PyTorch is also a low-level math library as NumPy, but built for deep learning. It compiles these compute graphs into highly efficient C++/CUDA code.</p> <p>The sources for this content is from product documentation, Zero to mastery - learning pytorch, and WashU training website.</p>"},{"location":"coding/pytorch/#environment-setup","title":"Environment setup","text":"<p>Use pip or  mini conda for package management and virtual environment management, and jupyter notebooks.</p>"},{"location":"coding/pytorch/#install","title":"Install","text":"<ul> <li> <p>Using Python 3 and pip3, use a virtual environment, install torch</p> <pre><code>pip3 install torch torchvision torchaudio\n</code></pre> </li> <li> <p>Using Anaconda:</p> <ol> <li> <p>Install miniconda (it is installed in ~/miniconda3): </p> <pre><code># under ~/bin\ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\nsh Miniconda3-latest-MacOSX-arm64.sh -u\n</code></pre> </li> <li> <p>Verify installed libraries: <code>conda list</code></p> </li> <li>Environments are created under <code>~/miniconda3/envs</code>. To create a conda environment named \"torch\", in miniconda3 folder do: <code>conda create anaconda python=3 -n torch</code></li> <li>To activate conda environment: <code>conda activate torch</code></li> <li>Install pytorch <code>conda install pandas pytorch::pytorch torchvision torchaudio -c pytorch</code></li> <li>[optional] Install jupyter packaging: <code>conda install -y jupyter</code></li> <li>Register a new runtime env for jupyter: <code>python -m ipykernel install --user --name pytorch --display-name \"Python 3.11 (pytorch)\"</code></li> </ol> </li> </ul>"},{"location":"coding/pytorch/#run-once-conda-installed","title":"Run once conda installed","text":"<ol> <li>To activate conda environment: <code>conda activate torch</code></li> <li>Test my first program: <code>python basic-torch.py</code> </li> <li>If we need Jupyter: <code>jupyter notebook</code> in the torch env, and then http://localhost:8888/tree.</li> <li>Select the Kernel to be \"Python 3.9 (pytorch)\"</li> </ol> <p>My code studies are in pytorch folder.</p>"},{"location":"coding/pytorch/#concepts","title":"Concepts","text":""},{"location":"coding/pytorch/#tensor","title":"Tensor","text":"<p>Tensor is an important concept for deep learning. It is the numerical representation of data, a n dimension matrix.</p> <p>Tensors are a specialized data structure, similar to NumPy\u2019s <code>ndarrays</code>, except that tensors can run on GPUs. </p> <pre><code>matrix1 = torch.tensor([[1, 1, 1], [1, 1, 2]], device=device, dtype=torch.float16)\n</code></pre> <p>Tensor attributes describe their shape, datatype, and the device on which they are stored.</p> <pre><code>import torch, numpy as np\nshape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\n</code></pre> <p>Tensor created from NumPy array:</p> <pre><code>import torch\nX= torch.from_numpy(X).type(torch.float)\ny= torch.from_numpy(y).type(torch.float)\nX[:5],y[:5]\n</code></pre> <p>Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other. See the set of basic operations on tensor.</p> <p>See the basic ML workflow using Pytorch to work on data and do a linear regression workflow-basic.ipynb.</p>"},{"location":"coding/pytorch/#constructs","title":"Constructs","text":"<p>PyTorch has two important modules we can use to create neural network: <code>torch.nn, torch.optim</code>, and two primitives to work with data: <code>torch.utils.data.DataLoader</code> and <code>torch.utils.data.Dataset</code>. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset. See dataset examples</p> <ul> <li>See basic code in basic-torch.py with inline explanations.</li> </ul> Modules Description <code>torch.nn</code> Contains all of the building blocks for computational graphs. <code>torch.nn. Parameter</code> Stores tensors that can be used with nn.Module. If requires_grad=True gradients descent are calculated automatically. <code>torch.nn.Module</code> The base class for all neural network modules. Need to subclass it. Requires a forward() method be implemented. <code>torch.optim</code> various optimization algorithms to tell the model parameters how to best change to improve gradient descent and in turn reduce the loss"},{"location":"coding/pytorch/#gpu","title":"GPU","text":"<p>On Linux or Windows with nvidia GPU, we need to use the Cuda (Compute Unified Device Architecture) library. See AWS deep learning container. For Mac, use <code>mps</code>.</p> <p>Here is sample code to set <code>mps</code> to access GPU (on Mac) for tensor computation:</p> <pre><code>has_mps = torch.backends.mps.is_built()\ndevice = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n</code></pre> <p>NumPy uses only CPU, so we can move to tensor and then tensor.to(device) to move the tensor to GPU, do computation and move back to NumPy</p> <pre><code>tensor=torch.tensor([1,2,3])\ntensor_on_gpu = tensor.to(device)\ntensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n</code></pre> <p>See Tim Dettmers's guide.</p>"},{"location":"coding/pytorch/#basic-algebra-with-pytorch","title":"Basic Algebra with Pytorch","text":"<p>See Algebra using Pytorch python code.</p>"},{"location":"coding/pytorch/#loss-functions","title":"Loss functions","text":"<p>Cost / loss functions selection depends on the problem to solve. </p> Loss function/Optimizer Problem type PyTorch module Stochastic Gradient Descent (SGD) optimizer Classification, regression, many others. torch.optim.SGD() Adam Optimizer Classification, regression, many others. torch.optim.Adam() Binary cross entropy loss Binary classification torch.nn. BCELossWithLogits or torch.nn.BCELoss Cross entropy loss Multi-class classification torch.nn.CrossEntropyLoss Mean absolute error (MAE) or L1 Loss Regression torch.nn.L1Loss Mean squared error (MSE) or L2 Loss Regression torch.nn.MSELoss <p>The binary cross-entropy / log loss is used to compute how good are the predicted probabilities. The function uses a  negative log probability for a label to be one of the expected class: {0,1}, so when a class is not 1 the loss function result is big.</p>"},{"location":"coding/pytorch/#neural-network","title":"Neural network","text":"<p>A PyTorch neural network declaration is a class that extends <code>nn.Module</code>. The constructor includes the neural network structure, and the class must implement the <code>forward(x)</code> function to pass the input to the network and get the output. The back propagation is done using SGD. This is the most flexible way to declare a NN. As an alternate the following code uses the Sequential method using the non linear (nn.ReLu()) function between layers.</p> <pre><code>model = nn.Sequential(\n    nn.Linear(x.shape[1], 50),\n    nn.ReLU(),\n    nn.Linear(50, 25),\n    nn.ReLU(),\n    nn.Linear(25, 1)\n).to(device)\n</code></pre> <p>Neural network has an input layer with # of parameters equal to the number of input features, and the number of output equal to the number of expected responses (1 output for binary classification). The first layer above, is a linear transformation to the incoming data (x): </p> <p> </p> <p>50 is the number of parameters to the first hidden layer. For activation function between hidden layers, ReLU (max(0,x)) is often used when we want non-linearity. The output layer will not use a transfer function for a regression neural network, or use the logistic function for binary classification (just two classes) or log SoftMax for two or more classes.</p> <p>The hyper-parameters to tune are:</p> <ul> <li> <p>The number of neuron in hidden layer: In general, more hidden neurons means more capability to fit complex problems. But too many, will lead to over fitting. Too few, may lead to under fitting the problem and will sacrifice accuracy.</p> </li> <li> <p>The number of layers: more layers allow the neural network to perform more of its feature engineering and data preprocessing.</p> </li> <li>The activation function between hidden layers and for the output layer.</li> <li>The loss and optimizer functions.</li> <li>The learning rate of the optimization functions</li> <li>Number of epochs to train the model. An epoch as one complete pass over the training set.</li> </ul> <p>For multi class training, <code>LogSoftmax</code> is used as transfer function and <code>CrossEntropyLoss</code> as loss function. With Softmax, the outputs are normalized probabilities that sum up to one.</p> <p>Some code samples:</p> <ul> <li>Basic NN in dual class classifier notebook to identify plots on 2 circles.</li> <li>Multi-class classifier notebook</li> <li>Python code for a PyTorch neural network for a binary classification on (Sklearn moons dataset) using Loss : nn-classifier.py.</li> <li>Computer vision and the CNN A notebook and Python code</li> </ul> <pre><code>{'model_name': 'FashionMNISTModel', 'model_loss': 0.41334256529808044, 'model_acc': tensor(0.8498, device='mps:0')}\n{'model_name': 'FashionNISTCNN', 'model_loss': 0.3709910213947296, 'model_acc': tensor(0.8716, device='mps:0')}\n</code></pre>"},{"location":"coding/pytorch/#model-training","title":"Model training","text":""},{"location":"coding/pytorch/#pytorch-training-loop","title":"PyTorch training loop","text":"<p>For the training loop, the steps to build:</p> Step What does it do? Code example Forward pass The model goes through all of the training data once, performing its forward() function calculations. model(x_train) Calculate the loss The model's predictions are compared to the ground truth and evaluated to see how wrong they are. loss = loss_fn(y_pred, y_train) Zero gradients The optimizers gradients are set to zero to be recalculated for the specific training step. optimizer.zero_grad() Perform back propagation on the loss Computes the gradient of the loss with respect for every model parameter to be updated (each parameter with requires_grad=True) loss.backward() Update the optimizer (gradient descent) Update the parameters with requires_grad=True with respect to the loss gradients in order to improve them. optimizer.step() <p>Example of code for training on multiple epochs:</p> <pre><code>loss_fn=nn.CrossEntropyLoss()\noptimizer= torch.optim.SGD(params=model.parameters(), lr=0.1)\n\nfor epoch in range(epochs):\n    model.train()\n    # 1. Forward pass\n    y_logits = model(X_train).squeeze()\n    # from logits -&gt; prediction probabilities -&gt; prediction labels\n    y_pred = torch.softmax(y_logits,dim=1).argmax(dim=1)\n    loss = loss_fn(y_logits,y_train)\n    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>The rules for performing inference with PyTorch models:</p> <pre><code>model.eval()\n\n# 2. Use the inference mode context manager to make predictions\nwith torch.inference_mode():\n    y_preds = model(X_test)\n</code></pre> <p>See train_step function in engine.py</p>"},{"location":"coding/pytorch/#pytorch-testing-loop","title":"PyTorch testing loop","text":"<p>The typical steps include:</p> Step Description Code example Forward pass The model goes through all of the test data model(x_test) Calculate the loss The model's predictions are compared to the ground truth. loss = loss_fn(y_pred, y_test) Calculate evaluation metrics Calculate other evaluation metrics such as accuracy on the test set. Custom function <pre><code>model.eval()\nwith torch.inference_mode():\n    # 1. Forward pass\n    test_logits = model(X_test).squeeze() \n    test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n    # 2. Caculate loss/accuracy\n    test_loss = loss_fn(test_logits,y_test)\n    test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n</code></pre> <p>See test_step function in engine.py</p>"},{"location":"coding/pytorch/#improving-a-model","title":"Improving a model","text":"Model improvement technique What does it do? Add more layers Each layer potentially increases the learning capabilities of the model with each layer being able to learn some kind of new pattern in the data, more layers is often referred to as making the neural network deeper. Add more hidden units Similar to the above, more hidden units per layer means a potential increase in learning capabilities of the model, more hidden units is often referred to as making the neural network wider. Fitting for longer (more epochs) The model might learn more if it had more opportunities to look at the data. Changing the activation functions Some data just can't be fit with only straight lines, using non-linear activation functions can help. Change the learning rate Less model specific, the learning rate of the optimizer decides how much a model should change its parameters each step, too much and the model over corrects, too little and it doesn't learn enough. Change the loss function Different problems require different loss functions. Use transfer learning Take a pre-trained model from a problem domain similar to ours and adjust it to our own problem."},{"location":"coding/pytorch/#evaluate-classification-models","title":"Evaluate classification models","text":"<p>Classification model can be measured using the at least the following metrics (see more PyTorch metrics):</p> Metric name/ Evaluation method Definition Code Accuracy Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct. torchmetrics.Accuracy() or sklearn.metrics.accuracy_score() Precision Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0). torchmetrics.Precision() or sklearn.metrics.precision_score() Recall Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives. torchmetrics.Recall() or sklearn.metrics.recall_score() F1-score Combines precision and recall into one metric. 1 is best, 0 is worst. torchmetrics.F1Score() or sklearn.metrics.f1_score() Confusion matrix Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right. torchmetrics.classification.ConfusionMatrixhttps://lightning.ai/docs/torchmetrics/stable/classification/confusion_matrix.html or sklearn.metrics.plot_confusion_matrix() Classification report Collection of some of the main classification metrics such as precision, recall and f1-score. sklearn.metrics.classification_report()"},{"location":"coding/pytorch/#pytorch-datasets","title":"Pytorch datasets","text":"<p>PyTorch includes many existing functions to load in various custom datasets in the TorchVision, TorchText, TorchAudio and TorchRec domain libraries.</p> <p>See prepare_image_dataset.py to get food images from PyTorch vision.</p>"},{"location":"coding/pytorch/#data-augmentation","title":"Data augmentation","text":"<p>Data augmentation is the process of altering the data in such a way that this artificially increases the diversity of the training set.</p> <p>The purpose of torchvision.transforms is to alter the images in some way and turning them into a tensor, or cropping an image or randomly erasing a portion or randomly rotating them.</p> <p>Training a model on this artificially altered dataset hopefully results in a model that is capable of better generalization (the patterns it learns are more robust to future unseen examples).</p> <p>Researches show that random transforms (like <code>transforms.RandAugment()</code> and <code>transforms.TrivialAugmentWide()</code>) generally perform better than hand-picked transforms.</p> <p>We usually don't perform data augmentation on the test set. The idea of data augmentation is to artificially increase the diversity of the training set to better predict on the testing set.</p> <p>See also in PyTorch's Illustration of Transforms examples.</p>"},{"location":"coding/pytorch/#transfer-learning-for-image-classification","title":"Transfer learning for image classification","text":"<p>With transfer learning is to take an already well-performing model on a problem-space similar to the one to address and then to customize it.</p> <p>For custom data to go into the model, need to be prepared in the same way as the original training data that went into the model.</p> <p>PyTorch models has weights and we can get the transformers from the weight.</p> <pre><code>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\ntransformer= weights.transforms()\nmodel=torchvision.models.efficientnet_b0(weights=weights).to(device)\n</code></pre> <p>efficientnet_b0 comes in three main parts:</p> <ul> <li>Features: A collection of convolutional layers and other various activation layers to learn a base representation of vision data.</li> <li>avgpool: Takes the average of the output of the features layer(s) and turns it into a feature vector.</li> <li>classifier: - Turns the feature vector into a vector with the same dimensionality as the number of required output classes (ImageNet has 1000 classes, out_features=1000).</li> </ul> <p>The process of transfer learning usually freezes some base layers of a pre-trained model, typically the features section, and then adjusts the output layers (also called head/classifier layers) to suit our needs.</p>"},{"location":"coding/pytorch/#some-how-to","title":"Some How to","text":"How to set the device dynamically <pre><code>def getDevice():\n    if torch.backends.mps.is_available():\n        device = torch.device(\"mps\")\n    elif torch.backends.cuda.is_available():\n        device = torch.device(\"cuda\")\n    else: \n        device = torch.device(\"cpu\")\n    return device   \n</code></pre> How to save and load a model? <pre><code># saving using Pytorch\nMODEL_SAVE_PATH = MODEL_PATH / filename\ntorch.save(model.state_dict(), MODEL_SAVE_PATH)\n# Load is reusing the class declaration\nmodel=FashionNISTCNN(input_shape=1,hidden_units=10,output_shape=10)\nmodel.load_state_dict(torch.load(\"models/fashion_cnn_model.pth\"))\n</code></pre> Display the confusion matrix for a multiclass prediction <pre><code>def make_confusion_matrix(pred_tensor, test_labels, class_names):\n    # Present a confustion matrix between the predicted labels and the true labels from test data\n    cm = MulticlassConfusionMatrix(num_classes=len(class_names))\n    cm.update(pred_tensor, test_labels)\n    fig,ax = cm.plot(labels=class_names)\n    plt.show()\n</code></pre> Transform an image into a Tensor <p>Use torchvision.transforms module <pre><code>train_transformer=v2.Compose([v2.Resize((224,224)), v2.TrivialAugmentWide(num_magnitude_bins=31), v2.ToTensor()])\n</code></pre></p> How to get visibility into a neural network <pre><code>import torchinfo\ntorchinfo.summary()\n</code></pre>"},{"location":"coding/pytorch/#code-samples","title":"Code samples","text":"<ul> <li>Basic operations on tensor: my own notebook and Learn Pytorch introduction.</li> <li>Pytorch workflow for training and testing model</li> <li>Compute image classification on Fashion NIST images in pythons and use_fashion_cnn.pn</li> <li>Pizza, steak, sushi image classifier</li> </ul>"},{"location":"coding/pytorch/#resources","title":"Resources","text":"<ul> <li>PyTorch.org tutorial</li> <li>Udemy content repo</li> <li>Zero to mastery - learning pytorch</li> <li>The incredible pytorch:  curated list of tutorials, projects, libraries, videos, papers, books..</li> <li>Dan Fleisch's video: What's a tensor?</li> <li>How to train state of the art models using Torchvision - PyTorch blog.</li> <li>Jeff Heaton - Using Convolutional Neural Networks.</li> </ul>"},{"location":"coding/sklearn/","title":"Scikit-learn library","text":"<p>Choosing an appropriate classification algorithm for a particular problem task requires practice: each algorithm has its own quirks and is based on certain assumptions.The performance of a classifier, its computational\u00a0power as well as predictive power, depend heavily on the underlying data that are available for learning.</p> <p>The five main steps that are involved in training a machine learning algorithm can be summarized as follows:</p> <ol> <li>Selection of features.</li> <li>Choosing a performance metric.</li> <li>Choosing a classifier and optimization algorithm.</li> <li>Tune parameters</li> <li>Evaluating the performance\u00a0</li> </ol>"},{"location":"coding/sklearn/#installation","title":"Installation","text":"<pre><code>pip3 install pandas scikit-learn\n</code></pre>"},{"location":"coding/sklearn/#getting-started","title":"Getting started","text":"<p>The sklearn APIs offer a lot of classifier algorithms and utilities to support those steps. See classifier note for examples.</p> <p>For example the code below loads the predefined IRIS flower dataset, and select the feature 2 and 3, the petals length and width.\u00a0</p> <pre><code>from sklearn import datasets\niris=datasets.load_iris()\nX=iris.data[:,[2,3]]\ny=iris.target\n</code></pre> <p>The size of <code>X</code> is typically (n_samples, n_features). The target values <code>y</code> which are real numbers for regression tasks, or integers for classification. Both variables are numpy arrays.</p> <p>Using numpy unique function to assess the potential classes we got 3 integers representing each class.</p> <pre><code>print(np.unique(y))\n&gt;&gt;[0,1,2]\n</code></pre>"},{"location":"coding/sklearn/#splitting-data","title":"Splitting data","text":"<p>To evaluate how well a trained model performs on unseen data, we will further split the dataset into separate training and test datasets.</p> <pre><code>from sklearn.model_selection import train_test_split\n# Randomly split X and y arrays into 30% test data and 70% training set\u00a0\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.3, random_state = 0)\n</code></pre>"},{"location":"coding/sklearn/#scale-data","title":"Scale data","text":"<p>Many machine learning and optimization algorithms also require feature scaling for optimal performance.\u00a0<code>StandardScaler</code> estimated the parameters <code>mu</code> (sample mean) and <code>delta</code> (standard deviation) for each feature dimension from the training data. The <code>transform</code> method helps to standardize the training data using those estimated parameters: <code>mu</code> and <code>delta</code>.\u00a0\u00a0</p> <pre><code>from sklearn.preprocessing import StandardScaler\n# standardize the features for optimal performance of gradient descent\nsc=StandardScaler()\n# compute mean and std deviation for each feature using fit\nsc.fit(X_train)\n# transform the features\nX_train_std=sc.transform(X_train)\n# Note that we used the same scaling parameters to standardize the test set so\u00a0\n# that both the values in the training and test dataset are comparable to each other.\nX_test_std=sc.transform(X_test)\n</code></pre> <p>Using the training data set, create a Multi-layer Perceptron (MLP) with 40 iterations and eta = 0.1</p> <pre><code>from sklearn.linear_model import Perceptron\nppn=Perceptron(n_iter=40,eta0=0.1,random_state=0)\nppn.fit(X_train_std,y_train)\n</code></pre> <p>Having trained the model now we can run predictions.</p> <pre><code>from sklearn.metrics import accuracy_score\ny_pred=ppn.predict(X_test_std)\nprint('Misclassified samples: %d' % (y_test != y_pred).sum())\nprint(' Accuracy: %.2f' % accuracy_score( y_test, y_pred))\n</code></pre> <p>Scikit-learn also implements a large variety of different performance metrics that are available via the metrics module. For example, we can calculate the classification accuracy of the perceptron on the test set. </p> <p></p> <p>The Perceptron biggest disadvantage is that it never converges if the classes are not perfectly linearly separable.</p>"},{"location":"coding/sklearn/#pipeline","title":"Pipeline","text":"<p>Transformers and estimators (predictors) can be combined together into a single unifying object: a Pipeline.</p> <pre><code>from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\npipe = make_pipeline(StandardScaler(), LogisticRegression())\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\npipe.fit(X_train, y_train)\nprint(\n    \"Accuracy with Logistic regression is: \"\n    + str(accuracy_score(pipe.predict(X_test), y_test))\n)\n</code></pre>"},{"location":"coding/visualization/","title":"Visualization of the Data","text":""},{"location":"coding/visualization/#matplotlib","title":"Matplotlib","text":"<ul> <li>Getting Started, the sigmoid function play with plot in notebook. Or in python code to plot MNist fashion dataset.</li> <li>Code for decision boundaries plotting</li> </ul>"},{"location":"concepts/","title":"Important ML concepts","text":""},{"location":"concepts/#variance","title":"Variance","text":"<p>Variance measures the consistency (or variability) of the model prediction for a particular sample instance if we would retrain the model multiple times: if the training set is splitted in multiple subsets, the model can be trained with those subsets and each time the sample instance prediction is run, the variance is computed. If the variability is big, then the model is sensitive to randomness.</p>"},{"location":"concepts/#bias","title":"Bias","text":"<p>Bias measures how far off the predictions are from the correct values in general.\u00a0One way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization.\u00a0</p>"},{"location":"concepts/#common-performance-metrics-used","title":"Common Performance Metrics used","text":"<p>During hyperparameter tuning, several performance metrics can be used to evaluate the model's performance. </p> <ul> <li>Accuracy: measures the proportion of correct predictions over the total number of predictions. It is commonly used for classification problems when the classes are balanced.</li> <li> <p>Precision and Recall: Precision represents the ratio of true positive predictions to the total predicted positives, indicating the model's ability to correctly identify positive instances. Recall measures the ratio of true positive predictions to the total actual positives, indicating the model's ability to find all positive instances. Precision and recall are useful for imbalanced classification problems where the focus is on correctly identifying the positive class.</p> </li> <li> <p>F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a single metric that balances precision and recall, which is especially useful when both metrics are important.</p> </li> <li> <p>Mean Squared Error (MSE): MSE is commonly used as an evaluation metric for regression problems. It measures the average squared difference between the predicted and actual values. Lower MSE values indicate better performance.</p> </li> <li> <p>Root Mean Squared Error (RMSE): RMSE is the square root of MSE and provides a more interpretable metric, as it is in the same unit as the target variable. It is widely used in regression tasks.</p> </li> <li> <p>R-squared (R\u00b2): measures the proportion of the variance in the target variable that can be explained by the model. It ranges from 0 to 1, with higher values indicating better fit. R-squared is commonly used in regression problems.</p> </li> <li> <p>Area Under the ROC Curve (AUC-ROC): AUC-ROC is used for binary classification tasks. It measures the model's ability to distinguish between positive and negative instances across different classification thresholds. A higher AUC-ROC score indicates better performance.</p> </li> <li> <p>Mean Average Precision (MAP): MAP is often used for evaluating models in information retrieval or recommendation systems. It considers the average precision at different recall levels and provides a single metric to assess the model's ranking or recommendation performance.</p> </li> </ul>"},{"location":"concepts/#regularization","title":"Regularization","text":"<p>Regularization is a very useful method to handle collinearity (high correlation among features), filter out noise from data, and eventually prevent overfitting. It adds a penalty term to the regression equation.\u00a0For regularization to work properly, we need to ensure that all our features are on comparable scales. There are various regularization techniques available, such as L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization. Each technique adds a regularization term to the model's objective function, which penalizes certain model parameters and helps control their impact on the final predictions. Use regularization strength to determine the degree of influence the regularization term has on the model. It is typically controlled by a hyperparameter (e.g., lambda or alpha) that needs to be tuned.</p> <p>Higher regularization strength results in simpler models while reduced variance but possibly increased bias.</p> <p>Lasso regression tends to have a higher bias but lower variance compared to Ridge regression. This means that Lasso might underfit the data if the regularization parameter is too high, but it\u2019s more robust to multicollinearity.</p> L1 or Lasso regularization <p>L1 regularization adds a penalty term proportional to the absolute value of the coefficients. It adds the sum of the absolute values of the coefficients multiplied by a tuning parameter (alpha) to the loss function. Residual Sum of Squares (RSS), which represents the difference between the predicted and actual values  <pre><code># Lasso loss function\nRSS = np.sum((y - X.dot(weights)) ** 2)  # Residual Sum of Squares\npenalty = alpha * np.sum(np.abs(weights))  # L1 penalty term\nreturn RSS + penalty\n</code></pre> Due to its ability to eliminate variables, Lasso tends to produce sparse models, where only a subset of features has non-zero coefficients.</p> L2 or Ridge regularization <p>It adds a penalty term proportional to the square of the coefficients. <pre><code>RSS = np.sum((y - X.dot(weights)) ** 2)  # Residual Sum of Squares\npenalty = alpha * np.sum(weights ** 2)  # L2 penalty term\nreturn RSS + penalty\n</code></pre> Ridge regression only shrinks the coefficients towards zero but rarely sets them exactly to zero, hence it retains all variables in the model.</p> <p>See this demo code tp compare Lasso and Ridge.</p> elastic net regularization <p>Elastic net regression combines the ridge and lasso penalties to automatically assess relevance of the regularization techniques. The math is (r is the mixing ratio between ridge (r=1) and lasso (r=0) regression):</p> <p> </p> <p>Choosing values for alpha and l1_ratio can be challenging; however, the task is made easier through the use of cross validation.</p> <p>See this notebook to demonstrate Elastic Net Regularization in wine dataset.</p>"},{"location":"concepts/#fitting","title":"Fitting","text":"<p>Overfitting is a common problem in machine learning, where a model performs well on training data but does not generalize well to unseen data.\u00a0If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters that lead to a model that is too complex\u00a0given the underlying data.</p>"},{"location":"concepts/#how-to-deal-with-overfitting","title":"How to deal with overfitting?","text":"<p>A common technique of preventing overfitting is known as regularization.</p>"},{"location":"concepts/#methods-to-prevent-overfitting","title":"Methods to prevent overfitting","text":"Method to prevent overfitting What is it? Simplify the model If the current model is overfitting the training data, it may be too complicated. This means it's learning the patterns of the data too well and isn't able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer. Use data augmentation Data augmentation manipulates the training data in a way so that's harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data. Use transfer learning Transfer learning involves leveraging the pre-trained weights one model has learned to use as the foundation for the next task. We could use one computer vision model pre-trained on a large variety of images and then tweak it slightly to be more specialized for specific images. Use dropout layers Dropout layers randomly remove connections between hidden layers in neural networks, effectively simplifying a model but also making the remaining connections better. See torch.nn.Dropout(). Use learning rate decay Slowly decrease the learning rate as a model trains. The closer it gets, the smaller the steps. The same with the learning rate, the closer it gets to convergence, the smaller are weight updates. Use early stopping Early stopping stops model training before it begins to over-fit. The model's loss has stopped decreasing for the past n epochs, we may want to stop the model training here and go with the model weights that had the lowest loss (y epochs prior) <p>The decision boundary is the hypothesis that separate clearly the training set.</p> <p>Decreasing the factor of control of overfitting, C, means the weight coefficients are shrinking so leading to\u00a0overfitting. Around C=100 the coefficient values stabilize leading to good decision boundaries:</p> <p></p> <p>For C=100 we have now</p> <p></p>"},{"location":"concepts/#dealing-with-under-fitting","title":"Dealing with under-fitting","text":"<p>Model is under-fitting when it generates poor predictive ability because the model hasn't fully captured the complexity of the training data. To increase model's predictive power, we may look at different techniques:</p> Method Description Add more layers/units to the model Model may not have enough capability to learn the required patterns/weights/representations of the data to be predictive. Increase the number of hidden layers/units within those layers. Tweak the learning rate Perhaps the model's learning rate is too high. Trying to update its weights each epoch too much, in turn not learning anything.Try lowering the learning rate. Use transfer learning Transfer learning may also help preventing under-fitting. It involves using the patterns from a previously working model and adjusting them to our own problem. Train for longer time Train for a more epochs may result in better performance. Use less regularization By preventing over-fitting too much, it may to under-fit. Holding back on regularization techniques can help your model fit the data better. <p>Preventing overfitting and under-fitting is still an active area of machine learning research.</p>"},{"location":"concepts/maths/","title":"Mathematical foundations","text":""},{"location":"concepts/maths/#covariance","title":"Covariance","text":""},{"location":"concepts/maths/#correlation","title":"Correlation","text":""},{"location":"concepts/maths/#probability","title":"Probability","text":"<ul> <li>the probability of two independent events happening at the same time: </li> </ul> <ul> <li>the probability of two dependent events happening at the same time:</li> </ul> <ul> <li>the probability of disjoint events A and B (they are mutually exclusive) is:</li> </ul> <ul> <li>if A and B are not mutually exclusive:</li> </ul> <p>What is the probability that a card chosen from a standard deck will be a Jack or a heart? becomes P(Jack or Heart) = P(Jack) + P(heart) - P( jack of Hearts) = 16/52</p>"},{"location":"concepts/maths/#bayesian","title":"Bayesian","text":"<p>In machine learning, there are two main approaches: the Bayesian approach and the frequentist approach. The Bayesian approach is based on probability theory and uses Bayes' theorem to update probabilities based on new data. The frequentist approach, on the other hand, is based on statistical inference and uses methods such as hypothesis testing and confidence intervals to make decisions.</p> <p>Bayes theorem:</p> <p>The theorem works by taking into account the probability of events on their own as well as two events occurring in conjunction with each other. It helps to assess what causes events to occur and why. It then helps to predict future.</p> <p>The Bayes formula for the probability of having event A occuring knowing B occured:</p> <p></p> <p>The numerator is the probability of event B given event A multiplied by the probability of event A occurring on its own. Denominator is probability of event B occuring on its own.</p> <p></p> <p>The Bayesian approach is often used in situations where there is uncertainty and the data is complex, while the frequentist approach is often used in situations where there is a large amount of data and the relationships between the variables are well-defined.</p> <p>Bayesian analysis allows you to compare false negative and false positive rates against the likelihood of obtaining a true negative or true positive result.</p> <p>See the conditional probability notebook exercise to simulate the probability of buying thing knowing the age and previous buying data: <code>totals</code> contains the total number of people in each age group and <code>purchases</code> contains the total number of things purchased by people in each age group.</p>"},{"location":"concepts/maths/#data-distributions","title":"Data distributions","text":"<p>See this notebook presenting some python code on different data distributions like Uniform, Gaussian, Poisson. It can be executed in VScode using the pytorch kernel.</p>"},{"location":"concepts/maths/#normalization","title":"Normalization","text":"<p>Normalization of ratings means adjusting values measured on different scales to a notionally common scale, often prior to averaging.</p> <p>In statistics, normalization refers to the creation of shifted and scaled versions of statistics, where the intention is that these normalized values allow the comparison of corresponding normalized values for different  datasets in a way that eliminates the effects of certain gross influences, as in an anomaly time series.</p> <p>Feature scaling used to bring all values into the range [0,1]. This is also called unity-based normalization.</p> <p>)</p>"},{"location":"concepts/maths/#sigmoid-function","title":"Sigmoid function","text":"<p>The Sigmoid function has a S shaped curve, one of them being the logistic function, to change a real to a value between 0 and 1.</p> <p></p> <p>It is used as an activation function of artificial neuron. The logistic sigmoid function is invertible, and its inverse is the logit function:</p> <p></p> <p>P being a probability,  is the corresponding odds.</p>"},{"location":"concepts/skill/","title":"The data scientist skill set","text":"<p>The following items come from different sources, and grouped in question and answer format. The sources:</p> <ul> <li>See Udemy blog for what skills to have to become a data scientist.</li> <li>14 Data Science Projects From Beginner to Advanced Level.</li> <li>LinkedIn blogs and posts</li> </ul> <p>The five important categories are: 1/ Mathematics, 2/ Statistics, 3/ Python, 4/ Data visualization, 5/ Machine learning (including deep learning).</p> Explain supervised vs unsupervised <p>Supervised means; build a model from labeled training data. With unsupervised, developers do not know upfront the outcome variable. read more in this section &gt;&gt; </p> Explain the bias-variance tradeoff <p>Bias measures how far off the predictions are from the correct values in general. Variance measures the consistency (or variability) of the model prediction. One way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization. Read more  on variance &gt;&gt; . Perform cross-validation with hyperparameter tuning.</p> Provide examples of regularization techniques <p>The regularization techniques are L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization.  See regularization section.</p> Overfitting / underfitting <p>Overfitting is a common problem in machine learning, where a model performs well on training data but does not generalize well to unseen data. Read more in fitting section</p> What are the steps involved in the machine learning pipeline? <p>A typical machine learning pipeline involves several steps, which can be summarized as:</p> <ol> <li>Define the problem</li> <li>Collect and Prepare Data: Gather, clean and preprocess it to make it suitable for machine learning. This can include tasks such as data wrangling, feature engineering, and data splitting.</li> <li>Select a Model </li> <li>Train the Model: Use the training data to train the model, adjust the model's parameters to minimize the prediction errors.</li> <li>Evaluate the Model on test data using metrics such as accuracy, precision, recall, and F1 score.</li> <li>Optimize the Model by tuning hyperparameters, adding or removing features, or trying different models.</li> <li>Deploy the Model and monitor the model performance to maintain high-quality predictions over time.</li> </ol> Compare and contrast classification and regression algorithms <p>Classification algorithms are used to predict categorical labels or class labels. Regression algorithms are used to predict continuous values.</p> What are the evaluation metrics commonly used? <p>Accuracy, Precision  and recall, F1-Score, MSE, Root MSE, R-squared, Area Under the ROC Curve, MAP,... See this section</p> What is cross-validation, and why is it important in machine learning? <p>Split your data into training and validation sets. Use techniques like k-fold cross-validation to assess the model's performance across different hyperparameter values. Iterate through different regularization strengths and evaluate the model's performance metrics (e.g., accuracy, mean squared error) to find the optimal balance between bias and variance.</p> How does feature selection impact machine learning models? Discuss different feature selection methods. <p>Feature selection helps 1/ reducing overfitting, which occurs when there is noise in the training data, 2/ improve the interpretability of the model by identifying the most important features that contribute to the prediction, 3/ reduce the computational cost of training and using the model, 4/ improve the accuracy of the model by reducing the noise in the data.</p> <p>Different feature selection methods:</p> <ul> <li>Filter methods involve selecting features based on their statistical properties, such as correlation with the target variable or variance. These methods are computationally efficient and can be used as a preprocessing step before training the model.</li> <li>Wrapper methods involve selecting features based on their impact on the performance of the model. These methods use a search algorithm to find the subset of features that maximizes the performance of the model. Wrapper methods can be computationally expensive, but they can often lead to better performance than filter methods.</li> <li>Embedded methods learn which features are important while training the model. Examples of embedded methods include LASSO (Least Absolute Shrinkage and Selection Operator) and Ridge Regression.</li> <li>Ensemble methods involve combining multiple feature selection methods to select a subset of features. These methods can be more robust than individual methods and can lead to better performance. Examples of ensemble methods include recursive feature elimination and random forests.</li> </ul>"},{"location":"data/","title":"Data Management","text":"<p>Data is a fundamental element of every business and crucial for any AI/ML project. Data is our record of current state of the business, the history of what has happened, and is the base which enables us to predict what may happen in the future.</p> <p>However, on its own data doesn't do anything and to realise value from data we have to do something with it, we have to understand it, and act on it. One of the biggest and most complex challenges comes with managing data. Data is inert; it is not self-organized or even self-understandable. </p> <p>Therefore how do we manage the data? How do we organize an attach meaning so that the data can easily be used by the business, or a computer program?</p> <p>The DIKW pyramid, provides a simple visualisation of the value chain growing from data to Wisdom / integrated knowledge:</p> <ul> <li>Data is the base with the least amount of perceived usefulness.</li> <li>Information has higher value than data.</li> <li>Knowledge has higher value than information.</li> <li>Wisdom has the highest perceived value of all.</li> </ul> <p>To move up the value chain <code>data</code> requires something else such as a program, a machine, or even a person\u2014to add understanding to it so that it becomes <code>Information</code>.</p> <p>By organizing and classifying information, the value chain expands from data and information to be regarded as knowledge.</p> <p>At the top of the data value chain is Wisdom. Wisdom comes from a combination of inert data, which is the fundamental raw material in the modern digital age, combined with a series of progressive traits such as:</p> <ul> <li>perspective.</li> <li>context.</li> <li>understanding.</li> <li>learning.</li> <li>the ability to reason.</li> </ul>"},{"location":"data/#data-progression","title":"Data progression","text":"<p>Any AI/ML project includes the following phases to create this valuable knowledge:</p> <ul> <li>Collect.</li> <li>Organize.</li> <li>Analyze.</li> <li>Infuse.</li> </ul> <p>The AI solution progresses through the levels to infuse, a state of capability that means an enterprise has taken artificial intelligence beyond a science project. Infusion means that advanced analytical models have been interwoven into the essential fabric of an application or system whereby driving new or improved business capabilities.</p>"},{"location":"data/#collect-making-data-simple-and-accessible","title":"Collect \u2013 Making Data Simple and Accessible","text":"<p>The first step is <code>Collect</code>, a primitive action that serves as the first element towards making data actionable and to help drive automation, insights, optimization, and decision-making. Collect is an ability to attach to a data source \u2013 whether transient or persistent, real or virtual, and while being agnostic as to its actual location or its originating (underlying) technology. In linking to the DIKW pyramid we could say that, data lies below the first rung, recognizing the inert nature of data.</p> <p>Properties of data include:</p> <ul> <li>Structured, semi-structured, unstructured</li> <li>Proprietary or open</li> <li>In the cloud or on-premise</li> <li>Any combination above</li> </ul>"},{"location":"data/#organize-trusted-governed-analytics","title":"Organize \u2013 Trusted, Governed Analytics","text":"<p>The second step is <code>Organize</code> and is about how an enterprise can make data known, discoverable, usable, and reusable. The ability to organize is prerequisite to becoming data-centric. Additionally, data of inferior quality or data that can be misleading to a machine or end-user can be governed in such that any use can be adequately controlled. Ideally, the outcome of <code>Organize</code> is a body of data that is appropriately curated and offers the highest value to an enterprise. </p> <p>Organize allows data to be:</p> <ul> <li>Discoverable.</li> <li>Cataloged.</li> <li>Profiled.</li> <li>Categorized.</li> <li>Classified.</li> <li>Secured (e.g. through policy-based enforcement)</li> <li>A source of truth and utility</li> </ul>"},{"location":"data/#analyze-insights-on-demand","title":"Analyze \u2013 Insights On-Demand","text":"<p>The <code>Analyze</code> step is about how an organization approaches becoming a data-driven enterprise. Analytics can be human-centered or machine-centered. In this regard the initials AI can be interpreted as Augmented Intelligence when used in a human-centered context and Artificial Intelligence when used in a machine-centered context. Analyze covers a span of techniques and capabilities, from basic reporting and business intelligence to deep learning. </p> <p>Analyze, through data, allows to:</p> <ul> <li>Determine what has happened</li> <li>Determine what is happening</li> <li>Determine what might happen</li> <li>Compare against expectations</li> <li>Automate and optimize decisions</li> </ul>"},{"location":"data/#infuse-operationalize-ai-with-trust-and-transparency","title":"Infuse \u2013 Operationalize AI with Trust and Transparency","text":"<p>The last <code>Infuse</code> step is about how an enterprise can use AI as a real-world capability. Operationalizing AI means that models can be adequately managed which means an inadequately performing model can be rapidly identified and replaced with another model or by some other means. Transparency infers that advanced analytics and AI are not in the realm of being a dark art and that all outcomes can be explained. Trust infers that all forms of fairness transcend the use of a model. </p> <p><code>Infuse</code> allows data to be:</p> <ul> <li>Used for automation and optimization</li> <li>Part of a causal loop of action and feedback</li> <li>Exercised in a deployed model</li> <li>Used for developing insights and decision-making</li> <li>Beneficial to the data-driven organization</li> <li>Applied by the data-centric enterprise</li> </ul>"},{"location":"data/features/","title":"Feature Engineering","text":""},{"location":"data/features/#goals","title":"Goals","text":"<p>The goal of feature engineering is to make the data better suited to the problem at hand. Features are the measurable data points that a model uses to predict an outcome.</p> <p>We might perform feature engineering to:</p> <ul> <li>Improve a model's predictive performance.</li> <li>Reduce computational or data needs.</li> <li>Improve interpretability of the results.</li> </ul> <p>For a feature to be useful, it must have a relationship to the target that our model is able to learn. Linear models, for instance, are only able to learn linear relationships. When using a linear model, our goal is to transform the features to make their relationship to the target, linear.</p> <p>See the Kaggle's concrete compressive strength example with some related notebooks on feature engineering.</p> <p>Feature engineering includes tasks like:</p> <ul> <li>Locating features with the most potential.</li> <li>Creating new features.</li> <li>Assessing potential clusters to discover complex spatial relationship.</li> <li>Analyzing variations to discover new features.</li> <li>Defining encoding.</li> </ul>"},{"location":"data/features/#some-technics","title":"Some technics","text":""},{"location":"data/features/#missing-values","title":"Missing values","text":"<p>Remove rows with missing target:</p> <pre><code>X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n# To keep things simple, we'll use only numerical predictors\nX = X_full.select_dtypes(exclude=['object'])\n</code></pre> <p>Assess number of missing values in each column. Pandas offers capabilities to easily assess missing cell in a matrix: </p> <pre><code>missing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column &gt; 0])\n</code></pre> <p>Avoid dropping a column when there are some missing values, except if the column has a lot of them, and it does not seem to bring much more impact to the result.</p> <pre><code># Get names of columns with missing values\ncols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\n\n# Drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n</code></pre> <p>We can use different interpolation techniques to estimate the missing values from the training samples. One of the most common interpolation techniques is mean imputation, where we simply replace the missing value by the mean value of the entire feature column. This is done by using the sklearn `Imputer`` class.</p> <p>The Imputer class belongs to the so-called transformer classes in scikit-learn that are used for data transformation. The two essential methods of those estimators are fit and transform. The fit method is used to learn the parameters from the training data, and the transform method uses those parameters to transform the data. Any data array that is to be transformed needs to have the same number of features as the data array that was used to fit the model.</p> <pre><code>from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer(strategy='median')\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n</code></pre> <p>We can add a boolean column which will have cell set to true when a mean was assigned to a missing value. In some cases, this will meaningfully improve results.</p> <pre><code>cols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\n# Make copy to avoid changing original data (when imputing)\nX_train_plus = X_train.copy()\nX_valid_plus = X_valid.copy()\n\n# Make new boolean columns indicating what will be imputed\nfor col in cols_with_missing:\n    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\nimputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n</code></pre>"},{"location":"data/features/#categorical-variables","title":"Categorical variables","text":"<p>A categorical variable takes only a limited number of values. We need to preprocess them to numerical values.</p> <pre><code># Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n# drop categorial variable columns\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n</code></pre> <p>Ordinal encoding assigns each unique value to a different integer.</p> <pre><code>from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nlabel_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\nlabel_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n</code></pre> <p>When there are some categorical value in test set that are not in the training set, then  a solution is to write a custom ordinal encoder to deal with new categories, or drop the column.</p> <pre><code># first get all categorical columns\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_valid[col]).issubset(set(X_train[col]))]\n\n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n</code></pre> <p>One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data. One-hot encoding does not assume an ordering of the categories:</p> <pre><code>from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to existing numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n</code></pre> <p>For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset. For this reason, we typically will only one-hot encode columns with relatively low cardinality.  Then, high cardinality columns can either be dropped from the dataset, or we can use ordinal encoding.</p> <pre><code># code to get low cardinality\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() &lt; 10]\n</code></pre> <p>Remarks that Pandas offers built-in features to do encoding</p> <pre><code># One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)\n</code></pre>"},{"location":"data/features/#mutual-information","title":"Mutual information","text":"<p>The first step is to construct a ranking with a feature utility metric, a function  measuring associations between a feature and the target. Then we can select a smaller  set of the most useful features to develop initially. Mutual information is a lot like correlation in that it measures a relationship between  two quantities.  The advantage of mutual information is that it can detect any kind of relationship,  while correlation only detects linear relationships.</p> <p>The least possible mutual information between quantities is 0.0. When MI is zero,  the quantities are independent: neither can tell us anything about the other.</p> <p>It's possible for a feature to be very informative when interacting with other features,  but not so informative all alone. MI can't detect interactions between features.  It is a univariate metric.</p> <p>We may need to transform the feature first to expose the association.</p> <p>The scikit-learn algorithm for MI treats discrete features differently from continuous features.  Consequently, we need to tell it which are which. Anything that must have a float dtype is not discrete. Categoricals (object or categorial dtype) can be treated as discrete by giving them a label encoding.</p> <p>See example of mutual information in ml-python/kaggle-training/car-price/PredictCarPrice.py.</p>"},{"location":"data/features/#discovering-new-features","title":"Discovering new features","text":"<ul> <li>Understand the features. Refer to the dataset's data documentation.</li> <li>Research the problem domain to acquire domain knowledge: Research fields a variety of formulas for creating potentially useful new features. </li> <li>Study Kaggle's winning solutions</li> <li>Use data visualization. Visualization can reveal pathologies in the distribution of a feature or  complicated relationships that could be simplified</li> </ul> <p>The more complicated a combination is, the more difficult it will be for a model to learn.</p> <p>Data visualization can suggest transformations, often a \"reshaping\" of a feature through powers or  logarithms.</p> <p>Features describing the presence or absence of something often come in sets. We can aggregate  such features by creating a count. </p> <pre><code># creating a feature that describes how many kinds of outdoor areas a dwelling has\nX_3 = pd.DataFrame()\nX_3[\"PorchTypes\"] = df[[\n    \"WoodDeckSF\",\n    \"OpenPorchSF\",\n    \"EnclosedPorch\",\n    \"Threeseasonporch\",\n    \"ScreenPorch\",\n]].gt(0.0).sum(axis=1)\n</code></pre> <p>Here is an example on how to extract roadway features from the car accidents and compute the number of such roadway in each accident. </p> <pre><code>roadway_features = [\"Amenity\", \"Bump\", \"Crossing\", \"GiveWay\",\n    \"Junction\", \"NoExit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n    \"TrafficCalming\", \"TrafficSignal\"]\naccidents[\"RoadwayFeatures\"] = accidents[roadway_features].sum(axis=1)\n</code></pre> <p>Extract Category from a column with string like: <code>One_Story_1946_and_Newer_All_Styles</code></p> <pre><code>X_4['MSClass'] = df.MSSubClass.str.split('_',n=1,expand=True)[0]\n</code></pre> <p>Group transforms aggregates information across multiple rows grouped by some category.  With a group transform we can create features like: </p> <ul> <li>\"the average income of a person's state of residence,\"</li> <li>\"the proportion of movies released on a weekday, by genre.\"</li> </ul> <p>Using an aggregation function, a group transform combines two features: a categorical feature  that provides the grouping and another feature whose values we wish to aggregate. Handy methods include <code>mean, max, min, median, var, std, count</code>.</p> <pre><code>customer[\"AverageIncome\"] = (\n    customer.groupby(\"State\")  # for each state\n    [\"Income\"]                 # select the income\n    .transform(\"mean\")         # and compute its mean\n)\n</code></pre> <p>If we're using training and validation splits, to preserve their independence,  it's best to create a grouped feature using only the training set and then join it  to the validation set. </p> <pre><code># Create splits\ndf_train = customer.sample(frac=0.5)\ndf_valid = customer.drop(df_train.index)\n\n# Create the average claim amount by coverage type, on the training set\ndf_train[\"AverageClaim\"] = df_train.groupby(\"Coverage\")[\"ClaimAmount\"].transform(\"mean\")\n\n# Merge the values into the validation set\ndf_valid = df_valid.merge(\n    df_train[[\"Coverage\", \"AverageClaim\"]].drop_duplicates(),\n    on=\"Coverage\",\n    how=\"left\",\n)\n</code></pre>"},{"location":"genAI/","title":"Generative AI","text":"Updates <p>Created Aug 2023 - Updated 08/2024</p>"},{"location":"genAI/#introduction","title":"Introduction","text":"<p>Generative AI is a combination of neural network models to create new content (text, image, music, videos..) from a requesting query. Models are pre-trained on vast amounts of unlabeled data, using from 7B up to 500B of parameters. Current Gen AI models are based on the Transformer architecture.</p> <p>Gen AI applies well to different category of use cases: improve customer experiences, improve employee's productivity, help around creativity, and help optimizing business process (See also the Use Case section). </p>"},{"location":"genAI/#transformer-architecture","title":"Transformer Architecture","text":"<p>Transformer is a neural network used to generate the next word in the sentence using the best probability. This is what most chat application use to propose the next word sugestion we can select. If we just select the most likely words we got paragraph with no real meaning. The context of the text is lost at each word. Adding the self-attention mechanism to the transformer, it helps to weight the significance of different words by taking into account the previously seen context. </p> <p>The attention mechanism computes the similarity between tokens (from the embeddings of words) in a sequence. That way, the model builds an intuition of what the text is saying. The closer two words are in a vector space, the higher the attention scores they will obtain and the higher the attention they will give to each other (Recall the example of \"bank of the river\" vs  \"money in the bank\"). </p> <p>The transformer architecture looks like:</p> <p></p> <ul> <li>The tokenization step takes every word, prefix, suffix, and punctuation signs and assign a matching token</li> <li>Embedding to transform token to numerical vector</li> <li>Positional encoding consists of adding a sequence of predefined vectors to the embedding vectors of the words. This ensures we get a unique vector for every sentence, and sentences with the same words in different order will be assigned different vectors</li> <li>The attention component is added at every block of the feedforward network. It uses multi-head attention where several different embeddings are used to modify the vectors and add context to them. </li> <li>There is a large number of transformer blocks in the network. The architecture has the layers of transformers stacked on top of each other. Within each layer, there are feed-forward neural networks to process the data.</li> <li>The last step of a transformer is a softmax layer, which turns these scores into probabilities (that add to 1). The model returns result tokens which are then turned back into readable text.</li> </ul> <p>The models are trained on vast amounts (Terabytes) of text data like books, articles, websites etc.  This helps the model learn grammar, facts, reasoning abilities and even some level of common sense from the content. </p> <p>GPT-3 (Generative Pre-trained Transformer 3) breaks the NLP boundaries with training on 175B parameters. </p> <p>The training has two stages: Pre-training where the model attempts to predict the next word in a sentence using its own corpus, and fine tuning where the model can be tuned for specific tasks or content. During the pre-training process, the model automatically takes context into account from all the training data, and tracks relationships in sequential data, like the words in a sentence, to develop some understanding of the real world. </p> <p>The models are commonly referred to as foundation models (FMs).</p> <p>The unlabeled data used for pre-training is usually obtained by crawling the web and public sources.</p> <p>At inference time, the input text is tokenized into individual tokens which are fed into the model. </p> Difference between ML and LLM <ul> <li>Foundational Models can perform many tasks because they contain a large number of parameters that make them capable of learning complex concepts. Through their pre-training exposure to internet-scale unstructured data in all its various forms and myriad of patterns, FMs learn to apply their knowledge within a wide range of contexts.</li> <li>Regular models are trained for one specific task, like image classification or speech recognition. ML models require lots of labeled data relevant to their task.</li> </ul> <p>The largest pre-trained model in 2019 (BERT) was 330M parameters while the state-of-the-art LLM in 2023 is 540B parameters.</p> <p>A transformer-based model has an encoder component that converts the input text into embeddings, and a decoder component that consumes these embeddings to emit some output text. Transformers process the entire input all at once, during the learning cycle, and therefore can be parallelized.</p> <p>Three types of transformer:</p> <ol> <li>Encoded only: generate no human readable content, used when applications need to efficiently query content to find similar items.</li> <li>Encoder-decoder model is trained to treat every natural language processing (NLP) problem (e.g., translate an input string in one language to another) as a text-to-text conversion problem.</li> <li>Decoder-only model is for text generation.</li> </ol> <p>Models with encoder-decoder and decoder-only architectures are generative models.</p> <p>The process is text -&gt; tokens (a token may be less than a word, and on average a 5 chars) -&gt; vector. Vectors of similar word are close in the multi-dimensional space. A vector, in NLP, has a lot of dimensions, representing its characteristics in the world of meaning. The best tokenization method for a given dataset and task is not always clear, and different methods have their own strengths and weaknesses. Sub-word tokenization combines the benefits of character and word tokenization by breaking down rare words into smaller units while keeping frequent words as unique entities.</p>"},{"location":"genAI/#huggingface-transformer","title":"HuggingFace Transformer","text":"<p>HuggingFace Transformer provides thousands of pre-trained models to perform tasks on text, images and audio.</p>"},{"location":"genAI/#pre-training-process","title":"Pre-training process","text":"<p>The goal of pre-training is to teach the model the structure, patterns and semantics of the human language. The pre-training process for GPT-3 involves collecting and preprocessing vast amounts of diverse text data, training a Transformer-based model to predict the next token in a sequence, and optimizing the model using powerful computational resources.</p> <p>Corpus = a collection of texts, and a vocabulary is the set of unique tokens found within the corpus. Corpus needs to be large and with high quality data.</p> <p>The process looks like in the figure below:</p> <p></p> <p>For the data collection, it is import to get diverse source of data, including web sites, books, curated datasets to address wide range of topics, writing styles and linguistic nuances. Data preparation is still key, but complex as to remove low-quality text, harmful content... As part of this preparation, text can be converted to lowercase to reduce variability. Tokenization helps to handle rare words and different languages.</p> <p>The primary objective during pre-training is to predict the next token in a sequence. This is a form of unsupervised learning where the model learns from the context provided by preceding tokens. </p> <p>The training phase includes the forward pass where input tokens go through the transformer layers. The loss calculation is computing the difference between predicted token and actual next token. Finally the backward pass applies gradients computation to minimize the loss, and tune the model parameters.</p> <p>The entire dataset is split into batches, and the model is trained over multiple epochs.</p> <p>The optimization phase includes tuning hyper parameters like learning rate and batch size. To be able to scale we need to run training on distributed computers.</p> <p>A portion of the data is set aside as a validation set to monitor the model's performance and prevent overfitting.</p> <p>Perplexity is a common metric used to evaluate language models, measuring how well the model predicts a sample</p>"},{"location":"genAI/#generic-development-approach","title":"Generic development approach","text":"<p>Some ways to use Generative AI in business applications:</p> <ul> <li>Build foundation model from scratch: very expensive and time consuming, with highly skilled ML scientists.</li> <li>Reuse existing foundation models available as open-source (hundred of model on Hugging Face hub), then add own corpus on top of it, to fine tune the model for better accuracy.</li> <li>Use generative AI services or APIs offered by foundation model vendors. There is no control over the data, cost and customization. Use prompt engineering or RAG techniques to get better answers, .</li> </ul> Hugging Face <p>Hugging Face is an open-source provider of natural language processing (NLP), which makes it easy to add state of the art ML models to applications. We can deploy and fine-tune pre-trained models reducing the time it takes to set up and use these NLP models from weeks to minutes.</p>"},{"location":"genAI/#model-fine-tuning","title":"Model fine-tuning","text":"<p>The Huggingface LLM leader board is a good source of information for model quality assessments relative to certain use cases.</p> <p>See this detailed article on developing a LLM and this one on fine tuning.</p>"},{"location":"genAI/#use-cases","title":"Use cases","text":"<p>We can group the Generative AI use cases in different categories:</p> Improve customer experiences <ul> <li>Chatbot functionality with context, with better user's experiences. Reduce operational costs using automated response.</li> <li>Documentation summarization: See model like Jurassic-2 Jumbo from AI21 studio, claude-v2 works well too.</li> <li>Personalization</li> </ul> Improve employee productivity <ul> <li>Code generation</li> <li>Translation, reports, summarization...</li> <li>Search via Q&amp;A Agent for specific subject, based on Corporate document processing. LLM helps understanding the text and the questions. The LLM is enriched, trained on proprietary corpus:</li> </ul> <p></p> <ul> <li>Self service tutor based on student progress, prompt activities, and respond to questions</li> <li>Personalized learning path generation</li> <li>Low-code development with GenAI agents</li> </ul> Creativity <ul> <li>Auto-generation of marketing material</li> <li>Personalized emails</li> <li>Sales scripts for customer's industry or segment</li> <li>Speeding the ideation phase of a product development</li> </ul> Business process optimization <ul> <li>Automatically extracting and summarizing data from documents: combine OCR with prompt to extract data and build json doc to be structured for downstream processing: Gen AI based intelligent document processing may looks like this:</li> </ul> <p></p> <ul> <li>Data augmentation to improve data set quality. Keep the privacy of original data sources, and help trains other models: generate image of rusted pumps to train an anomaly detection model on pumps.</li> <li>Propose some supply chain scenario</li> </ul> <ul> <li> <p>Uber has conducted Hackathon using genAI and identified that all Software Development Life Cycle phases are impacted by Generative AI usages, including: capturing complete specifications faster, explain existing code, generating UI code, automate code refactoring, unit test generation or e2e testing for mobile app, review code, code relationship map automatically created from call stacks, Pull Request and code review automation, code documentation generation based on PRs. But LLMs may generate buggy code, as well as spreading error-prone code pattern. </p> </li> <li> <p>Generative Adversarial Networks are used to limit the risk of adversarial manipulation in deep learning image recognition. It attempts to generate fake data that looks real by learning the features from the real data.</p> </li> </ul> <p>It would be difficult to find any business use-case where a base FM can be used effectively. Added techniques are needed to be useful in enterprise, like RAG, fine tuning, new training, knowledge graph and neuro-symbolic AI solutions.</p>"},{"location":"genAI/#industries","title":"Industries","text":"<p>These following industry-specific use cases present the potential applications of Generative AI:</p> <ol> <li> <p>Supply Chain Management:</p> <ul> <li>Improve visibility into multi-tier supplier performance concerns</li> <li>Identify potential risk areas within the supply chain</li> </ul> </li> <li> <p>Quality Control and Nonconformance Management:</p> <ul> <li>Identify the root cause of nonconformance issues</li> <li>Prescribe resolutions to address quality concerns</li> </ul> </li> <li> <p>Engineering Cost Optimization:</p> <ul> <li>Promote the reuse of common parts across different platforms to reduce costs</li> </ul> </li> <li> <p>Cross industry:</p> <ul> <li>Improve chatbot user's experience, with open responses more empathic to the user.</li> <li>Sentiment analysis: Gauge customer sentiment towards products, services, or brands</li> <li>Assist with proofreading tasks</li> <li>Update and maintain databases</li> <li>Analyze customer reviews</li> <li>Monitor social media platforms</li> </ul> </li> <li> <p>Education and Universities:</p> <ul> <li>Moderate and develop educational content</li> <li>Help students find the most effective pathways to graduation</li> </ul> </li> <li> <p>Safety and Risk Management:</p> <ul> <li>Identify potential safety risks, such as gas leaks</li> <li>Generate recommendations for remedial work to mitigate risks</li> </ul> </li> <li> <p>Travel Industry:</p> <ul> <li>Enhance trip planning with personalized recommendations, services, and offers</li> </ul> </li> <li> <p>Product Review Summarization:</p> <ul> <li>Offload the task of summarizing product reviews from humans to LLMs</li> <li>Add unstructured reviews as a new corpus for search functionality</li> <li>Separate reviews based on user-provided ratings</li> <li>Task an LLM to extract different sets of information from each high-level category of reviews</li> </ul> </li> </ol>"},{"location":"genAI/#classical-concerns-and-challenges","title":"Classical concerns and challenges","text":"<p>LLM's are amazing tools for doing natural language processing.   But they come with challenges due to the underlying training and inference technology, due to the fact that they are trained only occasionally and are thus always out of date, and also due to the fact that natural language generation is not grounded in any model of reality or reasoning but instead uses probabilistic techniques based on correlations of a huge number of strings of tokens (words). Which means hallucination and approximate retrieval are core of their architecture: the completion they are generating is in the same distribution as the text they have been trained on. Prompt engineering does not change hallucination as the decision to assess the response is a factual completion depends of the knowledge of the prompter and requires to continuously assess all the responses.</p> <ul> <li> <p>Accuracy: The accuracy of LLM's is not acceptable to any enterprise that must follow regulations and policies and respect contractual agreements with suppliers and customers. Because they cannot truly reason or take into account regulations and policies precisely, models often produce incorrect and contradictory answers when asked for decisions or actions to undertake. A single large language model is unlikely to solve every business problem effectively.  With classical ML, probabilistic output is expected. Symbolic approaches like business rules that precisely express policies produce reliable results at the cost of coding the policies mostly manually.</p> </li> <li> <p>Specificity: A single large model is unlikely to solve every business problem effectively because it is trained on generally-available information rather than enterprise-specific information. To differentiate their generative AI applications and achieve optimal performance, enterprises should rely on their own data sets tailored to their unique use case.   Even then, enterprise data changes constantly, so techniques such as RAG and tool calling are needed to leverage the most up-to-date and relevant information for a specific query.</p> </li> <li> <p>Cost and Risk of training and inference, as well as privacy and intellectual property are top concerns. LLM's can be \"fine-tuned\" for a specific task by using a small number of labeled examples specific to the company's industry or use case. Fine-tuned models can deliver more accurate and relevant outputs. But training and retraining models, hosting them, and doing inference with them are expensive. Cloud providers see this opportunity to sell more virtual servers with GPU's at a higher price. </p> </li> <li> <p>Skills: developing a new LLM may not make sense today, but fine tuning an existing model may in some circumstances. There are relatively few developers with expertise in model tuning, understanding their architecture and limitations, integrating them in applications, and in tuning their hyper parameters. Reinforcement learning to fine-tune existing LLM requires a huge number of trials, and data quality is still a very difficult and poorly-mastered topic.</p> </li> <li> <p>Reliability and reasoning: Generative AI models do not reason and do not plan accurately. New versions of LLMs attempt to improve in this domain, but by design the transformer architecture is probabilistic and greedy for text generation and does not inherently do any kind of structured symbolic reasoning or manage ontologies of concepts (knowledge graphs). LLM is a very big system-1 with their knowledge based from digital representation of humanity created content.</p> </li> <li> <p>For generative AI, the input is very ambiguous, but also the output: there is no determinist output.  Models produce incorrect and contradictory answers. With classical ML, output is well expected. Trained sentiment analysis algorithms on labelled data will perform better than any LLM for that task. Always try to assess when to use ML versus using an existing LLM.</p> </li> <li>There are a lot of models available today, each with unique strengths and characteristics. How to get the ones best suited for business needs? The size of the model is linked to the compute power developers have and how much they can pay (availability of the hardware is also an issue). Effectice mechanisms need to be set up to assess the LLM's responses for each different use cases.</li> <li>There is difficulty to determine which source documents are leading to the given answers. Model that links results to source, via citations help to assess for any hallucinations.</li> <li>Developers need to optimize for training and inference cost, then assess how to amortize the training cost to better evaluate what may be billed to end-users.</li> <li>Current models comparison is based on Multi-task Language Understanding on MMLU benchmark. But CIOs care less about standard results, they want models that work well on their data.</li> <li>For large enterprise, adopting LLM at scale means running hundreds, even thousands, of models at any time. A high frequency of innovation, leads customers to replace their models quicker than expected, reinforcing the need to train and deploy new models in production quickly and seamlessly.</li> <li>Cost being a major issue in short term, model may become smaller and access to powerful distributed smaller hardware will help to do inference locally (smartphone with TPU).</li> </ul>"},{"location":"genAI/#some-fallacies","title":"Some fallacies","text":"<ul> <li>LLMs can't do planning in autonomous modes. They may support planning activities done by planner software and human to translate formats, or to elaborate the problem specifications.</li> <li>Chain of Though, ReAct, Fine tuning do not help for planning as they do not generalize well</li> <li>There is no self-verification as LLM has no mean to do self verification</li> </ul>"},{"location":"genAI/#interesting-legal-considerations","title":"Interesting legal considerations","text":"<ul> <li>Think not created by a human could not be copyrighted.</li> <li>Model deployed will not use data sent to improve itself.</li> <li>The right to use an image/photo to train a model is a problem.</li> <li>Protect Intellectual Property: never pass confidential information to Gen AI SaaS based API.</li> <li>Protect the brand, avoid bias, discrimination, aligned to company values: any business decision should not be done by uncontrolled Gen AI.</li> </ul>"},{"location":"genAI/#discovery-assessment","title":"Discovery assessment","text":"<p>Classical questions to address before starting a Gen AI solutions:</p> <ul> <li>How to leverage open model like Mistral, LLama, DBRX?</li> <li>How to verify accuracy on unstructured data such as queries and document shunks? </li> <li>AI Models are becoming stronger when connected to data, enterprise data, and fine tuning. It is important to adopt a Gen AI strategy that is linked to the data strategy too. </li> <li>When transitioning to more stateful Gen AI-based solutions to make the model more specialized or better tuned for dedicated use cases, the model will require self-improvement capabilities. To support this, the solution will need to maintain states, which will be persisted using Lake House technology</li> <li>Current main stakeholder for Gen AI, is the developer, moving to business users will be challenging as some of their jobs are at risk, they may reject the technology at a all.</li> <li>How to stop doing a lot of prompt engineering and start doing model fine tuning? Always address what we should evaluate the solution on. Gen AI can help to build prompt with the use of meta prompting techniques. </li> <li>How to measure hallucination where the models make up inaccurate responses that are not consistent with the training data.</li> <li>How to integrate AI capability in the enterprise business processes and decisions? How does human in the loop step can be added to the process?</li> <li>Whenever we want to teach an LLM to use a tool, we need enough annotated tool calls to fine tune the LLM. We can use in-context learning to create a model that annotates tool calls for the input query. Incorrect calls can be filtered by executing the tools and filtering the outputs based on the ground truth answer.</li> </ul>"},{"location":"genAI/#concepts","title":"Concepts","text":"<p>A LLM is part of the evolution of NLP as it is a trained deep learning model that understands and generates text in a human like fashion.</p>"},{"location":"genAI/#nlp-processing","title":"NLP processing","text":"<p>To process an input text with a transformer model, the text is tokenized into a sequence of words or part of words. These tokens are then encoded as numbers and converted into embeddings, which are vector-space representations of the tokens that preserve their meaning: for example a word dog will have 512 potential numerical attributes used to describe what is a dog. Below is a simple representation of the embedding in the 3 dimension space:</p> <p></p> <p>See the web site projector.tensorflow.org/</p> Embedding <p>See the Encord's guide to embeddings in machine learning and this section</p> <p>The encoder, in the transformer, transforms the embeddings of all the tokens into a context vector. Using this vector, the transformer decoder generates output based on clues. The decoder can produce the subsequent word. We can reuse the same decoder, but this time the clue will be the previously produced next-word. This process can be repeated to create an entire paragraph. This process is called auto-regressive generation.</p> <p>When processing text, the AI looks at a few tokens around each word to help understand the context. This surrounding group of tokens is called the context window. It is the sliding group of tokens around a word that provides contextual information to help the AI understand and generate natural language.</p> Context Window <p>A context window is the sliding group of tokens around a word that provides contextual information to help the AI understand and generate natural language.</p> <p>If the current word is \"apple\", the AI might look at a context window of the 5 tokens before and after it. So the context window could be: \"I ate a sweet red [apple] this morning for breakfast\". The tokens in the context window give the AI useful information about the current word. In this case, they indicate [apple] is probably a noun referring to the fruit. </p> <p>With a narrow context window, the AI has less context to ensure the content flows logically and coherently over a long text. Restrictive context windows can result in more generic, impersonal text. The model has less perspective to generate nuanced or creative content.</p> <p>Transformers do not need to code the grammar rules, they acquire them implicitly from big corpus.</p> <p>During the training process, the model learns the statistical relationships between words, phrases, and sentences, allowing it to generate coherent and contextually relevant responses when given a prompt or query.</p> <p>The techniques to customize LLM applications from simplest to more complex are:</p> <ul> <li>Zero-shot inference: allows a pre-trained LLM to generate responses to tasks that it hasn\u2019t been specifically trained for. In this technique, the model is provided with an input text and a prompt that describes the expected output from the model in natural language. </li> <li>Prompt engineering with zero-shot inference.</li> <li>Prompt engineering with few-shot inference: Few-shot learning involves training a model to perform new tasks by providing few examples. This is useful where limited labeled data is available for training.</li> <li>Retrieval augmented generation (more complex).</li> <li>Fine tune an existing foundation model.</li> <li>Pre-train an existing foundation model: example is domain specific model, like the Bloomberg's LLM.</li> <li>Build a foundation model from scratch.</li> <li>Support human in the loop to create high quality data sets.</li> </ul>"},{"location":"genAI/#important-terms","title":"Important Terms","text":"Term Definition Agent Agents give AI apps a fundamentally new set of capabilities: to solve complex problems, to act on the outside world, and to learn from experience post-deployment. Ex. Auto GPT, LangGraph AI21 Labs AI21 Studio provides API access to Jurassic-2 large language models. Their models power text generation and comprehension features in thousands of live applications. AI21 is building state of the art language models with a focus on understanding meaning. BARD AI chat service from Google - powered by the LaMDA model. Similar to ChatGPT. Evolved to Gemini BLOOM BLOOM is an auto regressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks. It is a popular open source instructor based model. Developers who want an open source alternative to GPT might look at this. co:here Co:here platform can be used to generate or analyze text to do things like write copy, moderate content, classify data and extract information, all at a massive scale. Model compression Technique to reduce the size of the model in memory, it includes quantization (approximating a neural network by using smaller precision 8-bit integers instead of 32-bit floating point numbers) and distillation (transferring of knowledge from a larger teacher model to a smaller student model). Data Distributed Training A distributed training algorithm which can speed up ML training by distributing batches of data between forward and backward passes in a model. This can be very helpful when we have large datasets but does not solve the problem of not being able to fit a model on one machine DeepSpeed DeepSpeed is an open source deep learning optimization library for PyTorch. The library is designed to reduce computing power and memory usage and to train large distributed models with better parallelism on existing computer hardware. DeepSpeed is optimized for low latency, high throughput training. It can be used to help both inference and training of large models which don't fit on a single GPU. Distributed Training In distributed training the workload to train a model is split up and shared among multiple mini processors, called worker nodes. These worker nodes work in parallel to speed up model training. Few shot Learning or few-shot prompting is a prompting technique that allows a model to process examples before attempting a task. Fine Tuning Foundation model further trained to specific tasks. Example: training BLOOM to summarize chat history where we have examples of these text examples. FLAN FLAN(Fine-tuned LAnguage Net): is a LLM with Instruction Fine-Tuning. It is a popular open source instructor based model which scientists can train. Persons who want an open source alternative to GPT might look at this. Generative adversarial network (GAN) A deep learning architecture where two networks compete in a zero sum game. When one network wins, the other loses and vice versa. Common applications of this, includes creating new datasets, image generation, and data augmentation. This is a common design paradigm for generative models. GPT OpenAI's generalized pre-trained transformer foundation model family. GPT 1 and 2 are open source while 3 and 4 are proprietary. GPT1,2,3 are text-to-text while gpt4 is multi-modal. Hallucinations LLMs may give answers which are incorrect or seemingly made up. Hallucinations are mainly a data problem, LLMs suffer from knowledge cut-off where they only know up to the point their training data stops. They also are trained on wide varieties of data some of which can be inaccurate or incomplete. To minimize it, use Top-P, Top-K, Temperature and RAG models. Jurassic This is AI21 lab's foundation text to text model. It has instructor and non-instructor based versions and is available on AWS marketplace. This is very appealing for customers because they can get 1) extremely high model quality/accuracy and 2) deploy the model to a dedicated endpoint for dedicated compute. LaMDA Language model was trained on dialogue from Google. Very similar to ChatGPT but produced by Google. It is a proprietary model. Model compilation Model compilation is the act of tracing a model computational graph in order to deploy to lower level hardware and code. This is a necessary step to run on specialized hardware. Model Distribution When a model's size prohibits it from being stored on one GPU. This occurs when models start to be in the 10's of billions of parameter range. This has a few consequences 1) it costs a lot to train and host these models 2) specialized libraries are required to help. MultiModal Models Multi-modal learning attempts to model the combination of different modalities of data, often arising in real-world applications. An example of multi-modal data is data that combines text (typically represented as discrete word count vectors) with imaging data consisting of pixel intensities and annotation tags. Pre-training Unsupervised learning method which is used to steer foundation models to domain specific information. Example: pre-training FLAN with Medical documents to understand medical context previously missing from the model. Reinforcement learning with human feedback (RLHF) The secret sauce to making chat based foundation models. The process involves using human feedback with LLM chat interactions to inform a reinforcement learning procedure to help train an LLM to \"talk to humans\" instead of only prompts. There are two huge benefits 1/ this substantially reduces the amount of prompt engineering required and 2/ this allow the LLM to take into account chat context as well as the information it has available to it. Single shot learning Zero-shot learning (ZSL) is a problem setup in ML where, at test time, a learner observes samples from classes which were not observed during training, and needs to predict the class that they belong to Stability.ai Stability AI is open source generative AI company currently developing breakthrough AI models applied to imaging, language, code, audio, video, 3D content, design, biotech. With AWS they provide the world\u2019s fifth-largest supercomputer \u2013 the Ezra-1 UltraCluster \u2013 supplying the necessary power to generate these advancements. Stability AI\u2019s premium imaging application DreamStudio, alongside externally built products like Lensa, Wonder and NightCafe, have amassed over 40 million users. Stable Diffusion Stable diffusion is a popular open source text to image generation tool. It can be used for use cases like 1/ marketing content generation 2/ game design 3/ fashion design and more. Text to text Any model which takes in text inputs and produces text outputs. Ex: entity extraction, summarization, question answer. Transfer learning The act of transferring the power of a foundation model to a specific task. Transformer A ML model for transforming one sequence into another, using attention."},{"location":"genAI/#summarization","title":"Summarization","text":"<p>Text summarization is a Natural Language Processing (NLP) technique that involves extracting the most relevant information from a text document and presenting it in a concise and coherent format.</p> <p>Summarization works by sending a prompt instruction to the model, asking the model to summarize our text.</p> <p>See hands-on notes on LangChain.</p>"},{"location":"genAI/#retrieval-augmented-generation-rag","title":"Retrieval augmented generation (RAG)","text":"<p>See separate chapter.</p>"},{"location":"genAI/#common-llm-inference-parameters","title":"Common LLM inference parameters","text":""},{"location":"genAI/#randomness-and-diversity","title":"Randomness and Diversity","text":"<p>Foundation models support the following parameters to control randomness and diversity in the response:</p> <p>Temperature \u2013 Large language models use probability to construct the words in a sequence. For any given next word, there is a probability distribution of options for the next word in the sequence. When we set the temperature closer to zero, the model tends to select the higher-probability words. When we set the temperature further away from zero, the model may select a lower-probability word which leads to creative output.</p> <p>In technical terms, the temperature modulates the probability density function for the next tokens, implementing the temperature sampling technique. This parameter can deepen or flatten the density function curve. A lower value results in a steeper curve with more deterministic responses, and a higher value results in a flatter curve with more random responses.</p> <p>Top K \u2013 Top K defines the cut off where the model no longer selects the words. For example, if K=50, the model selects from 50 of the most probable words that could be next in a given sequence. This reduces the probability that an unusual word gets selected next in a sequence.</p> <p>In technical terms, Top K is the number of the highest-probability vocabulary tokens to keep for Top-K-filtering - This limits the distribution of probable tokens, so the model chooses one of the highest-probability tokens.</p> <p>Top P \u2013 Top P defines a cut off based on the sum of probabilities of the potential choices. If we set Top P below 1.0, the model considers the most probable options and ignores less probable ones. Top P is similar to Top K, but instead of capping the number of choices, it caps choices based on the sum of their probabilities. For the example prompt \"I hear the hoof beats of ,\" we may want the model to provide \"horses,\" \"zebras\" or \"unicorns\" as the next word. If we set the temperature to its maximum, without capping Top K or Top P, we increase the probability of getting unusual results such as \"unicorns.\" If we set the temperature to 0, we increase the probability of \"horses.\" If we set a high temperature and set Top K or Top P to the maximum, we increase the probability of \"horses\" or \"zebras,\" and decrease the probability of \"unicorns.\"</p>"},{"location":"genAI/#lengths","title":"Lengths","text":"<p>The following parameters control the length of the generated response.</p> <p>Response length \u2013 Configures the minimum and maximum number of tokens to use in the generated response.</p> <p>Length penalty \u2013 Length penalty optimizes the model to be more concise in its output by penalizing longer responses. Length penalty differs from response length as the response length is a hard cut off for the minimum or maximum response length.</p> <p>In technical terms, the length penalty penalizes the model exponentially for lengthy responses. 0.0 means no penalty. Set a value less than 0.0 for the model to generate longer sequences, or set a value greater than 0.0 for the model to produce shorter sequences.</p>"},{"location":"genAI/#repetitions","title":"Repetitions","text":"<p>The following parameters help control repetition in the generated response.</p> <p>Repetition penalty (presence penalty) \u2013 Prevents repetitions of the same words (tokens) in responses.  1.0 means no penalty. Greater than 1.0 decreases repetition.</p>"},{"location":"genAI/#vector-database","title":"Vector Database","text":"<p>A vector database is optimized for storing and querying large vector arrays using machine learning techniques. It's highly scalable and fast at performing operations like similarity searches across vectors. </p> <p>Similarity search helps to identify items (vectors) that share similar characteristics or properties with the query item (a new vector). </p> <p></p> <p>Queries return results based on vector similarity scores, revealing hidden semantic connections in data. </p> <p>FAISS from Facebook is a library for efficient similarity search and clustering of dense vectors. Faiss can compute vector Euclidien distance using GPU or CPU.</p> <p>ChromaDB is an open source embedding database which supports Queries, filtering, density estimation and similarity search. It can persist on local disk or use a server deployment. It uses collection for storing the documents, metadatas, embeddings, and ids. Chroma DB by default uses a sentence transformer model to calculate embeddings.</p> <p>(Code using ChromaDB end to end solution with qa-retrieval) or code langchain/rag folder specially build_agent_domain_rag.pyhttps://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/rag/build_agent_domain_rag.py</p> <p>Docker compose to start chromadb</p> <pre><code>  chroma:\n    image: ghcr.io/chroma-core/chroma:latest\n    volumes:\n      - ./chromadb/.chroma/index\n    ports:\n      - 8005:8000\n</code></pre> <p>Traditional open source index or database such as OpenSearch, Postgresql support now vector store and similarity search. </p>"},{"location":"genAI/#current-technology-landscape","title":"Current Technology Landscape","text":""},{"location":"genAI/#openai-chatgpt","title":"OpenAI - ChatGPT","text":"<p>OpenAI is an AI research and deployment company. Their vision: intelligence\u2014AI systems are generally smarter than humans: </p> <ol> <li>With broad general knowledge and domain expertise, GPT-4 can follow complex instructions in natural language and solve difficult problems with accuracy.</li> <li>DALL\u00b7E 2 can create original, realistic images and art from a text description. It can combine concepts, attributes, and styles.</li> <li>Whisper can transcribe speech into text and translate many languages into English.</li> </ol> <p>Chat Generative pre-trained Transformer is a proprietary instruction-following model, which was released in November 2022. It is a system of models designed to create human like conversations and generating text by using statistics. It is a Causal Language Model (CLM) trained to predict the next token.</p> <p>The model was trained on trillions of words from the web, requiring massive numbers of GPUs to develop. The model was trained using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with different data collection setup. </p> <ul> <li>Build an AI that can answer questions about your website: crawl, use embeddings, and a search function. It is a good starting point for knowledge based app.</li> </ul>"},{"location":"genAI/#meta-with-llama-2-3","title":"Meta with LLama 2 &amp; 3","text":"<p>A foundational, 65-billion-parameter large language model created by Facebook which has been open sourced for academic use. Many models have been released based on Llama2, but they also inherit the license requirement for non-commercial use.</p> <p>It is possible to run LLama2 on local machine with ollama, and a simple LangChain</p> <ul> <li>Anakin is a platform to access different models </li> <li>Perplexity Labs, a part of Perplexity AI, provides a user-friendly platform for developers to explore and experiment with large language models, including Llama 3</li> <li>https://huggingface.co/chat/</li> <li>Replicate to run, fine-tune open-source models, and expose them as APIs. They also lead Cog an open-source tool for packaging machine learning models.</li> <li>Vercel serverless, hosting platform for web app and AI app. The led vite for next.js project. See a langchain nextjs template to easily deploy to Vercel.</li> </ul>"},{"location":"genAI/#mistral-mixture-of-experts","title":"Mistral - Mixture of Experts","text":"<p>A french company who has developed the Mixtral 8x7B model, a high-quality sparse mixture of experts model (SMoE) with open weights.</p>"},{"location":"genAI/#google-gemini","title":"Google Gemini","text":"<p>Is the public Generative multimodal AI from Google DeepMind team with the support of 3 different sizes, the smallest being able to run on Mobile. Its reasoning capabilities can help make sense of complex written and visual information.</p>"},{"location":"genAI/#amazon-sagemaker","title":"Amazon SageMaker","text":"<p>SageMaker Jumpstart provides pre-trained, open-source models for a wide range of problem types to get started on ML.</p> <p>It supports training on LLMs not in Bedrock, like OpenLLama, RedPajama, Mosaic Pre-trained Transformer-7B, Flan-T5/UL2, GPT-J-6B, NEOX-20B and Bloom/BloomZ, with a gain of up to 40% faster.</p> <p>Some useful articles:</p> <ul> <li>AWS- Quickly build high-accuracy Generative AI applications on enterprise data using Amazon Kendra, LangChain, and large language models.</li> <li>SageMaker my own personal study..</li> </ul>"},{"location":"genAI/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Vulnerabilities of LLM.</li> <li>GANs for Synthetic Data Generation.</li> <li>Artificial Intelligence and the Future of Teaching and Learning.</li> <li>Fine-tune a pre-trained model HuggingFace tutorial.</li> <li>Prompt engineering is the new feature engineering.</li> <li>Amazon-sponsored workshop advances deep learning for code.</li> <li>RAG with OpenSearch Service.</li> <li>Running LLM on local laptop using llama.cpp</li> <li>BertNet knowledge graphs from llm</li> <li>Git repo explaining the transformer from Alammar.</li> <li>Attention Is All You Need - research paper</li> <li>On the role of LLM in planning - tutorial</li> </ul> <p>&gt;&gt;&gt; Prompt Engineering </p>"},{"location":"genAI/agentic/","title":"Agentic AI","text":"<p>Agent is an orchestrator pattern where the LLM decides what actions to take from the current query and context. With chain, developer code the sequence of tasks, with agent the LLM decides. </p>"},{"location":"genAI/agentic/#introduction","title":"Introduction","text":"<p>The agentic reference architecture was introduced by Lilian Weng in the following figure (light adaptation):</p> <p></p> <p>The planning phase includes techniques like Chain of Thought (\"think step by step\"), Tree of thoughts (explores multiple reasoning paths) or LLM+P (used external long-horizon planner).</p> <p>Short term memory is the context, and limited by the LLM context window size. Long term memory is the vector store supporting the maximum inner product search, it is also used to self improve agents. Entity memory is a third type of memory to keep information of the subjects of the interactions or work to be done. Short term memory helps exchanging data between agents too. </p> <p>Tools are used to call external services or other LLMs. Neuro-symbolic architecture can be built with expert system modules combined with general-purpose LLM. LLM routes to the best tool.</p> <p>There are different types of agent: Intended Model, Supports Chat, Supports Multi-Input Tools, Supports Parallel Function Calling, or Required Model Params.</p> <p>I believe AI agent applications at scale will not only be triggered by users, but by systems using asynchronous events. (Flink's event capabilities in real-time distributed event processing, state management and exact-once consistency fault tolerance make it well-suited as a framework for building such system-triggered agents)</p>"},{"location":"genAI/agentic/#use-cases","title":"Use cases","text":"<ul> <li>Agents to plan an article, write this article and review for better edition. See the research-agent.py code.</li> <li>Support Representative, the support_crew.py app demonstrates two agents working together to address customer's inquiry for the best possible way, using some sort of quality assurance. It uses memory and web scrapping tools.</li> <li>Customer outreach campaign: customer_outreach.py uses tools to do google searches with two agents doing sale lead analysis.</li> <li>Crew to tailor a job application with multiple agents: job_application.py</li> </ul>"},{"location":"genAI/agentic/#small-specialist-agents","title":"Small Specialist Agents","text":"<p>Small Specialist Agents (SSAs) is an agentic approach to perform planning and reasoning to enhance AI capabilities for complex problem using domain-specific knowledge. It may implement the OODA loop: Observe, Orient, Decide, and Act, with Hierarchical Task Planning to cut bigger tasks in smaller ones. Planning can use up to date data to define future actions. Agentic AI can respond swiftly and effectively to changing environments. SSAs predict maintenance needs, adjust operational parameters to prevent downtime, and ensure that energy production meets demand without excess waste. In healthcare, SSAs may analyzing genetic data, medical histories, and real-time responses to various treatments.</p> <p>See OpenSSA project</p>"},{"location":"genAI/agentic/#challenges","title":"Challenges","text":"<p>The current adoption of Agents since mid 2023 has highlighted the following challenges:</p> <ul> <li>Existing demonstrations of agent in action are for very specific use cases and are giving too much freedom to Agents without enough controls.</li> <li>The cost of running open-loop agent is high.</li> <li>Pure LLM plannification, reason and act is not optimized and the path to reach a response may be long. Tasks may be defined more than once.</li> <li>Get good results for reasoning is achieved on the last (mid 2024), most expensive, LLM.</li> <li>New model released recently demonstrates that existing agent workflow implementations become unstable. </li> <li>This is not ready for production usages as responses can reach a hole in the workflow or continuous iterations (reaching a max number of iterations)</li> <li>Big prompt for agent with a lot of tools (Multi-Action-Agent) deliver poor results in tool selection. </li> <li>Even with larger context window, they are still issue with the \"in-the-middle\" problem, where context instructions in the middle of the system prompt is ignored  by the LLM during generation.</li> <li>Even same LLMs used in the agent are loosing their efficiency over time. </li> </ul> <p>Developers need to address the level of freedom given to the LLMs.</p> Type Decide output Decide steps to take Determine step sequences Code Code Code Code LLM Call On step of LLM Code Code Chain Multiple calls to LLM Code Code Router LLM LLM without cycle Code State Machine LLM LLM with cycle Code Agent (Autonomous) LLM LLM LLM <p>Multiple agents, with more dedicated prompt, smaller list of tools, event Single Action Agent, and orchestration seems to be a viable solution for agentic solutions. This new approach adds complexity in designing, implementing and then tuning the solution, but authorize the usage of smaller LLM, and specific prompts. Current research looks after integrating agent with reinforcement learning as tools to do trial and error learning.</p> <p>LangGraph helps to better support the Router, State Machine and chain implementations.</p>"},{"location":"genAI/agentic/#guidelines","title":"Guidelines","text":"<p>Agents perform better if we define a role to play, instruct them with a specific prompt to help them to focus on a goal, add tools to access external systems, combine them with other agents to cooperate and chain content between agents. </p> <p>Focus is becoming important as the context windows are becoming larger. With too many information LLM can lose the important points and goals. Try to think about multiple agents to split the work and generate better results together.</p> <p>Too much tools adds confusion for the agents, as they have hard time to select tool, or distinguish what is a tool, a context or an history. Be sure to give them tools for what they need to do. </p> <p>For task definition, think about process, actors and tasks. Have a clear definition for each task, with expectation and context. Task may use tools, should be able to run asynchronously, output in different format like json, xml, ...</p>"},{"location":"genAI/agentic/#design-patterns","title":"Design Patterns","text":""},{"location":"genAI/agentic/#technologies","title":"Technologies","text":""},{"location":"genAI/agentic/#langchain-agent-module","title":"LangChain Agent module","text":"<p>In Agents, a language model is used as a reasoning engine to determine which actions to take and in which order. </p> <p>LangChain agents package API doc.</p>"},{"location":"genAI/agentic/#langgraph","title":"LangGraph","text":"<p>LangGraph supports well the implementation of Agents. See this samples repository</p>"},{"location":"genAI/agentic/#crewai","title":"CrewAI","text":"<p>crewAI is a framework to develop application using multiple-agent. It uses the concepts of Agent, Task and Crew to organize the work between agents. The concepts are common to any Agentic AI solutions.</p> <pre><code>from crewai import Agent, Task, Crew\n</code></pre> <p>Agent needs the following 6 elements:</p> <ol> <li> <p>Role Playing: Agents perform better when doing role playing. It is mapped to the first statement in a prompt, and it is a common practice in prompt engineering.</p> <pre><code>writer = Agent(\n            role=\"Content Writer\",\n            goal=\"Write insightful and factually accurate \"\n</code></pre> </li> <li> <p>Focus on goals and expectations to better prompt the agent: \"give me an analysis of xxxx stock\". Too much stuff in the context window is confusing the model, and may hallucinate. May be splitting into multiple agents is a better solution instead of using a single prompt.</p> </li> <li> <p>Tool is used to call external system, and is well described so the model can build parameters for the function and be able to assess when to call the function. Now too many tools will also add to the confusion. Small model will have hard time to select tools. So think to have multiple-agent with only the tools they need to do their task.</p> </li> <li>Cooperation has proved to deliver better results than unique big model. Model can take feedbacks from each others, they can delegate tasks.</li> <li>Guardrails are helping to avoid models to loop over tool usages, creating hallucinations, and deliver consistent results. Models work on fuzzy input, generate fuzzy output, so it is important to be able to set guardrails to control outcomes or runtime execution.</li> <li>Memory is important to keep better context, understand what was done so far, apply this knowledge for future execution. Short term memory is used during the crew execution of a task. It is shared between agents even before task completion. Long term memory is used after task execution, and can be used in any future tasks. LTM is stored in a DB. Agent can learn from previous executions. This should lead agent to self-improve. The last type of memory is the entity memory (person, organization, location). It is also a short term, and keep information of the entity extracted from NLP.</li> </ol> <p>CrewAI has tools to scrape website, search internet (Serper), load customer data, tap into previous conversations, load data from a CRM, checking existing bug reports, checking existing feature requests, checking ongoing tickets...</p> <p>See code examples in the techno/crew-ai folder</p>"},{"location":"genAI/agentic/#some-guidelines","title":"Some guidelines","text":"<ul> <li>Adapt the task and agent granularity</li> <li>Task can be executed in different ways, parallel, sequential,... so test and iterate</li> <li>With agent delegation parameter, the agent can delegate its work to another agent which is better suited to do a particular task.</li> <li>Try to add a QA agent to control and review task results</li> <li>Tools can be defined at the agent level so it will apply to any task, or at the task level so the tool is only applied at this task. Task tool overrides agent tools.</li> <li>Tools need to be versatile, fault tolerant, and implement caching. Versatile to be able to get the fuzzy input well interpreted by the model and call the relevant tools and by extracting structured input parameters in the form of json or key-value pairs. </li> <li>To be fault tolerant, function can stop execution, retries with exponential backoff, or report error message to the LLM so it can better extract and format parameters.</li> <li>CrewAI offers a cross-agent caching mechanism. It is also compatible with LangChain tools.</li> <li>Think as a manager: define the goal and what is the process to follow. What are the people I need to hire to get the job done. Use keyword and specific attributes for the role, agent needs to play.</li> </ul>"},{"location":"genAI/agentic/#autogen","title":"AutoGen","text":"<p>Microsoft AutoGen is a multi-agent conversation framework to help developers build LLM workflows. The first abstraction is a ConversableAgent</p>"},{"location":"genAI/agentic/#openssa","title":"OpenSSA","text":"<p>Small Specialist Agents for Problem-Solving </p>"},{"location":"genAI/agentic/#references","title":"References","text":"<ul> <li>LLM Powered Autonomous Agents - Lilian Wang</li> <li>Prompt engineering with external APIs</li> <li>Crew-ai tutorial on deeplearning.ai</li> </ul>"},{"location":"genAI/anthropic/","title":"Anthropic","text":"<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>"},{"location":"genAI/anthropic/#anthropic-notes","title":"Anthropic notes","text":"<p>Anthropic is the company behind of the Claude LLMs. Claude 3 Haiku, Sonnet, and Opus are the next generation of Claude models. The value propositions are safety, accuracy and security.</p>"},{"location":"genAI/anthropic/#important-source-of-information","title":"Important source of information","text":"<ul> <li>https://console.anthropic.com where we can do prompting.</li> <li>Claude.ai to chat with Claude.</li> <li>Anthropic cookbook</li> <li>Anthropic Prompt Library</li> <li>LangChain for Anthropic</li> </ul> <p>=======</p>"},{"location":"genAI/anthropic/#anthropic-notes_1","title":"Anthropic notes","text":"<p>Anthropic is the company behind of the Claude LLMs. Claude 3 Haiku, Sonnet, and Opus are the next generation of Claude models. The value propositions are safety, accuracy and security.</p>"},{"location":"genAI/anthropic/#important-source-of-information_1","title":"Important source of information","text":"<ul> <li>https://console.anthropic.com where we can do prompting.</li> <li>Claude.ai to chat with Claude.</li> <li>Anthropic cookbook</li> <li>Anthropic Prompt Library</li> <li>LangChain for Anthropic</li> </ul> <p>0d6401c17707d5173c4bc1b6e287c4b1ac9c4f0f</p>"},{"location":"genAI/cohere/","title":"Cohere summary","text":"<p>Cohere offers LLM products to deploy AI solutions with the enterprise.</p>"},{"location":"genAI/cohere/#value-proposition","title":"Value proposition","text":"<ul> <li>Focus to solve problem with ML and AI</li> <li>Strong in NLP in different human's languages.</li> <li>Command LLMs are easy to customize and fine tune</li> <li>Strong on RAG, with Embeddings, Rerank products. Dedicated AI research group. Support open science initiatives: Aya is a state-of-the-art multilingual open-source research model and dataset covering 101 languages.</li> <li>Use SaaS offering, with chat and playground: Playground allows developer to experience the power of LLM without coding a single line.</li> <li>Easy integration via SDK and APIs</li> <li>Scalable models</li> </ul>"},{"location":"genAI/cohere/#products","title":"Products","text":"<p>Three main products: Command for LLM, Embed, and Rerank.</p>"},{"location":"genAI/cohere/#command-r","title":"Command-R","text":"<p>A LLM optimized for long-context task, and to support enterprise deployments using: </p> <ul> <li>Advanced Retrieval Augmented Generation with citation to reduce hallucinations</li> <li>128K context length</li> <li>Multilingual coverage in 10 key languages to support global business operations</li> <li>Tool Use to automate sophisticated business processes using single-step tool or multi-step tool like an agent</li> </ul>"},{"location":"genAI/cohere/#embed","title":"Embed","text":"<p>Embeddings can be used for estimating semantic similarity between two texts, choosing a sentence which is most likely to follow another sentence, or categorizing user feedback. Embed improves the accuracy of search, classification, clustering, and RAG results. Cohere embedding associates each word with a vector of length 4096. It supports more than 100 languages.</p>"},{"location":"genAI/cohere/#rerank","title":"Rerank","text":"<p>Rerank models sort text inputs by semantic relevance to a specified query. They are often used to sort search results returned from an existing search solution.  Rerank is used to inject the intelligence of a language model into an existing search system.</p>"},{"location":"genAI/cohere/#references","title":"References","text":"<ul> <li>Product documentation.</li> <li>LLM university on youtube</li> <li>Transformer model</li> <li>Attention explained - Youtube - Luis Serrano</li> </ul>"},{"location":"genAI/mistral/","title":"Mistral.ai","text":"<p>French Startup to build mixture of experts based LLMs with open source offering. </p> <ul> <li>The open-weights models are Mistral 7B, Mixtral 8x7B, Mixtral 8x22B</li> <li>The commercial models (Mistral Small, Mistral Medium, Mistral Large, and Mistral Embeddings (retrieval score of 55 on MTEB), codetral for code generation.</li> </ul> <p>Models description and benchmarks notes.</p> <p>Model can be fine tuned.</p> model type of usage Mistral Small Classification, Customer support, text gen. Mistral 8x22B intermediate tasks that require moderate reasoning - like Data extraction, Summarizing a Document, Writing a Job Description, or Writing Product Descriptions Mistral Large Complex tasks that require large reasoning capabilities or are highly specialized - like Synthetic Text Generation, Code Generation, RAG, or Agents <p>Function calling is supported by Mistral Small, Large, 8x22B.</p> <p>Mistral delivers docker image for the model. To run locally with skypilot or Ollama and docker compose.</p> SkyPilot <p>skypilot delivers a CLI to deploy batch job on cluster deployed on AWS or GCP. Cluster of EC2 machines are created dynamically, as well as security group, security key,... The concept of task is used to run ML job by requesting specific resources like TPU, GPU, disk... The resources are created in the user's cloud account. It  allows easy movement of data between task VMs and cloud object stores.</p>"},{"location":"genAI/mistral/#mixture-of-experts","title":"Mixture of Experts","text":"<p>MoE combines multiple models to make predictions or decisions. Each expert specializes in a specific subset of the input space and provides its own prediction. The predictions of the experts are then combined, typically using a gating network, to produce the final output.</p> <p>It is useful when dealing with complex and diverse data, each expert extract different aspects or patterns in the data.</p> <p>MoE in language translation may use experts by language pairs</p>"},{"location":"genAI/openai/","title":"OpenAI","text":"<p>The offering includes ChatGPT URL, different GPT models (GPT-4, DALL.E, TTS, Whisper, Embeddings), a SDK, and a set of APIs.</p>"},{"location":"genAI/openai/#notes","title":"Notes","text":"<ul> <li>Personal data is not used to train or improve the models</li> <li>API data may be retained for up to 30 days. Except customers willing to do zero data retention.</li> <li>Support tool definitions to decide to call external systems </li> </ul>"},{"location":"genAI/openai/#quickstart","title":"Quickstart","text":"<p>Once we got an API key, it is important to understand the API limits and pricing.</p>"},{"location":"genAI/openai/#assistants-api","title":"Assistants API","text":"<p>Assistants can leverage models and tools like Code Interpreter (OpenAI hosted), Retrieval (OpenAI hosted), and Function calling, to respond to user queries.</p> <p>Assistants are created via API by specifying its name, instructions, a set of tools and then the LLM model name to use. One assistant per tool.</p> <p>OpenAI uses the concept of Thread to represent a conversation between a user and one or many Assistants. Messages are added to the Thread. The Thread is explicitly executed with a <code>run</code>.</p> <p>Threads simplify AI application development by storing message history and truncating it when the conversation gets too long for the model\u2019s context length.</p> <pre><code>from openai import OpenAI\nclient = OpenAI()\n\nassistant = client.beta.assistants.create(\n  name=\"Math Tutor\",\n  instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n  tools=[{\"type\": \"code_interpreter\"}],\n  model=\"gpt-4-turbo-preview\",\n)\n\nthread = client.beta.threads.create()\n\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\".... Can you help me?\"\n)\n\nrun = client.beta.threads.runs.create(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  instructions=\"Please ...\"\n)\n</code></pre> <p>Code Interpreter may execute our own nodejs or python code on OpenAI hosts.</p> <p>The Assistants API automatically manages the context window such that we never exceed the model's context length.</p> <p>Run has states: <code>Queued, In progress, Requires actions, Expired, Completed, Failed, Cancelling, Cancelled</code>. We can poll the run states via API. </p>"},{"location":"genAI/prompt-eng/","title":"Prompt engineering","text":"Update <p>Created Aug 2023 - Updated 05/2024</p> <p>This chapter includes a summary of prompt engineering practices and links to major information on this subject. Main sources of knowledge are:</p> <ul> <li>Prompt engineering guide from (promptingguide.ai) which covers the theory and practical aspects of prompt engineering and how to leverage the best prompting techniques to interact and build with LLMs.</li> <li>Wikipedia- prompt engineering</li> <li>Anthropic - Claude - Prompt engineering.</li> <li>Mistral prompting capabilities.</li> </ul> <p>This repository includes code, prompts to test on different LLMs.</p>"},{"location":"genAI/prompt-eng/#introduction","title":"Introduction","text":"<p>A prompt is an input that the model uses as the basis for generating a text. Prompts are a way to directly access the knowledge encoded in large language models. While all the information may be codes in the model, the knowledge extraction can be a hit or miss.</p> <p>Prompt involves instructions and context passed to a language model to achieve a desired task.</p> <p>Prompt Engineering is a practice of developing and optimizing prompts to efficiently use LLMs for a variety of applications. It is still a major research topic.</p> <p>Prompt engineering typically works by converting one or more tasks to a prompt-based dataset and training a language model with what has been called \"prompt-based learning\" or just \"prompt learning\".</p> <p>We can provide a prompt with examples so the LLM will condition on the new context to generate better results. Examples in summarization.</p> <p>Prompts may also help incorporating domain knowledge on specific tasks and improve interpretability. Creating high-quality prompts requires careful consideration of the task at hand, as well as a deep understanding of the model\u2019s strengths and limitations.</p> <p>LLMs are very sensitive to small perturbations of the prompt: a single typo or word change can alter the output.</p> <p>There is still need to evaluate models robustness to prompt.</p> <p>Many recent LLMs are fine-tuned with a powerful technique called instruction tuning, which helps the models generate responses to prompts without prompt-specific fine-tuning. It does not involve updating model weights.</p> Instruction tuning <p>Technique to train the model with a set of input and output instructions for each task (instead of specific datasets for each task), allowing the model to generalize to new tasks that it hasn\u2019t been explicitly trained on as long as prompts are provided for the tasks. It helps improve the accuracy and effectiveness of models and is helpful in situations where large datasets aren\u2019t available for specific tasks.</p>"},{"location":"genAI/prompt-eng/#tuning-parameters","title":"Tuning parameters","text":"<p>The classical Temperature, Top-P and max length parameters need to be tuned to get more relevant responses. The Stop sequence, frequency penalty (penalty on next token already present in the response), presence penalty (to limit repeating phrases) are also used in the prompt.</p>"},{"location":"genAI/prompt-eng/#prompting-techniques","title":"Prompting techniques","text":"<ul> <li>Zero-shot prompting: a unique question. No instruction</li> <li>Few-shot prompting includes some samples like a list of Q&amp;A. </li> <li>A prompt contains any of the following elements: instruction, context, input data, output indicator</li> <li>Use command to instruct the model to do something specific: \"Write\", \"Classify\", \"Summarize\", \"Translate\", \"Order\"</li> <li>Be very specific about the instruction and task. </li> <li>Providing examples in the prompt is very effective to get desired output in specific formats.</li> </ul>"},{"location":"genAI/prompt-eng/#chain-of-thought","title":"Chain of Thought","text":"<p>Chain-of-thought (CoT) prompting using intermediate steps.  It is used to address more complex arithmetic, commonsense, and symbolic reasoning tasks. The zero-shot CoT seems to get good results by adding \"Let's think step by step\" sentence. (see the test code under llm-langchain/bedrock folder: <code>python TestBedrockCoT.py -p cot3.txt -m  ai21.j2-mid</code>)</p> <ul> <li>Examples:</li> </ul> <pre><code>explain Quantum mechanics to high school student\nA:\n</code></pre> <p>See also the article from Anthropic: \"ask Claude to think step by step\"</p> <ul> <li> <p>RLHF (reinforcement learning from human feedback) has been adopted to scale instruction tuning wherein the model is aligned to better fit human preferences.</p> </li> <li> <p>Self consistency prompt uses sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer.</p> </li> <li> <p>To test CoT with Bedrock LLMs. See the code in TestBedrockCot with <code>python TestBedrockCoT.py -p cot1.txt -m anthropic.claude-v2</code>  should gives good answer. While <code>python TestBedrockCoT.py -p cot2.txt -m  ai21.j2-mid</code> returns bad answers.</p> </li> </ul>"},{"location":"genAI/prompt-eng/#prompt-chaining","title":"Prompt chaining","text":"<p>Break prompt task into subtasks, and add the response of a subtask to the next subtask call to LLM. It creates a chain of prompts. Prompt chaining is useful when building conversational assistants or for document QA, and when the prompt is detailed.</p> <p>It brings better performance, and helps to boost transparency of the LLM application, increasing controllability, and reliability.</p> <p>For document QA, a first prompt is used to extract the important quotes from the document given the question, the second prompt uses the generated quotes as input.</p> <p>See Anthropic Claude - Prompt Chaining examples.</p> <p>It can be used to validate a previous response to a prompt.</p>"},{"location":"genAI/prompt-eng/#tree-of-thoughts","title":"Tree of Thoughts","text":"<p>Tree of Thoughts is a generalization of CoT, where thoughts represent coherent language sequences that serve as intermediate steps toward solving a problem. The thoughts are organized in tree, which search algorithms (breadth-first search and depth-first search) are used to assess the best combination of thoughts via a multi-round conversation.</p>"},{"location":"genAI/prompt-eng/#automatic-prompt-engineering","title":"Automatic Prompt Engineering","text":"<p>Another approach to automate the prompt creation and selection using LLMs as inference models followed by LLMs as scoring models. DSPy  is a framework for algorithmically optimizing LM prompts and weights. </p>"},{"location":"genAI/prompt-eng/#other-techniques","title":"Other techniques","text":"<ul> <li> <p>Directional Stimulus Prompting: \"Summarize the above article briefly in 2-3 sentences based on the hint. Hint:....\"</p> </li> <li> <p>Program-Aided Language Models: use LLMs to read natural language problems and generate programs as the intermediate reasoning steps. See example of such prompt in this python code TestPALwithClaude.py</p> </li> <li> <p>ReAct Prompting uses LLMs to generate both reasoning traces and task-specific actions.</p> </li> </ul>"},{"location":"genAI/prompt-eng/#openai","title":"OpenAI","text":"<p>The best practices for prompt engineering using OpenAI models can be summarized as:</p> <ul> <li>Write clear instruction with \"ask for brief replies\", or \"expert-level writing\", or demonstrate expected result.</li> <li>Provide reference text</li> <li>Split complex tasks into simpler subtasks</li> <li>Give the model time to \"think\"</li> </ul>"},{"location":"genAI/prompt-eng/#playground","title":"Playground","text":"<ul> <li>Use Bedrock interface for text playground one of the integrated model.</li> <li>A repository with Bedrock content with text generation notebooks.</li> <li>In this repository the llm-langchain/bedrock includes  Python bedrock client codes with prompt samples.</li> </ul> How to use Bedrock client <p>The basic instruction to use the code of this repository to interact with an LLM deployed in Bedrock</p> <ul> <li>Use virtual Python env</li> </ul> <p><pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre> * Install dependencies: <code>pip3 install -r requirements.txt</code> * Be sure to have  AWS_SESSION_TOKEN set up * Run any of the python code. <code>python TestClaudeOnBedrock.py</code></p> <ul> <li>A notebook with Mixtral LLM</li> </ul>"},{"location":"genAI/rag/","title":"Retrieval Augmented Generation (RAG)","text":"Updates <p>12/2024</p>"},{"location":"genAI/rag/#what-is-covered-in-this-section","title":"What is covered in this section","text":"<p><pre><code>mindmap\n  root((RAG))\n    Context\n      Goals\n      Reduce Hallucinations\n      Integrate corporate knowledge\n    Reference Architecture\n        Product capability\n    Review Document Pipeline\n      Ingestion\n      Embeddings\n      Practices\n    Using Tools\n      Vector Store\n      Embeddings\n      Retriever\n      LangChain/LangGraph\n</code></pre> Figure 1: RAG subject areas</p>"},{"location":"genAI/rag/#context","title":"Context","text":"<p>LLMs have a knowledge cut-off time, where data coming after this time are not known by the models.  Pre-training is a one-off exercise. When enterprises need to get their private knowledge integrated to LLM, they can do fine tuning or present semantic search results as part of the input context window. RAG addresses this problem, as it is the act of supplementing generative text models with data outside of what it was trained on. </p> <p>While model increases in token pre-training size, they also increase the size of the context window, as illustrated in the figure below:</p> <p></p> <p>Figure 2: Bigger Context Window</p> <p>When the context window is big enough, application can send more contextual data, that leads to better results. This technique can be used to provide more up-to-date or more use-case-specific information via the context window to enhance its accuracy for specific use cases. It can also help reduce hallucinations, for example, by specifying that the model should only respond with information contained in the search results.</p>"},{"location":"genAI/rag/#basic-rag-architecture","title":"Basic RAG architecture","text":"<p>The Retrieval Augmented Generation may be seen as a three stages process:</p> <p></p> <ol> <li> <p>Indexing is a batch processing to ingest documents and data from different sources and indexing them. During processing, semantic search is used to retrieve relevant documents from the indexes. The <code>Indexing</code> step supports loading the documents, splitting large documents into smaller chunks. Chunks help to stay within the LLM's context window. Indexing includes storage of the chunks and the index of the splits. See the simple indexing code: build_agent_domain_rag.py using LangChain <code>RecursiveCharacterTextSplitter</code>, OpenAI embeddings and Chroma DB for vector store and retriever.</p> </li> <li> <p>Retrieval: retrieves the relevant data (splits) from the indexes using similarity search, then passes the resulting chunks to the LLM as part of the context window. The similarity search uses the embeddings to vectorize the query, perform the search and get the resulting indexes.</p> </li> <li>Generation: LLM generates the response in plain natural language.</li> </ol> <p>This process is supported by different tools for documents ingestion, splitting, embedding, indexing, retrieval and integration with the real time conversation flow. From the simple query text, the process needs to do query construction, translation, and LLM calling. The following diagram illustrates a classical a natural conversation application with RAG architecture:</p> <p></p> <ol> <li>The user asks queries via a Q&amp;A or Chat user interface. The query may be decomposed in sub-queries and embedded. </li> <li> <p>An application orchestrator uses the vector store retriever to do a similarity search into the vector database, and build the conversation context with retrieved documents. </p> <pre><code>retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n# Example of similarity search query\nretrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n</code></pre> </li> <li> <p>Context, query, system prompt are sent to the model to get the generated text. For this step, there are two approaches: 1/ sequential where text generation follows retrievals, or 2/ parallel where retrievals and text generations are done in parallel and then intertwined. </p> <pre><code># with a langchain that use the context variable as defined within the Prompt to pass the retrieved documents:\nrag_chain = (\n  {\"context\": retriever, \"question\": RunnablePassthrough()}\n  | prompt\n  | llm\n  | StrOutputParser()\n)\n</code></pre> </li> <li> <p>Response is sent back to the user.</p> </li> </ol> <p>RunnablePassthrough pass inputs unchanged to the following Runnable.</p> <p>RAG systems work well because LLMs has the in-context learning capability, which allows models to use previously unseen data to perform accurate predictions without weight training.</p> Domain specific vector store <p>The presence of mixed-domain data in the same vector store collection may introduce noise and potentially degrade performance. Isolating vector store collections for each domain can help maintain domain-specific information and improve model accuracy within individual domains</p> In-context learning <p>The NLP in-context learning involves continually updating the model as new data becomes available. The techniques include online learning, transfer learning, fine-tuning, or using memory-based architectures.</p>"},{"location":"genAI/rag/#embeddings","title":"Embeddings","text":"<p>Embeddings are the numerical representation of the document chunks, and should help to represent semantic concepts. They are key part of the NLP processing and used in Gen AI models. See dedicated Embedding chapter</p>"},{"location":"genAI/rag/#challenges","title":"Challenges","text":"<p>Naive RAG has very important limitations which has generated some adoption challenges:</p> <ul> <li>It is hard to do a reliable, scalable RAG on a large knowledge corpus</li> <li>Limited to single-shot prompt</li> <li>No query understanding, just a semantic search</li> <li>No query decomposition for planning</li> <li>No tool use, to query an external data source to enrich the context</li> <li>No reflection and error correction to improve the quality of the response.</li> <li>No persistence or memory of previous queries</li> </ul> <p>See also the retriever considerations section for more challenges.</p> <p>Those challenges can be addressed by adding agent before RAG. </p>"},{"location":"genAI/rag/#rag-assessment-scoping-questions","title":"RAG assessment scoping questions","text":"<p>Before doing an efficient RAG implementation, we need to address a set of important questions:</p> <ul> <li>Who is the end user? External users, clients of the enterprise, may have an incredible power to impact the enterprise's brand. </li> <li>What is the source of the documentation? What are the current data pipelines in place for each data sources, what are the touch points and security boundaries in place?</li> <li>Is there any structured data sources we need to consider for the search?</li> <li>How often the documentation change over time? Is there any governance in place to manage documents quality?</li> <li>How to extract relevant information from the document? Is there any PII in any document that may breach?</li> <li>How to avoid ingestion of malicious content into the knowledge corpus? Who can access what?</li> <li>How does the content look like in the document to determine chunk size and overlap?</li> <li>When to retrieve? How and what to retrieve?</li> <li>What are the expected set of most asked questions on the current corpus?</li> <li>How to post process the answer?</li> <li>How to learn new information?</li> <li>What to optimize in this entire system?</li> <li>How to handle queries outside of the domain? This may lead to adopt a domain specific LLM and combined with a generalized LLM.</li> <li>How the deployment will occur? Self-hosted, using API-based LLM? What is the expected latency supported? What cost is expected?</li> <li>Is there any compliances and regulations to follow?</li> </ul> <p>The above questions should help to address:</p> <ul> <li>Can RAG give good answers?</li> <li>Do we need to fine tuning an existing model?</li> </ul> <p>Training time includes addressing how to update LLM, how to update the document encoder, and the query encoder. Some major questions to address are: Do we need to pre-train from zero or leverage an existing model? What is the current data source quality? how far the indexing data will be from the future query data. </p>"},{"location":"genAI/rag/#zoom-to-the-rag-process","title":"Zoom to the RAG process","text":""},{"location":"genAI/rag/#document-pipeline","title":"Document Pipeline","text":"<p>The RAG preparation is a very important part of the process to ensure good retrieval results.</p> <p></p> <ul> <li> <p>Text extraction is employed to isolate relevant textual information and remove any noise. Document content is used for keyword or similarity search in any RAG apps. In a document, some elements are important for RAG preparation: the title, narrative text, list item, table, image, but also element's metadata like filename, type, page number and section. It is the most expensive task in the RAG process. Tools like Apache Spark may be used for the data preparation.</p> </li> <li> <p>Chunk creation is to segment text into smaller chunks or sections. This step is essential for efficient retrieval and processing. Chunk size is important to keep the context and some sort of semantic, while overlapping between chunks will help to create vectors that are still close while the text section is bigger than the chunk size. Tuning chunk parameters is balancing between preserving context and keeping accuracy. Sophisticated sentence segmentation techniques can be used. Some references are SBERT and HuggingFace SentenceTransformer.</p> </li> </ul> <p>Document processing is hard because of the different document types (HTML, PDF, Doc, Markdown, Tex...) and structure. It is better to use template and document structure to facilitate content extraction. Extracting metadata requires understanding of the document structure. For pdf, advanced solutions mix OCR and neural network.</p> <p>For short sentence smaller than 256 chars, the all sentence can be used as chunk. For bigger text section, the embedding used may have some better results with chunks from 256 to 512 tokens. The bigger the size of the chunk, the higher the risk of hallucination. While too short, will miss the meaning. The word embeddings introduced with <code>Word2Vec</code> or <code>Bert</code> have still given good results but with a lot of computation.</p> <p>One chunk strategy is to construct chunk from document elements: the document is broken down into atomic elements, which are combined into chunk until reaching a token threshold. Align the chunking break condition to the structure of the document: per section, new title...</p> <ul> <li>The Unstructured.io API Services complemented with a python library can be used for ingesting and digesting unstructured documents of various type (See also deeplearning.ai tutorial).</li> </ul> <p>Code with text splitting:</p> Code Cover build_agent_domain_rag.py LangChain <code>RecursiveCharacterTextSplitter</code>, OpenAI embeddings and Chroma DB for vector store and retriever. Content manager in owl agent framework integrate pdf,docs, html, text, markdown parsers <p>Langchain examples for text processing</p> <p>ChromaDB document processing example</p> Extract from pdf <p>There are different techniques to get content from unstructured file like a pdf. The first is to use object detection to draw and label bounding boxes around the layout elements on a document image (Document Layout Detection). OCR is used to extract text from bounding box. Some pdf as text the extraction can be done without OCR. The second technique use vision transformers.</p>"},{"location":"genAI/rag/#vector-store","title":"Vector Store","text":"<p>The vector store is a database used to store the document chunks and the corresponding embeddings. It is used by the retriever to search for the document chunks related to the query. The first vector databases were based on FAISS, a library for efficient similarity search and clustering of dense vectors.</p> <p>The vector store can be implemented using different databases such as:</p> <ul> <li>Elasticsearch.</li> <li>Faiss.</li> <li>Annoy.</li> <li>HNSW.</li> <li>ChromaDB</li> </ul> <p>The vector store is usually implemented using a distributed database to improve the performance of the retrieval process. </p>"},{"location":"genAI/rag/#frozen-rag","title":"Frozen RAG","text":"<p>This is the Retrieval augmented generation with no training. Data are in context only. The prompt drives the LLM to maximize the in-context learning performance. The selection of the right data chunk and the correct embedding model are crucial.</p> <p></p> <p>The result of the search is pass to the LLM as context. This is limited to in-context learning. </p>"},{"location":"genAI/rag/#retrievers","title":"Retrievers","text":"<p>RAG architecture is based by the retrieval method used, such as BM25 (a traditional one) or more advanced dense retrievers which use neural network based embeddings.</p> <p>The main concept is using the TF-IDF measure: it is a parse (most words never occur) retrieval approach using to compute a cost function for a query within a document, based on the term-frequency (TF) and the inverse document frequency (IDF) which measures the importance of a word to a document. </p> <p>Dense retrieval brings semantic similarity (cosinus score between embeddings) on a dense representations of words and documents by pre-training the retriever with relevant information. </p>"},{"location":"genAI/rag/#retriever-considerations","title":"Retriever considerations","text":"<ul> <li>RAG models need to be fine-tuned to improve the retrieval and the generation processes. The fine tuning includes integrating the specific, domain knowledge to assess the quality of the retrieved information and the generated one.</li> <li>RAG processing needs to take into consideration the different configuration parameters to get good results with minimum latency: consider the number of documents to return, the size of the returned vectors, the total length of the text returned, then number of query to run in parallel. The retrieval vector size impacts the granularity of the semantic match between the query and the documents.</li> <li>Adding efficient metadata about the document (section or chunk) may help to implement an Hybrid search strategy which combines semantic search with filtering on metadata or search on keywords.</li> <li>In a multi-step question answering system, it is challenging to select the correct documents based on the question alone. IRCoT uses LLM to generate a thought sentence used to retrieve documents from the corpus. The documents are then added to the context and the prompt.</li> <li>Effective retrieval is fundamental in RAG system. Assessing the quality of the search results is not easy, and may combine similarity matrix and rule based systems. Different algorithms may be used like cosine similarity, multi query retrievers, ensemble retrievers.</li> <li>RAG may generate wrong results, so some quality control needs to be deployed to remove noise.</li> <li>RAG application design should address all the discovery questions and so use extensive planning, extensive testing using multi-scenario of user behavior and query. Use \"what-if\" simulations. Address hallucination prevention, privacy protection, and source quality control.</li> <li>Start small with all the guard rails in place.</li> <li>Using small LLM for embedding may lead to issues as some terms used in the knowledge based may not be part of the primary LLM corpus. If cost and skill are not an issue, then training its own LLM may be a better solution to reach higher quality, as even fine tuning a model may not bring enough quality to the responses.</li> </ul>"},{"location":"genAI/rag/#assessing-the-quality-of-rag-systems","title":"Assessing the quality of RAG systems","text":"<p>Evaluating the performance of a RAG systems, brings its own challenges, specially the ability to retrieve relevant information. Traditional approach uses human annotations, or heuristic prompts. Annotations is time consuming and expensive, and subject to human biases.</p> <p>Looking at the results, we can have too many matches, or not match at all because of loss of important information within the document which is relevant to the search. The user may want to get the most recent information which may not be the most semantically similar.</p> <p>There are two new approaches to evaluate RAG, the Facts as a Function (FaaS) Katranidis-Barany or ARES (Automated Retrieval Augmented Generation Evaluation System). With FaaS, a fact is a callable function using json objects, used to improve the interpretability of the RAG evaluation.</p> <p>ARES uses LLM to generate query-passage-answer triples and then fine-tuned LLM judges to assess the RAG. It leverages both human annotated data and LLM generated ones.</p>"},{"location":"genAI/rag/#more-advanced-rag","title":"More advanced RAG","text":"<p>The following advanced techniques to improve RAG limitations can be done with sequences of LLM call in a controlled manner or via Agents.</p> <p>The important part is to be able to build a better context content by being able to retrieve the relevant documents (using keyword retrieval, embedding retrieval or an hybrid approach), and order the documents by relevance. Most LLM tends to do better response from the beginning of the context.</p>"},{"location":"genAI/rag/#query-transformations","title":"Query transformations","text":"<p>Query transformations focus on re-writing and / or modifying questions for retrieval using LLM to create other related questions. The prompt declares to generate different version of the user's question. The goal is to try to address limitations of the distance-based similarity search. </p> <ul> <li>With multiple query LLM returns 4 to 5 questions which are used to query the vector store via the retriever and then merge the returned documents by removing any duplicate, to finally send the merged documents to the LLM for generation. </li> </ul> <p></p> <p>See the code in multiple_queries_rag.py.</p> <ul> <li>With Rag fusion the approach is to apply merging logic using a function, so developer can apply some filtering and heuristics:</li> </ul> <p></p> <p>And the related code in rag_fusion.py.</p> <ul> <li>Answer recursively chain the Q&amp;A and use the response of previous calls as part of the input context for the next question. The prompt looks like:</li> </ul> <pre><code>template = \"\"\"Here is the question you need to answer:\n\\n --- \\n {question} \\n --- \\n\nHere is any available background question + answer pairs:\n\\n --- \\n {q_a_pairs} \\n --- \\n\nHere is additional context relevant to the question: \n\\n --- \\n {context} \\n --- \\n\nUse the above context and any background question + answer pairs to answer the question: \\n {question}\n\"\"\"\n</code></pre> <ul> <li>Answer individually use LLM for each question and regroup the answers in the context for the original question. </li> </ul> <p></p> <p>The question creation prompt is rlm/rag-prompt. The final prompt looks like:</p> <pre><code>context = format_qa_pairs(questions, answers)\ntemplate = \"\"\"Here is a set of Q+A pairs:\n\n{context}\n\nUse these to synthesize an answer to the question: {question}\n\"\"\"\n</code></pre> <ul> <li>With Hypothetical Document Embedding (HyDE) the first prompt create an hypothetical document using a prompt like:</li> </ul> <pre><code>template = \"\"\"Please write a scientific paper passage to answer the question\nQuestion: {question}\nPassage:\"\"\n</code></pre> <p>See the related code in rag_HyDE.py.</p>"},{"location":"genAI/rag/#query-routing","title":"Query Routing","text":"<p>When we have multiple indexes for different domains, and for different questions, we want to query different subsets of these indexes.  Query routing is the process of classifying which index or subset of indexes a query should be performed on. For that we use logical and semantic routing.</p>"},{"location":"genAI/rag/#knowledge-graph-integration-in-rag","title":"Knowledge graph integration in RAG","text":"<p>From the standard RAG architecture, the pre-processing step may be modified by adding context to the query before it performs a retrieval from the vector database. This context may specify enterprise specific ontology and term definitions. Since years, Knowledge graphs (KG) are helping search engine to build acronym dictionaries.</p> Knowledge Graph <p>A Knowledge Graph is a set of data points connected by relations that describe a domain, for instance, a business, an organization, or a field of study.</p> <p>Question may be broken down into sub-questions and can require numerous documents to be provided to the LLM to generate an accurate answer.</p> <p>For chunks selection, document hierarchies can be used to reference which documents the query needs to use. One KG with document hierarchy to chunks in the vector database.</p> <p>Use contextual dictionary to understand which document chunks contain important topics. Natural language rules define how to search document related to the meaning of the query.</p> <p>KG may help to add additional information that must exist in any answer referring to a specific concept that failed to be retrieved or did not exist in the vector database. This is the concept of answer augmentation. </p> <p>Rules may be used to eliminate repetition within the LLM results, and personalize response to the users.</p> <p>An hypothetical sequence diagram for a RAG orchestrator enhanced by a knowledge graph, may look like:</p> <pre><code>sequenceDiagram\n    UI-&gt;&gt;Orchestrator: initial query\n    activate Orchestrator\n    Orchestrator-&gt;&gt;KG: initial query\n    deactivate Orchestrator\n    activate KG\n    KG-&gt;&gt;Orchestrator: specific terms and ontology\n    activate Orchestrator\n    deactivate KG\n    Orchestrator-&gt;&gt;Vector Database: augmented query\n    deactivate Orchestrator\n    activate Vector Database\n    Vector Database-&gt;&gt;KG: search document context\n    activate KG\n    KG-&gt;&gt;Vector Database: chunks mapping\n    deactivate KG\n    Vector Database-&gt;&gt;Orchestrator: context for llm\n    deactivate Vector Database\n    activate Orchestrator\n    Orchestrator-&gt;&gt;LLM: query, context\n    activate LLM\n    LLM-&gt;&gt;Orchestrator: LLM response\n    deactivate LLM\n    Orchestrator-&gt;&gt;KG: LLM response\n    KG-&gt;&gt;Orchestrator: augmented LLM response\n    Orchestrator-&gt;&gt;UI: augmented LLM response\n    deactivate Orchestrator\n</code></pre> <p>Knowledge graph is easily extractable in a coherent form. </p>"},{"location":"genAI/rag/#sources-of-information","title":"Sources of information","text":"<ul> <li> <p>Read more from this medium article.</p> </li> <li> <p>Pykg2vec- Python Library for KGE Methods</p> </li> <li>Deeplearning.ai - Preprocessing Unstructured Data for LLM Applications</li> </ul>"},{"location":"genAI/rag/#some-code-studies","title":"Some Code Studies","text":""},{"location":"genAI/rag/#langchain","title":"LangChain","text":"<p>For a classical RAG using LangChain:</p> <p></p> <p>RAG produces good results, due to augmenting use-case specific context coming directly from vectorized information stores. It has the highest degree of flexibility when it comes to changes in the architecture. We can change the embedding model, vector store and LLM independently with minimal to moderate impact on other components.</p> <p>Training from scratch produces the highest quality result amongst Prompt, RAG, fine tuning, but cost far more and need deep data science skill set.</p> <p>See hands-on with LangChain.</p> <ul> <li>Llm-langchain RAG folder</li> </ul>"},{"location":"genAI/rag/#langgraph","title":"LangGraph","text":""},{"location":"genAI/ref-arch/","title":"Ref arch","text":"<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>"},{"location":"genAI/ref-arch/#reference-architecture-for-llm-solution","title":"Reference Architecture for LLM solution","text":"<p>Developing and serving custom LLMs require a lot of work around managing infrastructure, data, models, pipelines, prompts, context windows, application states, observability, embeddings, storage mechanisms, caching and augmented generation. </p>"},{"location":"genAI/ref-arch/#application-integration-with-llm","title":"Application integration with LLM","text":"<p>This section present a generic reference architecture as defined by A16Z for a LLM solution.  I extended their architecture with event streaming as a source of data and knowledge:</p> <p></p> <ol> <li> <p>Data pipelines are batch processing, which in the context of LLM, may combine unstructured documents with structured CSVs, Json, or SQL table content. This data processing may be done in a map-reduce platform, like Apache Spark, to perform the Extract Transform Load job. Most of existing pipelines land their output to Data Lake. But modern data pipelines may call directly a LLM to build embeddings and save them into a Vector Store. The flow looks like in the figure below, which is based on classical Retrieval Augmented Generation (RAG) process.</p> <p></p> <p>RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context.</p> </li> <li> <p>Streaming is where connection to event-driven architecture lands: a lot of business services / microservices are generating important events to be part of the future context of the end-user interaction with the application. Those events can be aggregated, and a similar pipeline can be done with streaming application, consuming events, and doing the embedding via LLM calls then push to Vector Store.</p> </li> <li>Embeddings is the technique to create a numerical vector representation of each document chunks. To support embeddings creation, there are a set of open-source solutions, like the Sentence Transformers library from Hugging Face, or proprietary solutions using hosted APIs.</li> <li>Vector Store, persits vectors, a numerical representation of NL sentence, with indexing capability and similarity search function. Multiple solutions exist as Vector Store: Faiss, ChromaDB, AWS OpenSearch, Redis, Kendra, OpenSearch Serverless, RDS for PostgreSQL, Aurora PostgreSQL, Pinecone.</li> <li>Hosted LLM is a model serving service with LLM accessed via API. </li> <li>Orchestrator is the solution code, which connects all those components together. It may use session caching in distributed, cloud based environment, uses Vector Store to do silimarity semantic search, and exposes API to be used by a ChatBot or a Q&amp;A user interface.</li> </ol> <p>=======</p>"},{"location":"genAI/ref-arch/#reference-architecture-for-llm-solution_1","title":"Reference Architecture for LLM solution","text":"<p>Developing and serving custom LLMs require a lot of work around managing infrastructure, data, models, pipelines, prompts, context windows, application states, observability, embeddings, storage mechanisms, caching and augmented generation. </p>"},{"location":"genAI/ref-arch/#application-integration-with-llm_1","title":"Application integration with LLM","text":"<p>This section present a generic reference architecture as defined by A16Z for a LLM solution.  I extended their architecture with event streaming as a source of data and knowledge:</p> <p></p> <ol> <li> <p>Data pipelines are batch processing, which in the context of LLM, may combine unstructured documents with structured CSVs, Json, or SQL table content. This data processing may be done in a map-reduce platform, like Apache Spark, to perform the Extract Transform Load job. Most of existing pipelines land their output to Data Lake. But modern data pipelines may call directly a LLM to build embeddings and save them into a Vector Store. The flow looks like in the figure below, which is based on classical Retrieval Augmented Generation (RAG) process.</p> <p></p> <p>RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context.</p> </li> <li> <p>Streaming is where connection to event-driven architecture lands: a lot of business services / microservices are generating important events to be part of the future context of the end-user interaction with the application. Those events can be aggregated, and a similar pipeline can be done with streaming application, consuming events, and doing the embedding via LLM calls then push to Vector Store.</p> </li> <li>Embeddings is the technique to create a numerical vector representation of each document chunks. To support embeddings creation, there are a set of open-source solutions, like the Sentence Transformers library from Hugging Face, or proprietary solutions using hosted APIs.</li> <li>Vector Store, persits vectors, a numerical representation of NL sentence, with indexing capability and similarity search function. Multiple solutions exist as Vector Store: Faiss, ChromaDB, AWS OpenSearch, Redis, Kendra, OpenSearch Serverless, RDS for PostgreSQL, Aurora PostgreSQL, Pinecone.</li> <li>Hosted LLM is a model serving service with LLM accessed via API. </li> <li>Orchestrator is the solution code, which connects all those components together. It may use session caching in distributed, cloud based environment, uses Vector Store to do silimarity semantic search, and exposes API to be used by a ChatBot or a Q&amp;A user interface.</li> </ol> <p>0d6401c17707d5173c4bc1b6e287c4b1ac9c4f0f</p>"},{"location":"genAI/ref-arch/#llm-training-architecture","title":"LLM training architecture","text":""},{"location":"genAI/review/","title":"AI discussions","text":"<p>The goal of this section is to get a set of content to support deeper discussions around Gen AI, during chit-chat or interviews.</p>"},{"location":"genAI/review/#1-explain-llm-fundamentals","title":"1. \ud835\uddd8\ud835\ude05\ud835\uddfd\ud835\uddf9\ud835\uddee\ud835\uddf6\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\uddf3\ud835\ude02\ud835\uddfb\ud835\uddf1\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\uddee\ud835\uddf9\ud835\ude00","text":"<p>Cover the high-level workings of models like GPT-3, including transformers, pre-training, fine-tuning, etc.</p> <ul> <li> General LLM introduction</li> <li> Transformer and GPT-3 summary</li> <li> How LLM pre-training is done</li> <li> How to fine tune existing model</li> <li> How RAG works</li> </ul> Some code samples <ul> <li>OpenAI API Code review from openai_api.py. See readme to run the code.</li> </ul>"},{"location":"genAI/review/#2-discuss-prompt-engineering","title":"\ud835\udfee. \ud835\uddd7\ud835\uddf6\ud835\ude00\ud835\uddf0\ud835\ude02\ud835\ude00\ud835\ude00 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\ude01 \ud835\uddf2\ud835\uddfb\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf4","text":"<p>Talk through techniques like demonstrations, examples, and plain language prompts to optimize model performance.</p> <ul> <li>Prompt Engineering</li> <li>Demonstrate prompt engineering</li> <li>How to optimize model response performance with prompt</li> </ul>"},{"location":"genAI/review/#3-share-llm-project-examples","title":"\ud835\udfef. \ud835\udde6\ud835\uddf5\ud835\uddee\ud835\uddff\ud835\uddf2 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf7\ud835\uddf2\ud835\uddf0\ud835\ude01 \ud835\uddf2\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00","text":"<p>Walk through hands-on experiences leveraging models like GPT-3, Langchain, or Vector Databases.</p> <ul> <li>Review RAG positioning, architecture</li> <li>Streamlit app to demonstrate RAG with Chromadb. Offline tool to create vector store and indexing build_agent_domain_rag.py using a Lilian Weng's multi-agents blog.</li> <li>Multiple queries RAG with LangChain</li> </ul>"},{"location":"genAI/review/#4-stay-updated-on-research","title":"\ud835\udff0. \ud835\udde6\ud835\ude01\ud835\uddee\ud835\ude06 \ud835\ude02\ud835\uddfd\ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddf2\ud835\uddf1 \ud835\uddfc\ud835\uddfb \ud835\uddff\ud835\uddf2\ud835\ude00\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5","text":"<p>Mention latest papers and innovations in few-shot learning, prompt tuning, chain of thought prompting, etc.</p> <ul> <li>few-shot learning</li> <li>chain of thought</li> <li>Agentic</li> <li></li> </ul>"},{"location":"genAI/review/#5-dive-into-model-architectures","title":"\ud835\udff1. \ud835\uddd7\ud835\uddf6\ud835\ude03\ud835\uddf2 \ud835\uddf6\ud835\uddfb\ud835\ude01\ud835\uddfc \ud835\uddfa\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 \ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\ude00","text":"<p>Compare transformer networks like GPT-3 vs Codex. Explain self-attention, encodings, model depth, etc.</p>"},{"location":"genAI/review/#6-discuss-fine-tuning-techniques","title":"\ud835\udff2. \ud835\uddd7\ud835\uddf6\ud835\ude00\ud835\uddf0\ud835\ude02\ud835\ude00\ud835\ude00 \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\uddf5\ud835\uddfb\ud835\uddf6\ud835\uddfe\ud835\ude02\ud835\uddf2\ud835\ude00","text":"<p>Explain supervised fine-tuning, parameter efficient fine tuning, few-shot learning, and other methods to specialize pre-trained models for specific tasks.</p> <ul> <li>supervised fine-tuning</li> <li></li> </ul>"},{"location":"genAI/review/#7-demonstrate-production-engineering-expertise","title":"\ud835\udff3. \ud835\uddd7\ud835\uddf2\ud835\uddfa\ud835\uddfc\ud835\uddfb\ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddf2 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddf2\ud835\uddfb\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf2\ud835\ude05\ud835\uddfd\ud835\uddf2\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\ude00\ud835\uddf2","text":"<ul> <li>From tokenization to embeddings to deployment, showcase your ability to operationalize models at scale, and monitoring model inference.</li> </ul>"},{"location":"methodology/","title":"Implementation solution methodology","text":"Version <p>Created 02/2024 - Updated July 2024</p>"},{"location":"methodology/#introduction","title":"Introduction","text":""},{"location":"methodology/#what-is-agentic-applications","title":"What is agentic applications","text":"<p>Explain the purpose of the guide, its intended audience (developers, designers, project managers, etc.), and the scope of the methodology it outlines. </p> <p>TBW The adoption of LLM in an enterprise ecosystem should not be an afterthought process. Leaders should go over the hype, and really address what those technologies can help them with. Interesting to see that companies who offer hosted LLMs in the cloud are not encouraging their own employees to use those LLMs internally. With text generation and text enhancement, all employees will bring the same quality of results and it will be difficult to measure them.</p> <p>Currently AI systems are helping to take million of business decisions every days using the classical expert systems, combined or not with ML predictive scoring. Any business process activities involving human task, may be able to leverage better automation by adopting AI agent to help them doing their task quicker with better results and accuracy. Actually LLM has poor accuracy results to be trustable for any business. But there is way to improve this.</p> <p>The classical LLM use cases of summarization, text improvement, and idea generations are useful for a lot of business activities, and hallucinations have no dramatic impact as the human is still taking the lead on acting from the LLM responses. Human may judge the result content, pick and choose from the returned text what makes the most sense.</p> <p>The current tendency to imagine LLM being an orchestrator of functions or services and therefore being embeddable into a business process, may not be acceptable in many use cases. Process automation needs consistency, high level of accuracy and coded business logic to manage orchestration and compensation flows when exceptions occur. </p> <p>Nevertheless chatbot or Q&amp;A interface may be integrated in any business application where a human needs to process data. We can list the following capabilities the project methodology should be help developers to address:</p> <ul> <li>Helping users in the context of where they are in the current business application or workflow. Current Chatbot with coded conversation flow are the cheapest solution to implement helpful assistant. But with its strong NLP capability LLM will be more flexible and user friendly. The knowledge corpus can come from RAG or fine tuned model.</li> <li>Summarizing the information gathered so far in the context of different customer interactions.</li> <li>Searching in enterprise knowledge base for answer to customer's query</li> <li>Proposing a sequence of actions to address a certain problem based on internal guidelines</li> </ul> <p>TBC  For each function or service to integrate, developer needs to review the interface characteristics which provided semantic and behavior description of the system to integrate with. There are a lot of best practices inherited from SOA, that should help us deciding, how much a LLM can do for integration, and how much they may help human takes the next best actions.</p>"},{"location":"methodology/#introduction_1","title":"Introduction","text":"<p>The adoption of generative AI in the enterprise, follows the same project management practices as the ones of any other data, analytics and decision system solution implementations. In this chapter, we define what are the specifics methodology requirements to address and the proposed project phases to consider.</p> <p>The proposed methodology uses the successful practices used during thousands of consulting engagements around decision automation solution implementations.</p> <p>Always starts by the business problem to solve.</p>"},{"location":"methodology/#classical-business-process-generative-ai-can-help-to-address","title":"Classical business process Generative AI can help to address","text":"<p>In the context of a business process, like in insurance claim processing, there are some activities that were already automated with the adoption of decision services, which uses inference rule engine, (the AI of the 90s), with hundred of rules to address insurance product configuration, feature eligibility, pricing computation, basic claim acceptance, risk scoring, to name a few.</p> <p>Parts of those process, have human activities, and the complete orchestration is done by business process management system or application logic mostly coded as state machines. When humans are involved, they verify documents, correlate information between different data sources, some time rekey data into different systems, and apply some non-coded business policies, but documented somewhere to decide to accept, reject the claim or most likely ask for more information. </p> <p>Obviously with SOA and microservice adoption, rekeying data should be history, and most middleware integrations, can move those data between systems. Document processing is already well covered by OCR products. The document processing with OCR gives good results but are not 100% accurate, Robot Process Automation (RPA) has helped to integrate new data source to legacy systems, but still has issue in training underlying models. </p> <p>So how a Gen AI, multi model deep learning, may help in this claim example? Human can get document summarization, query a corporate document corpus to get better semantic search results, extract specific facts relevant to the claim from unstructured data like emails, paper mail scanned in pdf, or even images. Gen AI will not take decision. ML predictive scoring, classification models do not take decision, human does... or code or business rule engine do.</p> <p>Consider bring AI into business processes as soon as you see the following behavioral patterns:</p> <ul> <li>humans are involved to make the process progressing</li> <li>the process is running for a long time. Straight thought processing flows are already fine tuned for automation and bringing unstructured response may more impact the latency than improves it.</li> <li>humans needs a lot of knowledge about the business, the process, existing historical cases, to take better decisions.</li> <li>SMEs are leaving company, new comers need to be efficiently trained,</li> <li>human asking help from subject matter experts when a case is more complex.</li> <li>human may needs to consult other data sources to get better data to take decision. </li> </ul> <p>Event storming methodology helps to identify the business process to model. During this workshop we can add special notation to highlight human tasks candidate to be supported by generative AI.</p>"},{"location":"methodology/#methodology-requirements","title":"Methodology requirements","text":"<p>The methodology needs to support:</p> <ul> <li>a simple and short requirement discovery phase, and how to choose the right use case</li> <li>focusing on the task within a business process where AI may help human actors, so we can narrow the scope of the solution to bring immediate value</li> <li>the definition of a sustainable architecture, selecting the products, tools or software components for what they are supposed to help with,</li> <li>the implementation of  minimum valuable product to proof the value of the proposed solution with working software</li> <li>the integration of continuous end user feedbacks with solution playback practices and continuous measurements.</li> <li>how to integrate data and AI across any cloud</li> <li>how to measure AI bias, deploy in a secure and compliant environment</li> </ul>"},{"location":"methodology/#best-practices","title":"Best practices","text":"<p>As we focus in business problem to address. the methodology adopts the following well known and adopted practices of:</p> <ul> <li>Design thinking to address human's needs and perspectives of the end-users. It encourages developing empathy map, pain and challenges, end user's problem, develop innovative ideas to solve the problem at hand, and prototyping experimentations to get earlier feedbacks.</li> <li>Lean startup with Minimum Viable Product implementations, measure against hypothesis and pivot practices.</li> <li>Agile development practices with test-driven development.</li> <li>GitOps to simplify hybrid cloud solution deployment, using infrastructure as code</li> <li>Decisions and data focus to address the need for data, and normalize the decisions to be taken in the context of a business process, a business applications and the data to use.</li> </ul> <p>AI helps to take business decision, if not, it is useless. So understanding the decisiosn to take are keys to the success of any AI project implementation.</p>"},{"location":"methodology/#discovery-assessment","title":"Discovery Assessment","text":"<p>When engaging with a customer it is important to assess where they are in their GenAi adoption, and essentially what use cases they are planning to support. Think big about the opportunities, but start small with problems that cause day-to-day irritations for the employees or customer.</p> Research for opportunities <ul> <li>What manual or repetitive processes could be automated with generative AI?</li> <li>Where do employees spend the most time today gathering information to do their jobs?</li> <li>What customer/user pain points could be addressed with more natural conversation?</li> <li>What content creation processes could be enhanced through AI generated drafts?</li> <li>What expert skills are scarce in your organization that AI models could supplement?</li> <li>What insights could be uncovered from large volumes of unstructured data using AI?</li> <li>What risks or inefficiencies exist from decisions made with incomplete information?</li> <li>Where does communication break down today between teams, customers or regions?</li> <li>What predictions would help you make smarter real-time decisions?</li> <li>What new products, services or business models could AI capabilities enable?</li> <li>What tasks or processes still rely heavily on tribal knowledge that could be systematized?</li> <li>What information gets trapped in siloed systems and could be unlocked with AI?</li> <li>What customer research efforts could be accelerated with interactive AI agents?</li> <li>What compliance processes result in slowdowns getting products/services to market?</li> </ul> Use cases and business needs <ul> <li>What are the potential use cases? B2B, B2C, Employees?</li> <li>Is the use case considered a strategic priority? Sponsor?</li> <li>What is the value associated with the use case?</li> <li>Are subject matter experts available to support the use case?</li> <li>Who is the end user?</li> <li>What are the current user's challenges and pains?</li> <li>What will be the \"ha-ha\" moment for the user?</li> <li>Do you have data sets? Quality?</li> </ul> Experience in AI <ul> <li>Are you using AI in your current business applications, or processes?</li> <li>What are current/past successes by adopting AI in the business execution?</li> <li>What is the current level of ML support needed for your technical staff?</li> <li>Do you need capabilities like summarization, text generation, speech recognition?</li> <li>How will you monitor model performance and detect model drift over time?</li> </ul> Generative AI current adoption <ul> <li>How familiar with Generative AI? and its common use cases?</li> <li>What GenAI technologies have you/are you evaluating?</li> <li>Have you started prototyping?</li> <li>Do you have AI-powered products or features on your roadmap?</li> <li>Do you have tried to use an existing generative models? to tune it?</li> <li>What are the current process to evaluate Gen AI models?</li> <li>What is your risk appetite for model hallucination and its potential consequences?</li> <li>How do you plan to do domain  adaptation? Do you plan to pre-train, fine-tune or do some in-context  prompting for domain adaptation?</li> <li>How frequently data changes? How frequently do you expect to need to retrain models with new data?</li> </ul> Integration needs <ul> <li>Is it a new solution or extending an existing one?</li> <li>Where data coming from?</li> <li>What type of systems to integrate the solution with? </li> <li>Any expected performance requirements? </li> </ul> Security and compliance needs <ul> <li>Code privacy and IP related code control</li> </ul>"},{"location":"methodology/#interface-characteristics","title":"Interface Characteristics","text":"<p>This section lists the full set of interface characteristics to consider for each system to integrate with:</p> <ul> <li> <p>FUNCTIONAL DEFINITION</p> <ul> <li>Principal data objects</li> <li>Operation/function</li> <li>Read or change</li> <li>Request/response objects</li> </ul> </li> <li> <p>TECHNICAL INTERFACE</p> <ul> <li>Transport</li> <li>Protocol</li> <li>Data format</li> </ul> </li> <li> <p>INTERACTION TYPE</p> <ul> <li>Request-response or fire-forget</li> <li>Thread-blocking or asynchronous</li> <li>Batch or individual</li> <li>Message size</li> </ul> </li> <li> <p>PERFORMANCE</p> <ul> <li>Response times</li> <li>Throughput</li> <li>Volumes</li> <li>Concurrency</li> </ul> </li> <li> <p>INTEGRITY</p> <ul> <li>Validation</li> <li>Transactionality</li> <li>Statefulness</li> <li>Event sequence</li> <li>Idempotence</li> </ul> </li> <li> <p>SECURITY</p> <ul> <li>Identity/authentication</li> <li>Authorization</li> <li>Data ownership</li> <li>Privacy</li> </ul> </li> <li> <p>RELIABILITY</p> <ul> <li>Availability</li> <li>Delivery assurance</li> </ul> </li> <li> <p>ERROR HANDLING</p> <ul> <li>Error management capabilities</li> <li>Known exception conditions</li> <li>Unexpected error presentation</li> </ul> </li> </ul> <p>From these characteristics, it does not seem realist to design a solution where the LLM service provider will be able to execute some function code from their SaaS platform to integrate on-premises services, with interface characteristic that is no REST, json, stateless, idempotent, synchronous. So outside of toy solution we need something else to get real adoption.</p>"},{"location":"methodology/#generative-ai-context","title":"Generative AI context","text":"<p>Some cloud provider leaders think, that enterprise will have a large library of dedicated models. Training model cost a lot of money for ensure results. Fine tuning a model may be attractive, but needs experts in deep learning tuning. We are currently more seeing a library of prompts and a small set of reliable LLMs that developers can trust. </p> <p>There will be still developers, data scientists, product managers, to develop solution around LLMs. A solution will use different LLMs and different capabilities to support multiple requirements that a business application needs: summarization, Q&amp;A, chatbot, translation for example will mostly be combined in a lot of enterprise solutions.</p> <p>Data enrichment, prompt engineering, user interface, deployment, HA, multi tenancy, security, decision governance may all be part of any AI solution.</p> <p>Deeplearning.ai proposes the following LLM project life cycle:</p> <p></p>"},{"location":"methodology/#gen-ai-specific-scoping","title":"Gen AI Specific Scoping","text":"<ol> <li>Go over the discovery assessment</li> <li>Define what the key metrics are and how to evaluate the solution. If the use cases fall into the Document Q&amp;A and Document Summarization categories, the metric used will be accuracy. Accuracy will be determined based on the documents (data) provided and the respective questions users ask against the model.</li> <li>Define a list of questions that we expect the application to answer. Be sure to have a list of correct answers. In case of summarization use cases, we need sample summaries and sample questions to generate those summaries for document summarization use cases.</li> </ol>"},{"location":"methodology/#model-evaluation","title":"Model Evaluation","text":"<p>There are web sites to evaluate existing LLMs, but they are based on public data, and may not perform well in the context of a specific use case with private data.</p> <p>The methodology looks like in the following steps:</p> <ul> <li>Select models based on specific use case and tasks</li> <li>Human calibration of the models: understand behavior on certain tasks, fine tune prompts and assess against a ground truth using cosine-sim. Rouge scores can be used to compare summarizations, based on statistical word similarity scoring.</li> <li>Automated evaluation of models: test scenario with deep data preparation, was is a good answer. LLM can be used as a judge: variables used are accuracy, coherence, factuality, completeness. Model card</li> <li>ML Ops integration, self correctness</li> </ul> <p>Considerations</p> <ul> <li>Licensing / copyright</li> <li>Operational</li> <li>Flexibility</li> <li>Human language support</li> </ul>"},{"location":"methodology/#consumers-of-lms","title":"Consumers of LMs","text":"<p>This is the category of application that consumes pre-trained models to generate text, image, videos, audio or code.</p> <p>Building a guide for a methodology to develop agentic applications involves providing a structured framework that developers and designers can follow to create applications that exhibit autonomous and proactive behavior. Here's a suggested structure for your guide: </p>"},{"location":"methodology/#introduction_2","title":"Introduction:","text":"<ul> <li> <p>Provide an overview of the concept of agentic applications, defining what they are, their characteristics, and their potential benefits to users. </p> </li> <li> <p>Explain the purpose of the guide, its intended audience (developers, designers, project managers, etc.), and the scope of the methodology it outlines. </p> </li> </ul>"},{"location":"methodology/#theoretical-foundation","title":"Theoretical Foundation:","text":"<ul> <li> <p>Dive into the theoretical underpinnings of agentic computing, discussing relevant concepts such as artificial intelligence, agency, autonomy, and proactive behavior. </p> </li> <li> <p>Reference existing research and studies in the field to provide a solid academic foundation for the methodology. </p> </li> </ul>"},{"location":"methodology/#methodology-overview","title":"Methodology Overview:","text":"<ul> <li> <p>Present a high-level overview of the steps or phases involved in the methodology. </p> </li> <li> <p>Explain how each step contributes to the development of agentic applications and how they build upon each other. </p> </li> </ul>"},{"location":"methodology/#step-by-step-process","title":"Step-by-Step Process:","text":"<ul> <li> <p>Define the first step, which could be understanding the problem domain and requirements gathering. Discuss techniques for identifying opportunities for agentic behavior and gathering user needs and expectations. </p> </li> <li> <p>Detail the design phase, covering aspects like defining the application's personality, determining the level of autonomy required, and designing interactions that showcase agentic behavior effectively. </p> </li> <li> <p>Move to the development phase, providing guidance on choosing the right technologies, frameworks, and tools to implement agentic behavior. Discuss best practices, design patterns, and potential challenges and how to overcome them. </p> </li> <li> <p>Address the testing and evaluation phase, offering strategies for validating the agentic behavior of the application. Discuss user testing methodologies, metrics for evaluating agentic performance, and techniques for refining the application based on feedback. </p> </li> <li> <p>Include a section on deployment and maintenance, covering considerations for releasing agentic applications into production and ensuring their long-term evolution and improvement. </p> </li> </ul>"},{"location":"methodology/#case-studies","title":"Case Studies:","text":"<ul> <li> <p>Provide real-world examples of agentic applications and how they were successfully developed using the methodology. </p> </li> <li> <p>Detail the specific challenges faced, the solutions implemented, and the outcomes achieved, offering practical insights to readers. </p> </li> </ul>"},{"location":"methodology/#best-practices-and-recommendations","title":"Best Practices and Recommendations:","text":"<ul> <li> <p>Offer a summary of key takeaways, best practices, and recommendations for developing agentic applications effectively. </p> </li> <li> <p>Include tips on common pitfalls to avoid and how to ensure the agentic behavior aligns with user expectations and ethical guidelines. </p> </li> </ul>"},{"location":"methodology/#conclusion-and-next-steps","title":"Conclusion and Next Steps:","text":"<ul> <li> <p>Wrap up the guide by reiterating the key points and summarizing the benefits of following the methodology. </p> </li> <li> <p>Provide suggestions for further reading and resources for those who want to delve deeper into specific aspects of agentic application development. </p> </li> </ul> <p>Remember to keep the language concise and clear throughout the guide, ensuring that it is accessible to your intended audience. Use visuals, diagrams, and examples to illustrate concepts and make the guide more engaging. This structure should provide a solid framework for developers and designers to understand and implement agentic behavior in their applications effectively.</p>"},{"location":"ml/","title":"Machine Learning","text":"<p>Machine learning is a system that automatically learns programs/ functions from data. There is not programming step. The goal is to find a function to predict y from a set of features, X, and continuously measures the prediction performance.</p> <p>Statistics work on data by applying a model of the world or stochastic models of nature, using linear regression, logistic regression, cox model,... </p> <p>Two types of machine learning algorithm:s</p> <ol> <li>Supervised learning.</li> <li>Unsupervised learning.</li> </ol>"},{"location":"ml/#supervised-learning","title":"Supervised learning","text":"<p>The main goal in supervised learning is to learn a model from labeled training data that allows us to make predictions about unseen or future data. </p> <p>Classification problem is when we are trying to predict one of a small number of discrete-valued outputs, such as whether it is Sunny (which we might designate as class 0), Cloudy (say class 1) or Rainy (class 2). The class labels are defined as multiple classes or binary classification task, where the machine learning algorithm learns a set of rules in order to distinguish between the possible classes. Classification can be defined when a human assigns a topic label to each document in a corpus, and the algorithm learns how to predict the label. The output is always a set of sets of items. Items could be points in a space or vertices in a graph.</p> <p>For more detail see this Classifier note</p> <p>Another subcategory of supervised learning is regression classification, where the outcome signal is continuous value output. In the table below the Price is the outcome (y), the square feet, # of bedrooms\u2026 are features</p> <p></p> <p>In regression analysis, we are given a number of predictor (explanatory) variables and a continuous response variable (outcome), and we try to find a relationship between those variables that allows us to predict future outcome. </p>"},{"location":"ml/#unsupervised-learning","title":"Unsupervised learning","text":"<p>Giving a dataset we are able to explore the structure of the data to extract meaningful  information without the guidance of a known outcome variable or reward function.\u00a0</p> <p>Clustering is an exploratory data analysis technique that allows to organize a pile of information into meaningful subgroups (clusters) without having any prior knowledge of their group memberships.</p> <p>See this note for more details.</p>"},{"location":"ml/#reinforcement-learning","title":"Reinforcement learning","text":"<p>The goal is to develop a system (agent) that improves its performance based on interactions with the environment. Through the interactions, an agent can then uses reinforcement learning to learn a series of actions that maximizes the reward via an exploratory trial-and-error approach or deliberative planning.  </p>"},{"location":"ml/#unsupervised-dimensionality-reduction","title":"Unsupervised dimensionality reduction","text":"<p>This is a commonly used approach in feature preprocessing to remove noise from data, which can also degrade the predictive performance of certain algorithms, and compress the data onto a smaller dimensional subspace while retaining most of the relevant information.</p>"},{"location":"ml/#ml-system","title":"ML System","text":"<p>Building machine learning system includes 4 components as outlined in figure below:</p> <p></p> <p>Raw data rarely comes in the form and shape that is necessary for the optimal performance of a learning algorithm. Thus, the preprocessing of the data is one of\u00a0the most crucial step in any machine learning application.\u00a0Clean data are becoming a feature for the training.</p> <p>The model testing is based on one of the three cross-validation types:</p> <ul> <li>Validation: where we split the dataset into three pieces: train, test, and validation.</li> <li>the Leave-one-out (LOOCV): only one data point as test sample, and use other rows for training set.</li> <li>K-fold validation: randomly split the dataset into kfold, and for each fold, we train and record the error.</li> </ul> <p>Many machine learning algorithms also require that the selected features are on the same scale for optimal performance, which is often achieved by transforming the features in the range [0, 1] or around a standard normal distribution with zero mean and the\u00a0unit variance.</p> <p>Some of the selected features may be highly correlated and therefore redundant to a certain degree. </p> <p>In those cases, dimensionality reduction techniques are useful for compressing the features onto a lower dimensional subspace.</p> <p>Reducing the dimensionality of the feature space has the advantage that less storage space is required, and the learning algorithm can run much faster.</p> <p>To determine whether a machine learning algorithm not only performs well on the training set but also generalizes well to new data, we need to randomly divide the dataset into separate training and test sets.</p> <p>In practice, it is essential to compare at least a handful of different algorithms in order to train and select the best performing model.\u00a0</p> <p>First we have to decide upon a metric to measure performance. One commonly used metric is classification accuracy, which is defined as the proportion of correctly classified instances.</p> <p>After selecting a model\u00a0that has been fitted on the training dataset, we can use the test dataset to estimate how well it performs on this unseen data to estimate the generalization error.</p>"},{"location":"ml/#model-representation","title":"Model Representation","text":"<p>The commonly agreed notation used is:</p> <pre><code>m= # of training examples\nX= input variables or features\ny= output or target\n(x(i),y(i)) for the ith training example\n</code></pre> <p>When the number of features is more than one the problem becomes a linear regression.</p> <p>Training set is the input to learning algorithm, from which we generate an hypothesis that will be used to map from X to y.</p> <p>In regression analysis, we are given a number of predictor (explanatory) variables and a continuous response variable (outcome), and we try to find a relationship between those variables that allows us to predict an outcome.\u00a0</p> <p>Hypothesis function <code>h(x)</code> can be represented as a linear function of <code>x</code>:\u00a0 </p> <p></p> <p>Xo = 1.</p> <p>A feature is a vector and <code>T</code> is also a row vector of dimension n+1, therefore <code>h(x)</code> is a matrix multiplication. It is called multivariate linear regression.</p> <p>To find the good coefficients , the algorithm needs to compare the results <code>h(x)</code> using a cost function.</p>"},{"location":"ml/#cost-function","title":"Cost function","text":"<p>One of the key ingredients of supervised machine learning algorithms is to define an objective function that is to be optimized during the learning process. This objective function is often a cost function that we want to minimize. So the weights update will minimize the cost function. The cost function could be the sum squared errors between the outcomes and the target label:</p> <p></p> <p>The algorithm to minimize the cost function is called the gradient descent, and uses the property of the cost function being continuous convex linear, so differentiable:</p> <p></p> <p>The principle is to climb down a hill until a local or global cost minimum is reached. In each algorithm iteration, we take a step away from the gradient where the step size is determined by the value of the learning rate (alpha) as well as the slope of the gradient.</p> <p>When J(Ti) is already at the local minimum the slope of the tangent is 0 so Tj will not change. When going closer to the local minimum the slope of the tangent will go slower so the algo will automatically take smaller steps. If alpha is too big, gradient\u00a0 descent can overshoot the minimum and fail to converge or worse, it could diverge. (The derivative is the slope of the tangent at the curve on point Tj; when derivative is close to zero, it means, we reach a minima).</p> <p>When the unit of each feature are very different the gradient descent will take a lot of time to find the minima. So it is important to transform each feature so they are in the same scale. (e.g. from -1 to 1 range, or [0,1] range)</p> <p>A cost function may be coded in Python as:</p> <pre><code>errors = (y - output)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\ncost = (errors** 2).sum() / 2.0\n</code></pre> <p>where: </p> <p></p> <p>in python:</p> <pre><code>def netInput(self,X):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# compute z = sum(x(i) * w(i)) for i from 1 to n, add the threshold\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return np.dot(X,self.weights[1:]) + self.weights[0]\n</code></pre> <p>The cost\u00a0function is convex continuous linear and can be derived, so that we can use the gradient descent algorithm to find the local minima:</p> <pre><code>def fit(X,y):\n\u00a0\u00a0\u00a0\u00a0weights=np.zeros(1+X.shape[1])\n\u00a0\u00a0\u00a0\u00a0costs=[]\n\u00a0\u00a0\u00a0\u00a0for _ in range(nbOfIteration):\n\u00a0 \u00a0 \u00a0   output = netInput(X)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0errors = (y - output)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# calculate the gradient based on the whole training dataset. Use the matrix * vector\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0weights[1:] += eta * X.T.dot( errors)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0weights[0] += eta * errors.sum()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cost = (errors**2).sum() / 2.0\n\u00a0 \u00a0 \u00a0   costs.append(cost)\n\u00a0\u00a0\u00a0 return costs\n</code></pre> <p>The weight difference is computed as the negative gradient * the learning rate <code>eta</code>.\u00a0To compute the gradient of the cost function, we need to compute the partial derivative of the cost function with respect to each weight <code>w(j)</code>.  So putting all together we have:</p> <p></p> <p>the weight update is calculated based on all samples in the training set (instead of updating the weights incrementally after each sample), which is why this approach is also referred to as \"batch\" gradient descent. So basically to minimize the cost function we took steps into the opposite direction of a gradient calculated from the entire\u00a0training set.</p>"},{"location":"ml/#experiment-tracking","title":"Experiment Tracking","text":"<p>The goal is to assess and track the different machine learning experiments. This is particularly useful when the number of experiments grows. </p> <p>To track we can use python discotionaries to keep model metadata and results. But others solutions includes Tensorboard, MLFlow tracking, Weights and biases system of records.</p>"},{"location":"ml/classifier/","title":"Classifiers","text":"<p>Classification problem is when we are trying to predict one of a small number of discrete-valued outputs. The class labels are defined as multiple classes or binary classification task, where the machine learning algorithm learns a set of rules in order to distinguish between the possible classes.</p> <p>Below is a python example of using the iris flower NIST dataset: 4 features, three potential classes:</p> <pre><code>feature_names= ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] \ntarget_names= ['setosa', 'versicolor', 'virginica']\ndata= [ 5.1,  3.5,  1.4,  0.2], [ 4.9,  3. ,  1.4,  0.2]\n\ntarget= [0, 0, 0, 0,\u20261,1,1,\u2026 2,2,2]\n</code></pre> <p>Three majors components to have for each machine learning classifier:</p> <ul> <li>Representation: define what is the classifier: a rule, a decision tree, a neural network...</li> <li>Evaluation: how to know if a given classifier is giving good or bad results: how to asses result rule.  Could be the <code># of errors</code> on some test set, <code># of recalls</code>, squared error, likelihood?...  We may compute the coverage of a rule: <code># of data points</code> that satisfy the conditions and the <code>accuracy = # of correct predictions / coverage</code>.</li> <li>Optimization: how to search among all the alternatives, greedy search or gradient descent?  One idea is to build a set of rules by finding the conditions that maximize accuracy.</li> </ul> <p>For each dataset, try to humanly inspect the data, and do some plotting diagrams with some attributes over others. Then to select a naive class, look at attribute, where we can derive some basic rules. This will build a first hypothesis. To assess an hypothesis build a confusion matrix: a square matrix where column and rows are the different class label of an outcome. The cells count the number of time the rules classified the dataset. Assess the accuracy number: sum good results/ total results.</p> Code execution <p>All the Classifier Python apps execute well from the python environment in docker. See environement note.</p>"},{"location":"ml/classifier/#perceptron","title":"Perceptron","text":"<p>Based on the human neuron model, Frank\u00a0Rosenblatt proposed an algorithm that would  automatically learn the optimal\u00a0weight coefficients that are then multiplied with the input  features in order to make the decision of whether a neuron fires or not.   In the context of supervised learning and classification, such an algorithm could then be  used to predict if a sample belonged to one class or the other.</p> <p>The problem is reduced to a binary classification (-1,1), and an activation function  that takes a linear combination of input X, with corresponding weights vector W,  to compute the net input as:</p> <pre><code>z = sum(w(i) * x(i)) i from 1 to n\n</code></pre> <p>If the value is greater than a threshold the output is 1, -1 otherwise. The function is called <code>unit step</code> function. </p> <p>If w0 is set to be -threshold and x0=1 then the equation becomes:</p> <p></p> <p>The following python functions in a Perceptron class, use numpy library to compute the matrix dot product wT*x:</p> <pre><code>def\u00a0netInput(self,X):\n\u00a0 \u00a0 \u00a0return np.dot(X,self.weights[1:]) + self.weights[0]\n\u00a0 \u00a0\ndef predict(self,X):\n\u00a0 \u00a0return np.where(self.netInput(X)&gt;=0.0,1,-1)\n</code></pre> <p>The weights are computed using the training set. The value of delta, which is used to update the weight , is calculated by the perceptron learning rule:</p> <p></p> <p>eta is the learning rate, Y(i) is the known answer or target for i th sample. The weight update is proportional to the value of X(i) \u00a0 It is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small.</p> <p></p> <p>The fit function implements the weights update algorithm.</p> <p>Test the python Perceptron implementation, uisnf NIST iris dataset. The way to use the perceptron: Create an instance by specifying the eta coefficient and the number of epochs (passes over the training set) to perform</p> <pre><code>#under ml-python/classifiers folder\npython TestPerceptron.py\n</code></pre> <p>The test loads the dataset, fit the Perceptron with a training set, plots some sample of the two types of Iris. Then displays the decision boundary to classify an Iris in one of the two classes: setosa, versicolor.</p>"},{"location":"ml/classifier/#adaline","title":"Adaline","text":"<p>In  ADAptive LInear NEuron classifier, the weights are updated based on a linear activation function (the <code>Identity</code> function) rather than a unit step function like in the Perceptron.</p> <p></p> <pre><code># Start python docker\n# under ml-python/classifiers folder\npython TestAdaline.py\n</code></pre> <p>The test works on the Iris dataset too, when we choose a learning rate that is too large, we have an error rate that becomes larger in every epoch because we overshoot the global minimum.</p> <p></p> <p>When the features are standardized (each feature value is reduced by the mean and divided by the standard deviation) the ADALine algorithm converges more quickly.</p> <p>The two regions illustrates the two classes, with good results: </p> <p></p> <p>The following curve shows the cost function results per iteration or epoch</p> <p></p> <p><pre><code>X_std = np.copy(X)\nX_std[:,0]=(X[:,0]-np.mean(X[:,0]))/np.std(X[:,0])\nX_std[:,1]=(X[:,1]-np.mean(X[:,1]))/np.std(X[:,1])\n</code></pre> \u00a0 The previous approach can take a lot of time when the dataset includes millions of records. A more efficient approach is to take the stochastic gradient descent approach. It is used with online training, where the algorithm is trained on-the-fly, while new training set arrives.</p> <p>The weights are computed with:\u00a0</p> <p></p> <pre><code>def updateWeights(self,xi,target):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0output = self.netInput( xi)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0error = (target - output)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.weights[1:] += self.eta * xi.dot( error)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.weights[0] += self.eta * error\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cost = (error** 2)/ 2.0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return cost\n</code></pre> <p>To obtain accurate results via stochastic gradient descent, it is important to present it with data in a random order, which is why we want to shuffle the\u00a0training set for every epoch to prevent cycles. \u00a0 </p>"},{"location":"ml/classifier/#logistic-regression","title":"Logistic regression","text":"<p>Another classification approach is to use \u2018Logistic Regression\u2019 which performs very well on linearly separable set:</p> <pre><code>from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression( C = 1000.0, random_state = 0)\nlr.fit( X_train_std, y_train)\nlr.predict_proba(X_test_std[0,:])\n</code></pre> <p>For C=1000 we have the following results:</p> <p></p> <p>Logistic regression uses the odds-ratio <code>P/(1-P)</code>, P being the probability to have event e: in our case P could be the probability that a set of values for the feature X leads that the sample is part of a class 1.\u00a0</p> <p>In fact, the mathematical model uses the <code>log (P/(1-P))</code> as function in the model. It\u00a0takes input values in the range 0 to 1 and transforms them to values over the entire real number range, which we can use to express a linear relationship between feature values and the log-odds:</p> <p></p> <p>For logistic regression, the hypothesis function is used to predict the probability of having a certain sample X being of class y=1. This is the sigmoid function:</p> <p></p> <p>Here, z is the net input, that is, the linear combination of weights and sample features= W\u2019.x\u00a0</p> <p>The sigmoid function is used as activation function in the classifier:</p> <p></p> <p>The output of the sigmoid function is then interpreted as the probability of particular sample belonging to class 1 </p> <p></p> <p>given its features X parameterized by the weights W.</p> <p>Logistic regression can be used to predict the chance that a patient has a particular disease given certain symptoms. As seen before to find the weights W, we need to minimize a cost function, which in the case of logistic regression is:</p> <p></p> <p>The C=1/lambda parameter used in logistic regression api is the factor to control overfitting.</p> <p> is the regularization bias to penalize extreme parameter weights.</p> <p>Logistic regression is a useful model for online learning via stochastic gradient descent, but also allows us to predict the probability of a particular event. </p>"},{"location":"ml/classifier/#maximum-margin-classification-with-support-vector-machines-svm","title":"Maximum margin classification with support vector machines (SVM)","text":"<p>In SVM, the goal is to maximize the margin: the distance between the decision boundary and the training samples.</p> <p></p> <p>The rationale behind having decision boundaries with large margins is that they tend to have a lower generalization error whereas models with small margins are more prone to overfitting.</p> <p>To prepare the data here is the standard code that is using SciKit <code>model_selection</code> to split the input data set into training and test sets and then a standardScaler to normalize values</p> <pre><code>from sklearn import model_selection\nfrom sklearn.preprocessing import StandardScaler\n\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    X, y, test_size=0.3, random_state=0\n)\n\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n</code></pre> <p>To train a SVM model using sklearn:</p> <pre><code>from sklearn.svm import SVC\nsvm = SVC(kernel='linear',C=1.0,random_state=0)\nsvm.fit(X_train_std,y_train)\n</code></pre> <p>See code in SVM-IRIS.py</p> <p>The SVMs mostly care about the points that are closest to the decision boundary (support vectors).</p> <p></p> <p>The SVM can use Radial Basis Function kernel, to create nonlinear combinations of the original features to project them onto a higher dimensional space via a mapping function phi() where it becomes linearly separable. </p> <pre><code>svm = SVC(kernel='rbf',C=10.0,random_state=0, gamma=0.10)\nsvm.fit(X_train_std,y_train)\n</code></pre> <p>Gamma is a cut-off parameter for the Gaussian sphere. If we increase the value for gamma, we increase the influence or reach of the training samples, which leads to a softer decision boundary. Gamma at 0.1. Optimizing Gamma is important to avoid overfitting.</p>"},{"location":"ml/classifier/#decision-trees","title":"Decision Trees","text":"<p>The decision tree model learns a series of questions to infer the class labels of the samples. </p> <p>The algorithm is to\u00a0start at the tree root and to split the data on the feature that results in the largest information gain (IG). In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are pure. This means that the samples at each node all belong to the same class. In practice, this can result in a very deep tree with many nodes, which can easily lead to overfitting. Thus, we typically want to prune the tree by setting a limit for the maximal depth of the tree.</p> <p>In order to split the nodes at the most informative features, we need to define an objective function that we want to optimize via the tree learning algorithm. In binary decision trees there\u00a0are 3 commonly used impurity function:\u00a0Gini_impurity(), entropy(), and the classification_error().</p> <pre><code>def gini(p):\n    return p *(1-p) + (1-p)*(1-(1-p))\n\ndef entropy( p):\n    return - p* np.log2( p) - (1 - p)* np.log2(( 1 - p))\n\ndef error( p):\n    return 1 - np.max([ p, 1 - p])\n</code></pre> <p>Decision trees are particularly attractive if we care about interpretability. </p> <p>See DecisionTreeIRIS.py code and this DecisionTree notebook.</p>"},{"location":"ml/classifier/#combining-weak-to-strong-learners-via-random-forests","title":"Combining weak to strong learners via random forests","text":"<p>Random forests have gained huge popularity in applications of machine learning in 2010s due to their good classification performance, scalability, and ease of use. Intuitively, a random forest can be considered as an ensemble of decision trees. The idea behind ensemble learning is to combine weak learners to build a more robust model, that has a better generalization error and is less susceptible to overfitting.</p> <p>The only parameter to play with is the number of trees, and the max depth of each tree. The larger the number of trees, the better the performance of the random forest classifier at the expense of an increased computational cost. Scikit-learn provides tools to automatically find the best parameter combinations (via cross-validation)</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\nX_train, X_test, y_train, y_test = buildTrainingSet()\nforest = RandomForestClassifier(criterion ='entropy', n_estimators = 10, random_state = 1, n_jobs = 2)\nforest.fit( X_train, y_train)\n...\n</code></pre> <p>the sample size of the bootstrap sample is chosen to be equal to the number of samples in the original training set.</p>"},{"location":"ml/classifier/#k-nearest-neighbor-classifier-knn","title":"k-nearest neighbor classifier (KNN)","text":"<p>For KNN, we define some distance metric between the items in our dataset, and find the K closest items.</p> <p>Machine learning algorithms can be grouped into parametric and nonparametric models. Using parametric models, we estimate parameters from the training dataset to learn a function that can classify new data points without requiring the original training dataset anymore.</p> <p>With nonparametric models there is no fixed set of parameters, and the number of parameters grows with the training data (decision tree, random forest and kernel SVM). </p> <p>KNN belongs to a subcategory of nonparametric models that is described as instance-based learning which are characterized by memorizing the training dataset, and lazy learning is a special case of instance-based learning that is associated with no (zero) cost during the learning process.</p> <p>The KNN algorithm is fairly straightforward and can be summarized by the following steps:</p> <ol> <li>Choose the number of k and a distance metric function.</li> <li>Find the k nearest neighbors of the sample that we want to classify.</li> <li>Assign the class label by majority vote.</li> </ol> <p>In the case of a tie, the scikit-learn implementation of the KNN algorithm will prefer the neighbors with a closer distance to the sample.</p> <p>See the KNN notebook.</p> <p>It is important to mention that KNN is very susceptible to overfitting due to the curse of dimensionality. The curse of dimensionality describes the phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset.</p> <p>The K-nearest neighbor classifier offers lazy learning that allows us to make predictions without any model training but with a more computationally expensive prediction step.</p>"},{"location":"ml/classifier/#see-also","title":"See also","text":"<p>See also classifiers done with PyTorch as neural network.</p>"},{"location":"ml/deep-learning/","title":"Deep learning","text":"<p>Deep learning is a machine learning techniques which uses neural networks with more than one layer.</p>"},{"location":"ml/deep-learning/#neural-network","title":"Neural Network","text":"<p>A Neural Network is a programming approach, based on the biological inspired neuron,  used to teach a computer from training data instead of programming it with structured code.</p> <p>The basic structure of a neural network includes an input layer (called \"feature vector\"), where the data is fed into the model, hidden layers that perform the computational processing, and an output layer that generates the final result. (See YouTube video: \"Neural Network the ground up\").</p> <p>A classical learning example of neural network usage, is to classify images, like the hand written digits of the NIST dataset (shallow_net_demo.ipynb).</p> <p>A simple neuron holds a function that returns a number between 0 and 1. For example in simple image classification, neuron may hold the grey value of a pixel of a 28x28 pixels image (784 neurons). The number is called activation. At the output layer, the number in the neuron represents the percent of one output being the expected response. Neurons are connected together and each connection is weighted.</p> <p>Convolutional neural networks (CNNs) (lenet_in_keras.ipynb) allows input size to change without retraining. For the grey digit classification, the CNN defines a neuron as a unique image pattern of 3x3. The output of the regression neural network is numeric, and the classification output is a class.</p> <p>The value of the neuron 'j' in the next layer is computed by the classical logistic equation taking into account previous layer neurons (<code>a</code>) (from 1 to n (i being the index on the number of input)) and the weight of the connection (<code>a(i)</code> to <code>neuron(j)</code>):</p> <p></p> <p>To get the activation between 0 and 1, it uses the sigmoid function, the bias is a number to define when the neuron should be active.</p> <p></p> <p>Modern neural network does not use sigmoid function anymore but the Rectifier Linear unit function.</p> <p></p> <p>Neural networks input and output can be an image, a series of numbers that could represent text, audio, or a time series...</p> <p>The simplest architecture is the perceptron, represented by the following diagram:</p> <p></p> <p>There are four types of neurons in a neural network:</p> <ol> <li>Input Neurons - We map each input neuron to one element in the feature vector.</li> <li>Hidden Neurons - Hidden neurons allow the neural network to be abstract and process the input into the output. Each layer receives all the output of previous layer.</li> <li>Output Neurons - Each output neuron calculates one part of the output.</li> <li>Bias Neurons - Work similar to the y-intercept of a linear equation. It introduces a 1 as input.</li> </ol> <p>Neurons is also named nodes, units or summations. See the sigmoid play notebook to understand the effect of bias and weights </p> <p>Training refers to the process that determines good weight values.</p> <p>It is possible to use different Activation functions,(or transfer functions), such as hyperbolic tangent, sigmoid/logistic, linear activation function, Rectified Linear Unit (ReLU), Softmax (used for the output of classification neural networks), Linear (used for the output of regression neural networks (or 2-class classification)).</p> <p>ReLU activation function is popular in deep learning because the gradiant descend function needs to take the derivative of the activation function. With sigmoid function, the derivative quickly saturates to zero as it moves from zero, which is not the case for ReLU.</p> <p>The two most used Python frameworks for deep learning are TensorFlow/Keras (Google) or PyTorch (Facebook).</p>"},{"location":"ml/deep-learning/#classification-neural-network-architecture","title":"Classification neural network architecture","text":"<p>The general architecture of a classification neural network.</p> Hyperparameter Classification Input layer shape (in_features) Same as number of features Hidden layer(s) Problem specific, minimum = 1, maximum = unlimited Neurons per hidden layer Problem specific, generally 10 to 512 Output layer shape (out_features) for binary 1 class, for multi-class: 1 per class Hidden layer activation Usually ReLU but can be many others Output activation For binary: Sigmoid, for multi-class: Softmax Loss function Binary cross entropy.  For multi-class Cross entropy Optimizer SGD (stochastic gradient descent), Adam (see torch.optim for more options) <p>Below is an example of a very simple NN in PyTorch, without any activation function:</p> <pre><code>from torch import nn\n\nmodel_0 = nn.Sequential(\n    nn.Linear(in_features=2, out_features=5),  # layer 1\n    nn.Linear(in_features=5, out_features=1)   # layer 2\n).to(device)\n\nmodel_0\n</code></pre> <p>We can use a subclass of pyTorch <code>nn.Module</code> to define the NN. See demonstration in classifier.ipynb notebook, to search for the circle classes in the sklearn circles dataset, or a multi classes classification in multiclass-classifier.ipynb.</p>"},{"location":"ml/deep-learning/#learning","title":"Learning","text":"<p>Same as previous ML problems, we can use supervised ( picture and corresponding classes) and unsupervised learning. For image or voice, the 'self-supervised learning' uses to generate supervisory signals for training data sets by looking at the relationships in the input data.</p> <p>Transfer learning combine a first neural network as input to a second NN. </p>"},{"location":"ml/deep-learning/#gpu-vs-cpu","title":"GPU vs CPU","text":"<ol> <li>When the training loss is way lower than the test loss, it means \"overfitting\" and so loosing time.</li> <li>When both losses are identical, time will be wasted if we try to regularize the model.</li> <li>To optimize deep learning we need to maximize the compute-bound processing by reducing time spent on memory transfer and other things. Bandwidth cost is by moving the data from CPU to GPU, from one node to another, or even from CUDA global memory to CUDA shared memory.</li> </ol>"},{"location":"ml/deep-learning/#computer-image","title":"Computer Image","text":"<p>Address how a computer sees, images.</p>"},{"location":"ml/deep-learning/#convolutional-neural-network","title":"Convolutional Neural Network","text":"<p>A Neural Network to process images by assigning learnable weights and biases to various aspects/objects in the image, and be able to differentiate one from the other. It can successfully capture the spatial and temporal dependencies in an image through the application of relevant filters.</p> <p>Image has three matrices of values matching the size of the picture (H*W) and the RGB value R matrix, G and B matrices. CNN reduces the size of the matrices without loosing the meaning. For that, it uses the concept of Kernel, a window, shifting over the image.</p> <p></p> <p>A typical structure of a convolutional neural network:</p> <p>Input layer -&gt; [Convolutional layer -&gt; activation layer -&gt; pooling layer] -&gt; Output layer</p> <p>The layers between [] can be replicated.</p> <p>Every layer in a neural network is trying to compress data from higher dimensional space to lower dimensional space. Below is an example of this method:</p> <pre><code># Convolutional layer\nnn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\nnn.ReLU(),  # activation layer\n# pooling layer\nnn.MaxPool2d(kernel_size=2, stride=2),    \n</code></pre> <ul> <li>Conv2d is compressing the information stored in the image to a smaller dimension image</li> <li>MaxPool2d takes the maximum value from a portion of a tensor and disregard the rest.</li> </ul> <p>See this CNN explainer tool.</p> <p>Simple image dataset using the Fashion NIST.</p> <p>The non-linear classifier and one CNN is in fashion_cnn.py.</p> <p>MIT - Convolutional Neural Network presentation - video</p>"},{"location":"ml/deep-learning/#transfer-learning","title":"Transfer Learning","text":"<p>Take an existing pre-trained model, and use it on our own data to fine tune the parameters. It helps to get better results with less data, and lesser cost and time. In Computer Vision, Image Net includes million of images on which models were trained. </p> <p>PyTorch has pre-trained models, Hugging Face too, PyTorch Image Models - Timm is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts that can be reused. Paper with code is a collection of the latest state-of-the-art machine learning papers with code implementations attached to the article.</p> <p>The custom data going into the model needs to be prepared in the same way as the original training data that went into the model: </p> <pre><code># load existing NN weights\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n# Get the transforms used to create our pretrained weights\ntransformer= weights.transforms()\n</code></pre> <p>The transformer is used to create the data loaders:</p> <pre><code>train_dl,test_dl, classes=data_setup.create_data_loaders(\n                            train_dir,\n                            test_dir,\n                            transformer,\n                            transformer,\n                            batch_size=BATCH_SIZE)\n</code></pre> <p>Then, take an existing model. Often bigger models are better but results may also being linked to the type of device used and the hardware resource capacity. <code>efficientnet_b0</code> has 288,548 parameters.</p> <pre><code>model=torchvision.models.efficientnet_b0(weights=weights).to(device)\n</code></pre> efficientnet_b0 parts <p><code>efficientnet_b0</code> comes in three main parts:</p> <ul> <li>features: A collection of convolutional layers and other various activation layers to learn a base representation of vision data.</li> <li>avgpool: Takes the average of the output of the features layer(s) and turns it into a feature vector.</li> <li>classifier: Turns the feature vector into a vector with the same dimensionality as the number of required output classes (since efficientnet_b0 is pretrained on ImageNet with 1000 classes.</li> </ul> <p>The process of transfer learning usually goes: freeze some base layers of a pre-trained model (typically the features section) and then adjust the output layers (also called head/classifier layers) to suit the needs.</p> <pre><code>for param in model.features.parameters():  # Freeze the features\n    param.requires_grad = False\n\nmodel.classifier = torch.nn.Sequential(\n        torch.nn.Dropout(p=0.2, inplace=True), \n        torch.nn.Linear(in_features=1280, \n                        out_features=len(classes), \n                        bias=True)).to(device)\n</code></pre> <p>Dropout layers randomly remove connections between two neural network layers with a probability of p.  This practice is meant to help regularize (prevent overfitting) a model by making sure the connections that remain learn features to compensate for the removal of the other connections.</p> <p>See PyTorch transfer learning for image classification code.</p>"},{"location":"ml/deep-learning/#sources-of-information","title":"Sources of information","text":"<ul> <li>Big source of online book Dive into Deep Learning from Amazons.</li> <li>Udemy PyTorch for deep learning</li> <li>Horace He- Making Deep Learning Go Brrrr From First Principles</li> <li>MIT - CNConvolutional Neural Network presentation - video</li> </ul>"},{"location":"ml/nlp/","title":"Natural Language Processing (NLP)","text":""},{"location":"ml/nlp/#embedding","title":"Embedding","text":"<p>An embedding is a mathematical representation of a set of data points in a lower-dimensional space that captures their underlying relationships and patterns. There are different embedding types: image, word, sentence, graph, video embeddings. CLIP, from OpenAI, is an embedding for both text and image in the same vector spaces, so enabling text to image generation.</p> <p>The vector numbers intent to capture the attributes of the object and the semantic and syntactic relationships between words. Dense embeddings were introduced by Google\u2019s Word2vec (Mikolov et al) in 2014 and used in GPT model. The transformation of word to vector, gives the capability to compute arithmetics with words, like similarity computation. Vectors which are closer together, mean they represent semantically similar concepts. </p> <p>Sentence embedding vectorize a complete sentence to do semantic similarity of sentences.  The technique works by training a neural network on a large corpus of text data, to predict the context in which a given word appears. Principal Component Analysis (PCA) and Singular Value Decomposition (SVD), auto-encoder, are dimensionality reduction techniques. </p> <p>See this basic code which uses <code>SentenceTransformer all-MiniLM-L6-v2</code> model to encode sentences of 100 tokens, construct from a markdown file.</p> <p>Embeddings are created using a pre-trained LLM, and a set of documents used to fine-tune the model. The fine-tuning process is done using a small subset of the documents, and the LLM is trained to predict the next word in the document. </p> <p>The fine-tuned LLM is then used to generate the embeddings. The embedding size is usually between 200 to 1000 dimensions. </p> <p>The embedding process is time consuming, and may take several days to complete. The embedding model is usually saved and re-used, and most of the time in open access. </p> <p>Embeddings are used to compare the query with the document chunks. The cosine similarity is used to compute the similarity between the query and the document chunks. </p> <p>The cosine similarity is a measure of the similarity between two non-zero vectors of an inner product space. It is defined to equal the cosine of the angle between them, which is also the same as the inner product of the same vectors normalized to both have length 1. </p> <p>Embedding can improve data quality, reduce the need for manual data labeling, and enable more efficient computation. It is used for similarity search, RAG and recommendations engine by using product description embeddding and simililarity searches.</p> <p>See the Encord's guide to embeddings in machine learning and * Deeplearning.ai - embedding models courses</p>"},{"location":"ml/nlp/#use-cases","title":"Use cases","text":"<ul> <li>LLM for token embedding</li> <li>Image embedding to represent images in the vector space.</li> <li>Audio and video embedding </li> <li>RAG with sentence embedding and similarity search.</li> <li>Product recommendations, via similarity search</li> <li>Anomaly detection </li> </ul>"},{"location":"ml/nlp/#bert","title":"BERT","text":"<p>Bidirectional Encoder Representations from Transformers (BERT) is a family of masked-language models published in 2018 by researchers at Google. It is much smaller than current LLMs, so if the task can be accomplished by BERT it can be very helpful for developers - however it usually does not perform as well as other foundation models because it is not large enough. </p>"},{"location":"ml/nlp/#named-entity-recognition","title":"Named Entity Recognition","text":"<p>Named Entity Recognition (NER) is a Natural Language Processing (NLP) technique used to identify and extract important entities from unstructured text data. It is achieved by using NN trained on labeled data to recognize patterns and extract entity from text.</p> <p>Some techniques uses Gen AI model to do NER with a good prompt.</p>"},{"location":"ml/nlp/#deeper-dive","title":"Deeper Dive","text":"<ul> <li>PyTorch based NLP tutorial</li> <li>Deeplearning.ai - embedding models</li> </ul>"},{"location":"ml/unsupervised/","title":"Unsupervised  Learning","text":"<p>Unsupervised algorithms don't make use of a target. The goals is to learn some property of the data, to represent the structure of the features in a certain way.</p> <p>Clustering is the technic to group data based  on how similar they  are to each other. Adding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity. Cluster feature  is categorical.</p> <p>The motivating idea for adding cluster labels is that the clusters will break up complicated  relationships across features into simpler chunks. Our model can then just learn the simpler chunks  one-by-one instead having to learn the complicated whole all at once. It's a \"divide and conquer\" strategy.</p> <p>K-means clustering measures similarity using euclidean distance. It creates clusters by placing a  number of points, called centroids, inside the feature-space. Each point in the dataset is  assigned to the cluster of whichever centroid it's closest to. The \"k\" controls how many centroids  to create.</p> <p>k-means clustering is sensitive to scale, so it is a good idea rescale or normalize data with extreme values. As a rule of thumb, if the features are already directly comparable (like a test result at different  times), then you would not want to rescale. On the other hand, features that aren't on comparable   scales (like height and weight) will usually benefit from rescaling.</p> <p>For example  in housing price prediction, lot area and living area  may  need  to be scaled to avoid big lot to impact too much the price.</p> <p>Here is an example of cluster labels:</p> <pre><code># Define a list of the features to be used for the clustering\nfeatures = [\"LotArea\", \"TotalBsmtSF\", \"FirstFlrSF\", \"SecondFlrSF\",\"GrLivArea\"]\n\n# Standardize\nX_scaled = X.loc[:, features]\nX_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n\n\n# Fit the KMeans model to X_scaled and create the cluster labels\nkmeans = KMeans(n_clusters=10,n_init=10 random_state=0)\nX[\"Cluster\"] =  kmeans.fit_predict(X_scaled)\n</code></pre> <p>Use cluster distance:</p> <pre><code>kmeans = KMeans(n_clusters=10, n_init=10, random_state=0)\n\n# Create the cluster-distance features using `fit_transform`\nX_cd = kmeans.fit_transform(X_scaled)\n\n# Label features and join to dataset\nX_cd = pd.DataFrame(X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])])\nX = X.join(X_cd)\n</code></pre>"},{"location":"neuro-symbolic/","title":"Hybrid AI","text":"<p>Hybrid AI or Neuro Symbolic AI combines elements of neuron networks and symbolic reasoning to develop intelligent systems. Neural networks excel at pattern recognition and learning from data, while  traditional AI like rule engines focuses on logic, reasoning, and symbolic representations.</p> <p>The goal is to create more interpretable, explainable, and robust AI systems.</p> <p>Symbolic reasoning can operate on the structured knowledge graph to perform tasks such as question answering, knowledge inference, and logical reasoning.</p> <p>Knowledge graphs represent information in a structured and semantically rich manner, using entities, relationships, and attributes</p> <p>LLMs are trained on static document sets, which means gaps exist with newly created knowledge. RAG helps to address this problem, but there is still gaps in semantic controlled response.</p>"},{"location":"neuro-symbolic/#use-cases","title":"Use Cases","text":"<p>We may consider three entry points for transforming existing business processes with AI and automation: </p> <ol> <li>Process automation using STP, integration of human workflow with document classification and data capture.</li> <li>Decision based on business policies, including risk scoring, fraud detection, KYC</li> <li>Improved user experience to find solutions and make adhoc decisions.</li> </ol> <p>LLM are amazing tool for understanding and generating natural language, however they are not able to make consistent business decisions. </p> <ul> <li>Healthcare: Deep learning can do medical images or analog graph pattern recognition, predictive analytics, classification, with symbolic reasoning to deliver personalized treatment recommendations, or help on diagnostic.</li> <li>Complaint management: combine workflow, chatbot, decision rules for next best actions, product recommendation, ML for sentiment analysis. </li> <li>Financial risk management: is about continuously and dynamically altering the user experience to reduce false positives and slow down or stop adversarial patterns. Risk identification is about integrating event and real time processing to get user action context within time windows. Risk context comes from data that may come from different sources. Risk scoring includes a combination of ML model developed on top of structured or unstructured data (anomaly detection), and business logic coded in a form of <code>if condition then action</code> rules and rule flow to organize the rule execution in minimum calls. Decision acts in the moment of the transaction.</li> </ul> Some risky behaviors <pre><code>1. If user tried to login 3 times in the last 10 minutes, change the password, add a new beneficiary, and trigger a transfer once the account is accepted. Some data elements may be of interest to assess for potential fraud, like the user IP address, the country of origin, the type of bank or country for the beneficiary.\n1. Two gas transactions within 5 minutes in the same gaz stations is most likely a fraud.\n1. Transaction amount higher than average spent habit.\n</code></pre> <p>As soon as we add a touch point with a human, like a natural language interface, we have to think about risk and playing the system.</p>"},{"location":"neuro-symbolic/#intelligent-assistant","title":"Intelligent Assistant","text":"<p>A tool which accesses the business applications any user accesses during a work day, gathers the information, curates it. It interacts with natural language, understands intent, completes a multi-step tasks across applications, systems, and people. The assistant learns over time on how we work with the systems.</p> <p>WatsonX Orchestrate uses NLP, Gen AI and skills to help implement custom orchestration. Skill is function wrapper with description. IBM predefined a set of skill like integration to SAP, Gmail...</p> <p>As any API can be wrapped into a skill or tool, then it can be orchestrated by a LLM. </p>"},{"location":"neuro-symbolic/#semantic-router","title":"Semantic Router","text":"<p>A semantic router serves as a sophisticated decision-making layer that can select the most appropriate language model or response for each user query. It uses semantic vector space to align user questions with the most fitting predefined responses. The router helps LLM or any action to make decisions or augment query with more info or add more context to find the best answers from a list of possible ones.</p> <p>Some use cases where semantic routing will be relevant:</p> <ul> <li>Defense against malicious query attacks as it may discern and counteract potential threats.</li> <li>Avoid sensitive topic to avoid inappropriate content</li> <li>Simplify function calling within applications</li> <li>Optimize database query and RAG query</li> </ul>"},{"location":"neuro-symbolic/#rule-engine","title":"Rule Engine","text":""},{"location":"neuro-symbolic/#sources","title":"Sources","text":"<ul> <li>Solving Reasoning Problems with LLMs in 2023</li> <li>Connecting AI to Decisions with the Palantir Ontology</li> <li>Semantic Router super fast decision layer for LLMs and AI agents.</li> </ul>"},{"location":"solutions/","title":"A set of simple studies and solutions","text":""},{"location":"solutions/#predict-whether-a-mammogram-mass-is-benign-or-malignant","title":"Predict whether a mammogram mass is benign or malignant","text":"<ul> <li>Data: The dataset can be found from University of Irvine: Mammographic Mass. </li> <li>Goal: Build a Multi-Layer Perceptron and train it to classify masses as benign or malignant based on its features.</li> <li>Challenges: The data needs to be cleaned; many rows contain missing data, and there may be erroneous data identifiable as outliers as well.</li> <li> <p>Approach:</p> <ul> <li>Review data quality, and missing data. Drop if not a lot of records are wrong</li> <li>Transform the data to be usable by sklearn using numpy</li> </ul> </li> </ul> <p>See personal notebook in mammogram_mass folder</p>"},{"location":"solutions/#computer-vision-with-pytorch-classify-sushi-pizza-and-steak","title":"Computer vision with PyTorch: classify sushi, pizza and steak","text":"<ul> <li>Data: The food 101 dataset from PyTorch vision</li> <li>Goal: Develop a NN to classify images</li> <li>Challenges: The number of layers</li> <li>Approach: Develop a basic NN and then compare it with existing CNN</li> </ul> Demonstration with pytorch scripts <ul> <li>Create a virtual env and install requirements under the pytorch folder; <code>pip install -r requirements.txt</code></li> <li>Under computer vision, load the data sets locally: <code>python  prepare_image_dataset.py --classes sushi,steak,pizza</code></li> <li>Do a simple classification using a Tiny VGG: <code>python classify_food.py</code></li> <li>Use transfer learning (see explanations here): <code>python transfer_learning.py</code></li> </ul>"},{"location":"solutions/#other-use-case","title":"Other use case","text":"<ul> <li>Data: </li> <li>Goal: </li> <li>Challenges: </li> <li>Approach:</li> </ul>"},{"location":"techno/airflow/","title":"Apache Airflow","text":"<p>Implement batch-oriented orchestration workflow platform, based on Python framework to connect to any technology.  </p>"},{"location":"techno/airflow/#value-propositions","title":"Value propositions","text":"<ul> <li>Deploy locally or to a distributed cluster.</li> <li>Airflow components are extensible to adjust to specific environment</li> <li>Version controlled with rollback to previous versions</li> <li>Not for event-based workflow, not streaming solution. Can be combined with Kafka to batch process data in topics.</li> </ul>"},{"location":"techno/airflow/#concepts","title":"Concepts","text":"<ul> <li>Use Directed A Graph represents the workflow to execute</li> <li> <p>Component Architecture</p> <p></p> </li> </ul>"},{"location":"techno/bedrock_watson/","title":"Amazon Bedrock vs IBM Watsonx.ai: A Comparative Analysis","text":""},{"location":"techno/bedrock_watson/#introduction","title":"Introduction","text":"<ul> <li>Briefly introduce Amazon Bedrock and IBM Watsonx.ai as two prominent AI-powered platforms</li> <li>Mention the importance of understanding their differences in the context of business innovation and digital transformation</li> </ul> <p>AI applications are moving to agent design patterns and adoption of fine tuned model for a specific knowledge corpus.</p> <p>Offer predictions for the future of AI-powered innovation in business and the role of Bedrock and Watsonx.ai in shaping that future Discuss the potential challenges and limitations of each platform. Businesses will need to invest in talent acquisition and development programs to ensure they have the necessary expertise to develop AI based applications.</p> <p>Enterprise need tp involve small AI startups while collaborating with technology providers and established independent software vendors who are adding AI capability to their products. Today such added capabilities are not bringing breakthrough value, yet, so it is important that IT enterprise architects are continuously monitoring progress from their software vendors. AI Startups are getting a lot of investment and good ideas, for bleeding edge technology: enterprise architects should look at those startups' progress and assess their technology fit for their use cases.</p> <p>As AI becomes more ubiquitous, governments and regulatory bodies will need to establish clear guidelines and standards for AI application development, deployment, and use. AI applications must be regulated not the large NLP models. </p>"},{"location":"techno/bedrock_watson/#use-cases-and-industry-applications","title":"Use Cases and Industry Applications","text":"<p>Before going into product comparison we need to review the major use cases for which enterprises may consider those services. The Generative AI models, are part of Natural Language Processing models so well fitted for text analysis, content generation, summarization,</p> <p>By providing managed services, pre-trained models, and easy deployment options, these platforms will enable developers to integrate AI capabilities more seamlessly, driving innovation and digital transformation.</p> <p>Explore the different use cases and industry applications for Bedrock and Watsonx.ai, such as:</p> <ul> <li>Customer service chatbots</li> <li>Predictive maintenance</li> <li>Supply chain optimization</li> <li>Healthcare and life sciences</li> <li>Financial services and banking</li> </ul> <p>The challenges with the cloud-native designs of Bedrock and WatsonX.AI will lead to more scalable, flexible, and resilient AI systems. But pay attention to quota and service limits.</p>"},{"location":"techno/bedrock_watson/#amazon-bedrock-an-overview","title":"Amazon Bedrock: An Overview","text":"<p>Amazon Bedrock is a managed platform for hosting and running different Foundation Models. Launched in late 2023 after the impressive acceptance of the people for Generative AI nd ChatGPT. Currently, it supports Amazon Titan, AI21 Labs, Cohere, Anthropic, Mistral, Meta Llamas  and Stability AI models. </p> <p>Asa managed service, developers using the Bedrock API do not have to think about infrastructure management. The only concerns should be service limits, latency and pricing. </p> <p>Amazon Bedrock's deep integration with the AWS ecosystem may be seen as positive, I will consider vendor locking as a major risk for Enterprises. As a developer the adoption of an abstraction layer to be able to use any API is beneficial.  and its focus on scalability and low-latency deployment. </p> <p>Provide an overview of Amazon Bedrock, including its features, capabilities, and use cases Discuss how Bedrock enables businesses to build, deploy, and manage AI models at scale</p> <p>Pricing for Bedrock is based on the amount of compute and storage resources consumed when running inference on the hosted models</p>"},{"location":"techno/bedrock_watson/#ibm-watsonxai-an-overview","title":"IBM Watsonx.ai: An Overview","text":"<p>Offer an overview of IBM Watsonx.ai, including its features, capabilities, and use cases Discuss how Watsonx.ai enables businesses to build, deploy, and manage AI models, with a focus on explainability and transparency</p>"},{"location":"techno/bedrock_watson/#key-differences-bedrock-vs-watsonxai","title":"Key Differences: Bedrock vs Watsonx.ai","text":"<p>Discuss the differences in:</p> <p>Architecture and deployment models AI capabilities and features (e.g., machine learning, natural language processing, computer vision) Explanability and transparency Integration and scalability Industry focus and use cases Pricing and cost structures</p> <p>The emphasis on explainability and transparency in WatsonX.AI, in particular, will likely drive a shift towards more accountable and responsible AI development. This focus on ethics and fairness will be crucial in building trust in AI systems and ensuring their alignment with business values and societal norms.</p>"},{"location":"techno/bedrock_watson/#conclusion","title":"Conclusion","text":"<p>Summarize the key differences between Amazon Bedrock and IBM Watsonx.ai Offer guidance on how businesses can choose the right platform for their AI and automation needs</p>"},{"location":"techno/feature_store/","title":"Feature Store","text":""},{"location":"techno/feature_store/#tecton-and-feature-store","title":"Tecton and Feature store","text":"<p>Features in Tecton are typically defined as transformation pipelines managed by Tecton.ai. The following diagram illustrates the high level architecture of Tecton feature platform.</p> <p></p> <p>The feature engines are the components to inject feature data to the Store. The second important element is the API to access those feature for offline training in classical Data Scientist's notebook or online inference, think about ML model as a service. </p> <p>Tecton defines the <code>Feature Views</code> to link data sources as inputs, or in some cases other Feature Views, with transformation to compute one or more features. <code>Entity</code> is part of the domain and leads to one or more features. </p> <p>Feature pipelines are done in simple declarative language which looks like SQL, but it is also built in Python so can be integrated in any Python code. The feature engine can orchestrate batch, streaming, and real-time transformations, and re-uses existing processing infrastructure like AWS EME, Databricks, Spark and Snowflake to process the data.</p> <p>The feature repository is an abstraction layer on top of storage like Amazon S3. Feature can be saved in source control and rollout to production with CI/CD tool. A Tecton Repository is a collection of Python files containing Tecton Object Definitions, which define feature pipelines and other dataflows within Tecton's framework. </p> <p>There are two types of Feature Store: offline, to be used by batch processing and notebooks, or online which is a distributed key-value store, used to keep the last value of a feature, and be used for online inference.</p> <p>Finally, a <code>Feature Service</code> represents a set of features that power a model. Feature Services provide convenient endpoints for fetching training data through the Tecton SDK or fetching real-time feature vectors from Tecton's HTTP API.</p> <p>The source for the data can be mobile events and data, streaming events from Kafka, MSK, Kinesis data streams, or data at rest like in S3, Delta lake, DynamoDB, EMR, Athena, Redshift.</p> <p>Feature store can be added to a LLM prompt, so developer can extract powerful insights from customer events as they unfold in real time and pass those as signals to LLMs.</p>"},{"location":"techno/feature_store/#a-typical-feature-workflow","title":"A typical feature workflow","text":"<ol> <li>Create and validate a new feature definition in a notebook</li> <li>Run the feature pipeline interactively to ensure correct feature data</li> <li>Fetch a set of registered features from a workspace and create a new feature set</li> <li>Generate training data to test the new feature in a model</li> <li>Copy the new feature definition into your feature repo</li> <li>Apply your changes to a live production workspace</li> </ol>"},{"location":"techno/feature_store/#feast-open-source","title":"Feast Open Source","text":"<p>Feast (Feature Store) is an operational data system for managing and serving machine learning features to models in production. Feast is able to serve feature data to models from a low-latency online store (for real-time prediction) or from offline store  </p> <p></p> <p>Tecton is the hosted SaaS feature platform with proprietary enhancements. It includes a fork of the Feast feature store.</p>"},{"location":"techno/feature_store/#value-propositions","title":"Value propositions","text":"<ul> <li>Feature reuse via central repository, feature metadata, searchable.</li> <li>The ML systems built is coupled with data sources, so any change in data infrastructure impact those systems. Decoupling via a Feature store brings stability with a single data access layer. </li> <li>Facilitate deployment of ML feature into production, with a centralized registry and a service layer to server the feature.</li> <li> <p>Avoid data leakage by providing point-in-time correct feature retrieval when exporting feature datasets for model training</p> </li> <li> <p>Feast is not a ETL or ELT.</p> </li> </ul>"},{"location":"techno/feature_store/#featureform","title":"FeatureForm","text":"<p>FeatureForm is another open-source Feature Store that transforms existing infrastructure into a feature store, it is an abstraction on top of infrastructure. It can work on top of Spark.</p> <p>A data scientist working in a notebook can push transformation, feature, and training set definitions to a centralized, local repository. Register a PySpark transformations and let FeatureForm orchestrate the data infrastructure from Spark to Redis, and monitor both the infrastructure and the data.</p> <p>It supports Native embeddings and vector databases as both for inference and training stores. FeatureForm on Kubernetes can be used to connect to your existing cloud infrastructure and can also be run locally on Minikube. </p> <p>The figure below presents the FeatureForm components:</p> <p></p>"},{"location":"techno/feature_store/#feature-store-deeper-dive","title":"Feature Store Deeper dive","text":"<ul> <li>Feast quickstart</li> <li>Feature Stores Explained: The Three Common Architectures</li> <li>Simple use of Feast with LLM</li> <li>Learning Feast by examples.</li> </ul>"},{"location":"techno/nicegui/","title":"Nice GUI","text":"<p>Nice GUI is another open-source python library to build UI with better support of states between pages.</p> <p>It uses pages, components, events, and handlers. Components are arranged on a page using layouts. Layouts provide things like grids, tabs, carousels, expansions, menu. It can be customized with Tailwind classes and Quasar components to control the style or behavior of the components.</p> <p>The server is FastAPI. </p>"},{"location":"techno/nicegui/#how-to","title":"How to","text":"<ul> <li>Product documentation getting started</li> <li>Git repo with samples</li> </ul>"},{"location":"techno/opensearch/","title":"OpenSearch","text":"<p>OpenSearch  is a distributed search and analytics engine. </p>"},{"location":"techno/opensearch/#main-concepts","title":"Main concepts","text":"<p>Documents are units that store information, and are stored in json format. An index is a collection of documents.</p> <p>Search are done on one or more nodes. Nodes need a lot of memory and disk. In cluster, there is a node which is elected as cluster manager, and orchestrates cluster-level operations.</p> <p>OpenSearch splits indexes into shards. Each shard is actually a full Lucene index.  Limit shard size to 10\u201350 GB. A shard may be either a primary (original) shard or a replica (copy) shard. OpenSearch distributes replica shards to different nodes than their corresponding primary shards.</p> <p>Doing a search, OpenSearch matches the words in the query to the words in the documents, and each document has a relevance score. Individual words in a search query are called search terms.</p> <p>For relevance score, the term frequency is used, combined with the inverse document frequency which measure the number of document in which the word occurs. OpenSearch uses the BM25 ranking algorithm to calculate document relevance scores.</p>"},{"location":"techno/players_to_look/","title":"AI Players to consider","text":""},{"location":"techno/players_to_look/#anthropic","title":"Anthropic","text":"<p>Anthropic is an AI safety and research company developing Claude 2 LLM, a secure LLM for a wide range of tasks, from sophisticated dialogue and creative content generation to complex reasoning and detailed instruction. Support 100k token context (around 70k words). Claude can handle a variety of basic instructions and logical scenarios, including formatting outputs as desired, following if-then statements, and making a series of logical evaluations in a single prompt.</p>"},{"location":"techno/players_to_look/#deeplearningai","title":"Deeplearning.ai","text":"<p>Andrew Ng's company to educate on deep learning and anything related.</p>"},{"location":"techno/players_to_look/#mistral","title":"Mistral","text":"<p>The mission to bring AI to all developers, pushing for more open platforms and spreading the adoption of AI. They develop different model size, with open model approach (weights are shared).</p>"},{"location":"techno/players_to_look/#outsystems","title":"outsystems","text":"<p>Low-code no-code platform with new AI agents builder.</p>"},{"location":"techno/players_to_look/#fireworksai","title":"fireworks.ai","text":"<p>An inference platform to server multi-modal AI models with fine tuning capability too (using LoRA). Here are the supported models.</p> <p>FireAttention is a custom CUDA kernel, optimized for Multi-Query Attention models (Mixtral is one of them).</p> <p>Serving Open Source Models- Blog, with mixtral model 4x time speed improvement than other OSS. FP8 shrinks model size 2x, it allows for more efficient deployment. Combined with memory bandwidth and FLOPs speed-ups this results in 2x improvement of the effective requests/second. </p>"},{"location":"techno/players_to_look/#workhelix","title":"WorkHelix","text":"<p>Help assessing a company\u2019s Generative AI opportunity. The software constantly scans 450M+ publicly available data points across 3,000+ jobs and 70,000 work activities to answer questions such as how big of a productivity boost can Generative AI bring to the organization?.</p>"},{"location":"techno/players_to_look/#bearingai","title":"bearing.ai","text":"<p>Starting January 2023, all shipping companies must comply with the Carbon Intensity Indicator, and Bearing.ai helps predict CII rating with extreme accuracy and make data-backed decisions to bring every vessel into compliance. It used Deep learning AI and no-code solution. Understand vessel performance, and optimize fuel consumption taking into account vessel, route and weather data.</p>"},{"location":"techno/players_to_look/#amorai","title":"Amorai","text":"<p>Amorai is a health and wellbeing company that harnesses AI to enhance real-world relationships. It offers personalized coaching exercises, primarily crafted and curated for Gen Z. Structured as a vertical AI product, the app builds on LLM (large language model) technology, has a subscription business model.</p>"},{"location":"techno/players_to_look/#workera","title":"Workera","text":"<p>Use AI to assess team's skill in an enterprise, for business transformation. Get skill related data,  to help drive assessment and learning plan.</p>"},{"location":"techno/players_to_look/#speechlab","title":"SpeechLab","text":"<p>Generative AI models for multi-speaker, multi-language text-to-speech, speech recognition, speaker labeling, source audio separation and machine translation.</p>"},{"location":"techno/players_to_look/#esteamai","title":"Esteam.ai","text":"<p>help assess and teach children to read independently.</p>"},{"location":"techno/players_to_look/#common-sense","title":"Common Sense","text":"<p>nonprofit organization dedicated to improving the lives of all kids and families by providing the trustworthy information, education, and independent voice</p>"},{"location":"techno/players_to_look/#netailai","title":"Netail.ai","text":"<p>Netail focuses on Retail &amp; CPG domain to develop AI based solution.</p>"},{"location":"techno/players_to_look/#kiralearning","title":"(kira*)learning","text":"<p>Kira Learning is transforming computer science and AI education for K-12 learners.</p>"},{"location":"techno/players_to_look/#validmind","title":"ValidMind","text":"<p>Model Risk Management with AI</p>"},{"location":"techno/players_to_look/#woebot-health","title":"Woebot Health","text":"<p>Woebot is at the heart of an AI-powered platform that systematically builds interventions for specific intended uses and patient populations.</p>"},{"location":"techno/players_to_look/#echelonai","title":"Echelon.ai","text":"<p>Leveraging advanced AI algorithms and communication protocols, enabling the drones to collaborate in real-time. * identify and track objects in real-time: surveillance, search and rescue, and inspection with greater accuracy and efficiency. * autonomous flight control system</p>"},{"location":"techno/players_to_look/#credoai","title":"Credo.ai","text":"<p>Credo AI is the intelligence layer for AI projects across your organization. Track, assess, report, and manage AI systems you build, buy, or use to ensure they are effective, compliant, and safe.</p>"},{"location":"techno/players_to_look/#landingai","title":"Landing.ai","text":"<p>visual prompting product for computer vision AI projects.</p>"},{"location":"techno/pydantic.ai/","title":"Pydantic AI","text":"<p>A Agent framework by the Pydantic team,  to bring that FastAPI feeling to GenAI app development.</p>"},{"location":"techno/pydantic.ai/#value-propositions","title":"Value propositions","text":"<ul> <li>Model agnostic: Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral</li> <li>Type-safe</li> <li>Structured Responses</li> <li>Streamed Responses</li> <li>Streamed Responses</li> </ul>"},{"location":"techno/pydantic.ai/#sources","title":"Sources","text":"<ul> <li>Example repository</li> </ul>"},{"location":"techno/pydantic.ai/#code-studies-and-demos","title":"Code studies and demos","text":"<ul> <li>GenAI-apps-demos/pydantic-ai folder</li> </ul>"},{"location":"techno/streamlit/","title":"Streamlit","text":"<p>Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. It uses an integrated cloud based IDE with integration to github codespaces and deploy on streamlit SaaS servers.</p> <p>See Getting started</p>"},{"location":"techno/streamlit/#main-concepts","title":"Main concepts","text":"<ul> <li>Use a CLI to start an Streamlit server.</li> </ul> <pre><code>streamlit run your_script.py\n# or \npython -m streamlit run your_script.py\n# or using a script in a git url\nstreamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py\n</code></pre> <ul> <li>Support continuous interactive loop development experience</li> <li>Can present Pandas dataframe directly in the page withing a table widget.</li> <li>Streamlit makes it easy to organize the widgets in a left panel sidebar with <code>st.sidebar</code>. </li> <li>It supports Light and Dark themes out of the box, and custom theme.</li> <li>Support multiple pages application.</li> </ul>"},{"location":"techno/streamlit/#samples","title":"Samples","text":"<ul> <li>Getting Started with matching code to test it.</li> <li>Run the app in Docker, see the Dockerfile in this project / folder (llm-ref-arch-demo/sa-tools/user-interface)</li> <li>Some best practices here.</li> <li>Streamlit Cheat Sheet app in Streamlit.</li> </ul>"},{"location":"techno/streamlit/#some-how-to","title":"Some How To","text":"File Uploaded and processing <p><pre><code>pdf = st.file_uploader('Upload your PDF Document', type='pdf')\nif pdf is not None:\n    reader = PdfReader(pdf)\n</code></pre> See e2e-demos/chat_with_pdf/app.py</p> How to share data between pages? <p>Use st.session_state. For example a page get some settings in a form and a save button. The supporting function needs to use the session_state.</p> <p><pre><code>    if save_button:\n        data={\"callWithVectorStore\":callWithVectorStore, \"callWithDecisionService\": callWithDecisionService, \"llm_provider\": llm_provider }\n        st.write(data)\n        st.session_state[\"app_config\"]=data\n</code></pre> In other page use something as:</p> <pre><code>if 'app_config' not in st.session_state:\nst.session_state['app_config']= {\n    \"callWithVectorStore\":False, \n    \"callWithDecisionService\": False, \n    \"llm_provider\": \"openAI\" \n    }\n\napp_config=st.session_state['app_config']\n</code></pre>"},{"location":"techno/watsonx/","title":"WatsonX.ai","text":"<p>WatsonX.ai offers a set of features to use LLM as APIs or within WatsonX Studio so a Data scientist may train, validate, tune and deploy AI models or LLMs. </p> <p></p>"},{"location":"techno/watsonx/#value-propositions","title":"Value Propositions","text":"<ul> <li>Studio environment to cover both traditional ML model development and tuning or work with LLMs</li> <li>Prompt Lab to build new LLM prompt or use existing ones, shareable between data scientists.</li> <li>Open sources LLMs from Mistral, LLama or IBM's Granite models.</li> <li>Support guardrail for model outcome control</li> <li>Fine tuning model on proprietary data</li> <li>Integrate AutoAI to create ML model in no-code environment</li> <li>Ability to create synthetic tabular data</li> <li>Open Data lake house architecture</li> <li>AI governance toolkit</li> <li>Bring your own model</li> </ul> Granite from IBM Research <p>Granite is IBM's flagship series of LLM foundation models based on decoder-only transformer architecture. Granite language models are trained on trusted enterprise data spanning internet, academic, code, legal and finance. The data sets that have been vigorously filtered to remove:</p> <ul> <li>Hate, Abuse, and Profanity content </li> <li>Copyright and licensed materials </li> <li>Duplications </li> <li>Any other undesirable, blacklisted material, and blocked URLs </li> </ul> <p>See model documentation.</p>"},{"location":"techno/watsonx/#getting-started","title":"Getting started","text":"<p>Once IBM Cloud account is created, we need to also sign-up to WatsonX.ai.</p> <p>Once done a sandbox project is created, we need to get the project ID using the Manage tab&gt; Info menu in the project page.</p> <p></p> <ul> <li>Using IAM, create an IBM API KEY with Manage (in top menu bar) &gt; Access (IAM) &gt; API keys.</li> <li> <p>Get the watsonx.ai endpoint URL to connect with (Fro dallas it should  be: https://us-south.ml.cloud.ibm.com). </p> </li> <li> <p>Start using the  ibm-watsonx-ai library for python. We can also use LangChain and LlamaIndex.</p> </li> <li> <p>With the Python library, we can use an API key or an IAM token (with expiration time). For REST API using curl we need api key and IAM token. In python, once we pass the API key to the APIClient in the library, the client generates the IAM token and refreshes the token as needed.</p> </li> <li> <p>Set as environment variables with URL, API KEY and PROJECT ID in a .env file</p> </li> </ul> <pre><code>IBM_WATSONX_APIKEY=Idx...\nIBM_WATSONX_URL=https://us-south.ml.cloud.ibm.com\nIBM_WATSON_PROJECT_ID=0b73....\n</code></pre> <ul> <li>Client code</li> </ul> <pre><code>from ibm_watsonx_ai.foundation_models import Model\nfrom ibm_watsonx_ai import APIClient, Credentials\nfrom ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n\ncredentials = Credentials(\n                   url = watsonx_url,\n                   api_key =watson_api_key))\n\nclient = APIClient(credentials)\n\nparameters = {\n            \"decoding_method\": \"greedy\",\n            \"max_new_tokens\": 255,\n            \"min_new_tokens\": 1,\n            \"temperature\": 0.5,\n            \"top_k\": 50,\n            \"top_p\": 1,\n        }\n\nmodel = Model(\n    model_id=ModelTypes.FLAN_UL2,\n    params=parameters,\n    credentials=credentials,\n    project_id=project_id\n)\n\ngenerated_response = model.generate(prompt=...)\nprint(generated_response['results'][0]['generated_text'])\n</code></pre> <p>For some Langchain sampe using Watson see watson-machine-learning-samples - foundation_models or the  LangChain Watsonx.ai documentation..</p> <ul> <li>Get the list of current model for inference:</li> </ul> <pre><code>from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n\nprint(\"--&gt; existing models in WatsonX.ai:\")\nprint(json.dumps( ModelTypes._member_names_, indent=2 ) )\n</code></pre> <ul> <li>Python code to connect to WatsonX ai model using LangChain:</li> </ul> <pre><code>from langchain_ibm import WatsonxLLM\nllm = WatsonxLLM(\n        model_id=\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\n        url=\"https://us-south.ml.cloud.ibm.com\",\n        project_id=project_id,\n        params=parameters,\n    )\n</code></pre>"},{"location":"techno/watsonx/#prompt-lab","title":"Prompt Lab","text":"<ul> <li>A shot represent prompt input and output, used to instruct the model on how to best respond to a query</li> <li>Prompts are tokenized before being passed into a model, and foundation model usage costs are calculated based on the number of tokens</li> <li>WatsonX.ai offers 3 sandbox in the Prompt Lab: chat, structured, freeform</li> </ul> <ul> <li>Watsonx.ai provides AI guardrails to prevent potential harmful input and output text</li> <li>Watsonx.ai provides sample prompts grouped into categories like: Summarization, Classification, Generation,  Extraction, Question Answering, Code, Translation.</li> <li>It selects the model that is most likely to provide the best performance for the given use case.</li> <li>All models have the same inference parameters:</li> </ul> <ul> <li>In Greedy mode, the model selects the highest probability tokens at every step of decoding. It is less creative. With Sampling we can tune temperature (float), top k(int) and top P (float). Top P sampling chooses from the smallest possible set of \"next\" words whose cumulative probability exceeds the probability p. The higher the value of Top P, the larger the candidate list of words and so the more random the outcome would be. Top K is for the number of words to choose from to be the output.</li> <li> <p>Repetition penalty (1 or 2) is used to counteract a model\u2019s tendency to repeat the prompt text verbatim.</p> </li> <li> <p>In general, the \"instruct\" models are better at handling requests for structured output and following instructions.</p> </li> <li>Model size does not guaranty better results. IBM's Granite models give excellent results on instruction, like generating, list, json output...</li> </ul> <p>Here is an example of one-shot prompting with an input, output example pair to better guide the model.</p> <p></p> <ul> <li>For better prompt engineering, WatsonX,ai offers save by session to keep a  history of the prompt  session, recording each individual change, which can also being seen in a timeline. Saving a prompt is like taking a snapshot of the prompt text and its settings. It also support to go back to a previous version.</li> <li>A Prompt can be saved as a Jupyter notebook, and then code is generated to run into the notebook.</li> </ul> <p></p> <ul> <li>The codellama-34b-instruct-hf is good with code translation and code generation tasks</li> </ul> <p>Foundation models are not answering questions. Instead, they are calculating the best next tokens based on what data was used to train it.</p>"},{"location":"techno/watsonx/#prompt-tuning","title":"Prompt tuning","text":"<p>This is not the same as prompt engineering, the goal is to have a user providing a set of labeled data to tune the model. Watsonx.ai will tune the model using this data and create a \"soft prompt\", without changing the model's weights.</p> <p>LLMs are generally not good enough where there are specific business languages and operational details, especially where terminologies and business requirements are constantly being updated. </p> <p>Prompt tuning may help to add classes to different query according to human labelled queries. Only certain all LLMs support this kind of tuning. A one-time tuning can outperform at a lower cost than multi-shot prompting. In addition, multi-shot prompting only works for a particular prompt, and it may not work for a different prompt.</p> <p>It is important to pay attention to the content of the training data. New data should not bring bias because of bad value distribution. LLM could not learn business rules with training.</p>"},{"location":"techno/watsonx/#synthetic-data","title":"Synthetic Data","text":"<p>Synthetic data can be used to augment or replace real data for improving AI models, protecting sensitive data, and mitigating bias. Developer starts from existing dataset, so generated data will conform to existing schema.</p> <p>Use Project &gt; Assets menu in WatsonX.</p> <p>WatsonX can generate categorical value to string given a list of string with some occurrence numbers. For numerical, it can use standard distribution with specific mean and deviation. Some column can be anonymized. It uses different methods to generate data: Kolmogorov-Smirnov and Anderson-Darling. And developers can profile the datasets and build correlations between different columns to reflect real-world data.</p> <p>The generated data can be saved in .xls format. </p>"},{"location":"techno/gcp/","title":"Google AI platform","text":"<p>Google offers a set of managed services to develop ML models and use Generative AI models. The model garden exposes a catalog of Google or 3nd party models.</p>"},{"location":"techno/gcp/#most-important-products","title":"Most important products","text":"<ul> <li>Vertex AI</li> <li>Gemini LLM Prompt and test in Vertex AI with Gemini, using text, images, video, or code. Gemini is multimodal model, it accepts text, image, video, audio and document data as input and produces text output.</li> <li>Gemma, a family of lightweight, state-of-the-art open models from 9B or 27 billion parameters. Based on Gemini embeddings. Uses a 256 k tokenizers. It supports  text-to-text, decoder-only large language models, with open weights for both pre-trained variants and instruction-tuned variants. Should be able to run on small device. Available on Hugging Face, kaggle.</li> <li>Search Generative Experience (Search lab)</li> <li>Google Cloud run Serverless platform to deploy any web app</li> <li>Colab A new notebook experience with enterprise-grade privacy and security.</li> <li>TPU designed ships specifically for matrix operations common in machine learning. Can be used in worker node of GKE.</li> </ul>"},{"location":"techno/gcp/#cloud-engine","title":"Cloud Engine","text":"<p>Run virtual machines on Google infrastructure. </p> <p>To create Linux or Windows based VM. e2-micro is free for &lt; 30Gb storage and 1GB of outbound data transfers. Spot instances to pay for less. See pricing calculator.</p> <p>Install Apache HTTP server:</p> <pre><code>sudo apt update &amp;&amp; sudo apt -y install apache2\n</code></pre>"},{"location":"techno/gcp/#colab","title":"Colab","text":"<p>Creating a Jupyter notebook in Google Drive will start colab. It can execute any python code and gets a VM as kernel</p> <p>Simple introduction video. </p>"},{"location":"techno/gcp/#cloud-workstation","title":"Cloud Workstation","text":"<p>Cloud Workstation is a fully managed dev env. It supports any code editors and applications that can be run in a container. And it supports Gemini Code Assist. Pricing is based of per-hour usage, management fees, control plane and network fees.</p>"},{"location":"techno/gcp/#cloud-shell","title":"Cloud Shell","text":"<p>Manage infrastructure and develop our applications from any browser with Cloud Shell. Free for all users, but has a weekly quotas of 50h.</p>"},{"location":"techno/gcp/#vertex-ai","title":"Vertex AI","text":"<p>Managed services for custom model training, but also app on top of Gen AI. It includes a SDK in Python, nodejs, Go, Java, C# or REST.API.</p> <p>Pricing is based on the Vertex AI tools and services, storage, compute, and Google Cloud resources used.</p> <p>The generative AI workflow:</p> <p></p> <p>Interesting features:</p> <ul> <li>offers multiple request augmentation methods that give the model access to external APIs and real-time information: Grounding, RAG amd function calling.</li> <li>checks both the prompt and response for how much the prompt or response belongs to a safety category</li> </ul> Grounding <p>Grounding is the ability to connect model output to verifiable sources of information. It reduces model hallucinations, links model responses to specific information, and it enhances the trustworthiness. LLM responses are based on Google Search to get public knowledge as facts. Grounding can be done with enterprise data using Vertex AI Search.</p>"},{"location":"techno/gcp/#document-ai","title":"Document AI","text":"<p>Google Document AI is used to process and understand documents:</p> <ul> <li>Extracts and understands structured information from a wide variety of document types, including PDFs, images, and scanned documents.</li> <li>It leverages advanced natural language processing (NLP) and computer vision technologies to deliver high-accuracy document extraction and understanding. </li> <li>Trained to extract data from invoices, receipts, and contracts.</li> <li>It allows to create custom document models to extract information from specialized or domain-specific documents.</li> <li>Different natural languages are supported.</li> <li>Scale up and down ensuring high throughput and low latency.</li> <li>Can run on-line or with batch processing.</li> <li>No-code tools to let developers quickly set up and configure document processing pipelines without writing any code.</li> <li>Integrated with monitoring and logging services.</li> <li>It is possible to combine with custom MM models to do better entity extraction.</li> </ul> <p>Document processing is a complex domain in AI as it groups optical character recognition, NLP, Entity Extraction, machine translation and data lost prevention. Documents can be categorized into three different groups:</p> <ol> <li>General docs, a basic content that should be parsed with general ML models, which includes OCR, structured form parser, and document quality analysis. </li> <li>Specialized docs, the content needs some specialized models pre-trained on w2s, driving licenses, invoices, expense reports, contracts which are high variance document types...</li> <li>Custom docs, where the content needs to be parsed with custom models developed by the enterprise on their own domain. </li> </ol> <p>Document AI uses document processors, that represents the interface to a machine learning model, it is responsible to classify, parse, analyze a Document. Developers need to create processor instances in their project to use Document AI. A processor can be generalized, specialized or custom. Each is addressing a specific task, like optical character recognition, general form parsing, classification, or parsing specialized document types like invoices, expenses..., </p> <p>The document is defined as a Document Object to include raw text, layout, extracted entities, languages...</p> <p>Developer needs to create service account for the Document AI application, then grant the DocumentAI API User role.</p> <p>Document AI workbench helps developers to develop custom document processors, from new one or an existing ones. Uptraining is the way to add custom field for the entity extraction by extending the schema. While new model creation follows the standard ML process of labelling, training, evaluation and deployment.</p> <p>It uses F1 score, accuracy and recall metrics to assess the quality of the entity extractions.</p>"},{"location":"techno/gcp/#hands-on","title":"Hands-on","text":"<ul> <li>Install gcloud</li> <li>Use Gemini inside VSCode</li> </ul>"},{"location":"techno/gcp/#deeper-dive","title":"Deeper dive","text":"<ul> <li>GCP architecture and ML specific</li> <li>LLM comparator</li> </ul>"},{"location":"techno/gradio/","title":"Develop UI with Gradio","text":"<p>Gradio is the fastest way to demo stuff with a user interface. One of its advantages against StreamLit is the capacity to share data between pages and elements.</p> <p>Quickstart</p> <pre><code># For continuous development\ngradio main.py\n# For running a server\npython main.app\n</code></pre> <p>Open in a browser on http://localhost:7860</p> <p>My first app</p>"},{"location":"techno/gradio/#main-concepts","title":"Main concepts","text":"<ul> <li>A python function can be wrapped with a user interface</li> <li>Use <code>gr.Blocks</code> to develop custom interface. Components are automatically added to the Blocks as they are created within the <code>with</code> clause. It is a classic python function. </li> <li>Any Component that acts as an input to an event listener is made interactive</li> <li> <p>Global state can be set via variable at the top of the program, and are shared between connected session</p> </li> <li> <p>Chatbot and ChatInterface to build simple user interface for a chat bot demo.</p> </li> </ul>"},{"location":"techno/gradio/#how-to","title":"How to","text":"<ul> <li>Add accordion to an interface</li> <li>To make an output R/W:  <code>output = gr.Textbox(label=\"Output\", interactive=True)</code></li> <li>Updating Component Configurations</li> </ul>"},{"location":"techno/taipy/","title":"Taipy","text":"<p>TapPy a Python open-source library designed for easy development of data-driven web applications. It supports defining scenario for data pipeline and integrate with UI elements to do the data presentations and interactions.</p> <p>It generates web pages from a Flask Server. The main class is <code>Gui</code>.</p> <ul> <li>Support multiple pages which are defined in markdown, html or python code. (See md_ui.py)</li> <li>Offer various visual elements that can interact with the Python variables and environment.</li> <li>Keep State of user connection and variables for dynamic binding.</li> <li>User interactions are event driven</li> <li>Page has name for navigation</li> <li>Include a CLI to create apps or run them.</li> <li>Blocks let developers organize controls (or blocks) in pages</li> <li>Scenarios are global variables available to everyone connected.</li> <li>Every callback, including submit_scenario(), receives a State object as its first parameter. </li> </ul>"},{"location":"techno/taipy/#some-how-to","title":"Some how to","text":"<ul> <li>Pages are created in different modules, the variables that they can bind to visual elements may have a scope limited to their origin module.</li> <li>For Single Page Application we need to associate one page to \"/\"</li> </ul>"},{"location":"techno/taipy/#cli","title":"CLI","text":"<p>'''sh taipy run main.py '''</p>"},{"location":"techno/taipy/#code","title":"Code","text":"<ul> <li>1<sup>st</sup> UI</li> <li>Markdown, html, navbar based pages</li> <li>A chatbot to integrate LangGraph for prompt builder</li> </ul>"}]}