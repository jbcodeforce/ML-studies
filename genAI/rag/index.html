<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://jeromeboyer.net/ML-studies/genAI/rag/ rel=canonical><link href=../../architecture/sol-design/ rel=prev><link href=../agentic/ rel=next><link rel=icon href=../../assets/logo.drawio.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.18"><title>RAG - Machine Learning Studies - Jerome Boyer</title><link rel=stylesheet href=../../assets/stylesheets/main.7e37652d.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#retrieval-augmented-generation-rag class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Machine Learning Studies - Jerome Boyer" class="md-header__button md-logo" aria-label="Machine Learning Studies - Jerome Boyer" data-md-component=logo> <img src=../../assets/logo.drawio.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Machine Learning Studies - Jerome Boyer </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> RAG </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/jbcodeforce/ML-studies title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill=currentColor d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> About </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ml/ class=md-tabs__link> Machine Learning </a> </li> <li class=md-tabs__item> <a href=../../techno/airflow/ class=md-tabs__link> Techno </a> </li> <li class=md-tabs__item> <a href=../../coding/ class=md-tabs__link> Coding </a> </li> <li class=md-tabs__item> <a href=../../solutions/ class=md-tabs__link> Solutions </a> </li> <li class=md-tabs__item> <a href=https://jbcodeforce.github.io class=md-tabs__link> Home </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Machine Learning Studies - Jerome Boyer" class="md-nav__button md-logo" aria-label="Machine Learning Studies - Jerome Boyer" data-md-component=logo> <img src=../../assets/logo.drawio.png alt=logo> </a> Machine Learning Studies - Jerome Boyer </label> <div class=md-nav__source> <a href=https://github.com/jbcodeforce/ML-studies title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill=currentColor d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <label class=md-nav__link for=__nav_1 id=__nav_1_label tabindex=0> <span class=md-ellipsis> About </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> About </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_2> <label class=md-nav__link for=__nav_1_2 id=__nav_1_2_label tabindex=0> <span class=md-ellipsis> Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_1_2> <span class="md-nav__icon md-icon"></span> Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../concepts/ class=md-nav__link> <span class=md-ellipsis> Core concepts </span> </a> </li> <li class=md-nav__item> <a href=../../concepts/maths/ class=md-nav__link> <span class=md-ellipsis> Math summary </span> </a> </li> <li class=md-nav__item> <a href=../../concepts/skill/ class=md-nav__link> <span class=md-ellipsis> Data scientist skill </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_3> <label class=md-nav__link for=__nav_1_3 id=__nav_1_3_label tabindex=0> <span class=md-ellipsis> Architecture </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_1_3> <span class="md-nav__icon md-icon"></span> Architecture </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../architecture/sol-design/ class=md-nav__link> <span class=md-ellipsis> Solution Design </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Data management </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Data management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../data/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../data/features/ class=md-nav__link> <span class=md-ellipsis> Feature Engineering </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Machine Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Machine Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../ml/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../ml/classifier/ class=md-nav__link> <span class=md-ellipsis> Classifier </span> </a> </li> <li class=md-nav__item> <a href=../../ml/unsupervised/ class=md-nav__link> <span class=md-ellipsis> Unsupervised Learning </span> </a> </li> <li class=md-nav__item> <a href=../../anomaly/ class=md-nav__link> <span class=md-ellipsis> Anomaly detection </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_5> <label class=md-nav__link for=__nav_2_5 id=__nav_2_5_label tabindex=0> <span class=md-ellipsis> Deep Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_5_label aria-expanded=false> <label class=md-nav__title for=__nav_2_5> <span class="md-nav__icon md-icon"></span> Deep Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../ml/deep-learning/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../ml/nlp/ class=md-nav__link> <span class=md-ellipsis> NLP </span> </a> </li> <li class=md-nav__item> <a href=../../coding/ddp/ class=md-nav__link> <span class=md-ellipsis> Distributed Data Parallel </span> </a> </li> <li class=md-nav__item> <a href=../../neuro-symbolic/ class=md-nav__link> <span class=md-ellipsis> Hybrid AI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_6 checked> <label class=md-nav__link for=__nav_2_6 id=__nav_2_6_label tabindex=0> <span class=md-ellipsis> Generative AI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_6_label aria-expanded=true> <label class=md-nav__title for=__nav_2_6> <span class="md-nav__icon md-icon"></span> Generative AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../review/ class=md-nav__link> <span class=md-ellipsis> Skill set </span> </a> </li> <li class=md-nav__item> <a href=../prompt-eng/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> </a> </li> <li class=md-nav__item> <a href=../../architecture/sol-design/ class=md-nav__link> <span class=md-ellipsis> Solution Design </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> RAG </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> RAG </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#context class=md-nav__link> <span class=md-ellipsis> Context </span> </a> </li> <li class=md-nav__item> <a href=#basic-rag-architecture class=md-nav__link> <span class=md-ellipsis> Basic RAG architecture </span> </a> </li> <li class=md-nav__item> <a href=#challenges class=md-nav__link> <span class=md-ellipsis> Challenges </span> </a> <nav class=md-nav aria-label=Challenges> <ul class=md-nav__list> <li class=md-nav__item> <a href=#rag-assessment-scoping-questions class=md-nav__link> <span class=md-ellipsis> RAG assessment scoping questions </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#zoom-to-the-rag-process class=md-nav__link> <span class=md-ellipsis> Zoom to the RAG process </span> </a> <nav class=md-nav aria-label="Zoom to the RAG process"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#document-pipeline class=md-nav__link> <span class=md-ellipsis> Document Pipeline </span> </a> </li> <li class=md-nav__item> <a href=#vector-store class=md-nav__link> <span class=md-ellipsis> Vector Store </span> </a> </li> <li class=md-nav__item> <a href=#frozen-rag class=md-nav__link> <span class=md-ellipsis> Frozen RAG </span> </a> </li> <li class=md-nav__item> <a href=#retrievers class=md-nav__link> <span class=md-ellipsis> Retrievers </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#more-advanced-rag class=md-nav__link> <span class=md-ellipsis> More advanced RAG </span> </a> <nav class=md-nav aria-label="More advanced RAG"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#query-transformations class=md-nav__link> <span class=md-ellipsis> Query transformations </span> </a> </li> <li class=md-nav__item> <a href=#query-routing class=md-nav__link> <span class=md-ellipsis> Query Routing </span> </a> </li> <li class=md-nav__item> <a href=#knowledge-graph-integration-in-rag class=md-nav__link> <span class=md-ellipsis> Knowledge graph integration in RAG </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#sources-of-information class=md-nav__link> <span class=md-ellipsis> Sources of information </span> </a> </li> <li class=md-nav__item> <a href=#some-code-studies class=md-nav__link> <span class=md-ellipsis> Some Code Studies </span> </a> <nav class=md-nav aria-label="Some Code Studies"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#langchain class=md-nav__link> <span class=md-ellipsis> LangChain </span> </a> </li> <li class=md-nav__item> <a href=#langgraph class=md-nav__link> <span class=md-ellipsis> LangGraph </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../agentic/ class=md-nav__link> <span class=md-ellipsis> Agents </span> </a> </li> <li class=md-nav__item> <a href=../anthropic/ class=md-nav__link> <span class=md-ellipsis> Anthropic </span> </a> </li> <li class=md-nav__item> <a href=../openai/ class=md-nav__link> <span class=md-ellipsis> OpenAI </span> </a> </li> <li class=md-nav__item> <a href=../mistral/ class=md-nav__link> <span class=md-ellipsis> Mistral </span> </a> </li> <li class=md-nav__item> <a href=../cohere/ class=md-nav__link> <span class=md-ellipsis> Cohere </span> </a> </li> <li class=md-nav__item> <a href=../../techno/watsonx/ class=md-nav__link> <span class=md-ellipsis> WatsonX.ai </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Techno </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Techno </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../techno/airflow/ class=md-nav__link> <span class=md-ellipsis> Airflow </span> </a> </li> <li class=md-nav__item> <a href=../../techno/feature_store/ class=md-nav__link> <span class=md-ellipsis> Feature Store </span> </a> </li> <li class=md-nav__item> <a href=../../kaggle/ class=md-nav__link> <span class=md-ellipsis> Kaggle </span> </a> </li> <li class=md-nav__item> <a href=../../techno/opensearch/ class=md-nav__link> <span class=md-ellipsis> OpenSearch </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/python-code/ class=md-nav__link> <span class=md-ellipsis> Python studies </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/yarfba/ai-ml/sagemaker class=md-nav__link> <span class=md-ellipsis> SageMaker </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/spark-studies/ class=md-nav__link> <span class=md-ellipsis> Spark studies </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> UI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> UI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../techno/gradio/ class=md-nav__link> <span class=md-ellipsis> Gradio </span> </a> </li> <li class=md-nav__item> <a href=../../techno/streamlit/ class=md-nav__link> <span class=md-ellipsis> Streamlit </span> </a> </li> <li class=md-nav__item> <a href=../../techno/taipy/ class=md-nav__link> <span class=md-ellipsis> TaiPy </span> </a> </li> <li class=md-nav__item> <a href=../../techno/nicegui/ class=md-nav__link> <span class=md-ellipsis> NiceGUI </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../techno/gcp/ class=md-nav__link> <span class=md-ellipsis> GCP </span> </a> </li> <li class=md-nav__item> <a href=../../techno/watsonx/ class=md-nav__link> <span class=md-ellipsis> WatsonX.ai </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Coding </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Coding </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../coding/ class=md-nav__link> <span class=md-ellipsis> Coding getting started </span> </a> </li> <li class=md-nav__item> <a href=../../coding/visualization/ class=md-nav__link> <span class=md-ellipsis> Data Visualization </span> </a> </li> <li class=md-nav__item> <a href=../../coding/haystack/ class=md-nav__link> <span class=md-ellipsis> Haystack.ai </span> </a> </li> <li class=md-nav__item> <a href=../../coding/langchain/ class=md-nav__link> <span class=md-ellipsis> LangChain </span> </a> </li> <li class=md-nav__item> <a href=../../coding/langgraph/ class=md-nav__link> <span class=md-ellipsis> LangGraph </span> </a> </li> <li class=md-nav__item> <a href=../../coding/llama-index/ class=md-nav__link> <span class=md-ellipsis> LlamaIndex </span> </a> </li> <li class=md-nav__item> <a href=../../coding/pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../../coding/pytorch/ class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../../coding/sklearn/ class=md-nav__link> <span class=md-ellipsis> Scikit-learn </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Solutions </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Solutions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../solutions/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../techno/players_to_look/ class=md-nav__link> <span class=md-ellipsis> Key AI startups </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#context class=md-nav__link> <span class=md-ellipsis> Context </span> </a> </li> <li class=md-nav__item> <a href=#basic-rag-architecture class=md-nav__link> <span class=md-ellipsis> Basic RAG architecture </span> </a> </li> <li class=md-nav__item> <a href=#challenges class=md-nav__link> <span class=md-ellipsis> Challenges </span> </a> <nav class=md-nav aria-label=Challenges> <ul class=md-nav__list> <li class=md-nav__item> <a href=#rag-assessment-scoping-questions class=md-nav__link> <span class=md-ellipsis> RAG assessment scoping questions </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#zoom-to-the-rag-process class=md-nav__link> <span class=md-ellipsis> Zoom to the RAG process </span> </a> <nav class=md-nav aria-label="Zoom to the RAG process"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#document-pipeline class=md-nav__link> <span class=md-ellipsis> Document Pipeline </span> </a> </li> <li class=md-nav__item> <a href=#vector-store class=md-nav__link> <span class=md-ellipsis> Vector Store </span> </a> </li> <li class=md-nav__item> <a href=#frozen-rag class=md-nav__link> <span class=md-ellipsis> Frozen RAG </span> </a> </li> <li class=md-nav__item> <a href=#retrievers class=md-nav__link> <span class=md-ellipsis> Retrievers </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#more-advanced-rag class=md-nav__link> <span class=md-ellipsis> More advanced RAG </span> </a> <nav class=md-nav aria-label="More advanced RAG"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#query-transformations class=md-nav__link> <span class=md-ellipsis> Query transformations </span> </a> </li> <li class=md-nav__item> <a href=#query-routing class=md-nav__link> <span class=md-ellipsis> Query Routing </span> </a> </li> <li class=md-nav__item> <a href=#knowledge-graph-integration-in-rag class=md-nav__link> <span class=md-ellipsis> Knowledge graph integration in RAG </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#sources-of-information class=md-nav__link> <span class=md-ellipsis> Sources of information </span> </a> </li> <li class=md-nav__item> <a href=#some-code-studies class=md-nav__link> <span class=md-ellipsis> Some Code Studies </span> </a> <nav class=md-nav aria-label="Some Code Studies"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#langchain class=md-nav__link> <span class=md-ellipsis> LangChain </span> </a> </li> <li class=md-nav__item> <a href=#langgraph class=md-nav__link> <span class=md-ellipsis> LangGraph </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=retrieval-augmented-generation-rag>Retrieval Augmented Generation (RAG)<a class=headerlink href=#retrieval-augmented-generation-rag title="Permanent link">&para;</a></h1> <details class="- info inline"> <summary>Updates</summary> <p>12/2024</p> </details> <h4 id=what-is-covered-in-this-section>What is covered in this section<a class=headerlink href=#what-is-covered-in-this-section title="Permanent link">&para;</a></h4> <p><pre class=mermaid><code>mindmap
  root((RAG))
    Context
      Goals
      Reduce Hallucinations
      Integrate corporate knowledge
    Reference Architecture
        Product capability
    Review Document Pipeline
      Ingestion
      Embeddings
      Practices
    Using Tools
      Vector Store
      Embeddings
      Retriever
      LangChain/LangGraph
</code></pre> <strong>Figure 1: RAG subject areas</strong></p> <h2 id=context>Context<a class=headerlink href=#context title="Permanent link">&para;</a></h2> <p>LLMs have a knowledge cut-off time, where data coming after this time are not known by the models. Pre-training is a one-off exercise. When enterprises need to get their private knowledge integrated to LLM, they can do fine tuning or present semantic search results as part of the input context window. RAG addresses this problem, as it is the act of supplementing generative text models with data outside of what it was trained on. </p> <p>While model increases in token pre-training size, they also increase the size of the context window, as illustrated in the figure below:</p> <p><img alt src=../diagrams/llm_size_window.drawio.png></p> <p><strong>Figure 2: Bigger Context Window</strong></p> <p>When the context window is big enough, application can send more contextual data, that leads to better results. This technique can be used to provide more up-to-date or more use-case-specific information via the context window to enhance its accuracy for specific use cases. It can also help reduce hallucinations, for example, by specifying that the model should only respond with information contained in the search results.</p> <h2 id=basic-rag-architecture>Basic RAG architecture<a class=headerlink href=#basic-rag-architecture title="Permanent link">&para;</a></h2> <p>The Retrieval Augmented Generation may be seen as a three stages process:</p> <p><img alt src=../diagrams/rag_3_stages.drawio.png></p> <ol> <li> <p><strong>Indexing</strong> is a batch processing to ingest documents and data from different sources and indexing them. During processing, semantic search is used to retrieve relevant documents from the indexes. The <code>Indexing</code> step supports loading the documents, splitting large documents into smaller chunks. Chunks help to stay within the LLM's context window. Indexing includes storage of the chunks and the index of the splits. See the <a href=https://github.com/jbcodeforce/ML-studies/blob/7a9d7b86fac629e01ad65bc390b2c7e83d019da5/llm-langchain/openAI/build_agent_domain_rag.py#L17-L43>simple indexing code: build_agent_domain_rag.py</a> using LangChain <code>RecursiveCharacterTextSplitter</code>, OpenAI embeddings and Chroma DB for vector store and retriever.</p> </li> <li> <p><strong>Retrieval</strong>: retrieves the relevant data (splits) from the indexes using similarity search, then passes the resulting chunks to the LLM as part of the context window. The similarity search uses the embeddings to vectorize the query, perform the search and get the resulting indexes.</p> </li> <li><strong>Generation</strong>: LLM generates the response in plain natural language.</li> </ol> <p>This process is supported by different tools for documents ingestion, splitting, embedding, indexing, retrieval and integration with the real time conversation flow. From the simple query text, the process needs to do query construction, translation, and LLM calling. The following diagram illustrates a classical a natural conversation application with RAG architecture:</p> <p><img alt src=../diagrams/rag.drawio.png></p> <ol> <li>The user asks queries via a Q&amp;A or Chat user interface. The query may be decomposed in sub-queries and embedded. </li> <li> <p>An application orchestrator uses the vector store retriever to do a <strong>similarity search</strong> into the vector database, and build the conversation context with retrieved documents. </p> <div class=highlight><pre><span></span><code><span class=n>retriever</span> <span class=o>=</span> <span class=n>vectorstore</span><span class=o>.</span><span class=n>as_retriever</span><span class=p>(</span><span class=n>search_type</span><span class=o>=</span><span class=s2>&quot;similarity&quot;</span><span class=p>,</span> <span class=n>search_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&quot;k&quot;</span><span class=p>:</span> <span class=mi>6</span><span class=p>})</span>
<span class=c1># Example of similarity search query</span>
<span class=n>retrieved_docs</span> <span class=o>=</span> <span class=n>retriever</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span><span class=s2>&quot;What are the approaches to Task Decomposition?&quot;</span><span class=p>)</span>
</code></pre></div> </li> <li> <p>Context, query, system prompt are sent to the model to get the generated text. For this step, there are two approaches: <strong>1/ sequential</strong> where text generation follows retrievals, or <strong>2/ parallel</strong> where retrievals and text generations are done in parallel and then intertwined. </p> <div class=highlight><pre><span></span><code><span class=c1># with a langchain that use the context variable as defined within the Prompt to pass the retrieved documents:</span>
<span class=n>rag_chain</span> <span class=o>=</span> <span class=p>(</span>
  <span class=p>{</span><span class=s2>&quot;context&quot;</span><span class=p>:</span> <span class=n>retriever</span><span class=p>,</span> <span class=s2>&quot;question&quot;</span><span class=p>:</span> <span class=n>RunnablePassthrough</span><span class=p>()}</span>
  <span class=o>|</span> <span class=n>prompt</span>
  <span class=o>|</span> <span class=n>llm</span>
  <span class=o>|</span> <span class=n>StrOutputParser</span><span class=p>()</span>
<span class=p>)</span>
</code></pre></div> </li> <li> <p>Response is sent back to the user.</p> </li> </ol> <p><a href=https://python.langchain.com/v0.1/docs/expression_language/primitives/passthrough/ >RunnablePassthrough pass inputs unchanged</a> to the following <a href=../../coding/langchain/#runnable>Runnable</a>.</p> <p>RAG systems work well because LLMs has the in-context learning capability, which allows models to use previously unseen data to perform accurate predictions without weight training.</p> <details class=practice> <summary>Domain specific vector store</summary> <p>The presence of mixed-domain data in the same vector store collection may introduce noise and potentially degrade performance. Isolating vector store <strong>collections</strong> for each domain can help maintain domain-specific information and improve model accuracy within individual domains</p> </details> <details class="- info"> <summary>In-context learning</summary> <p>The NLP in-context learning involves continually updating the model as new data becomes available. The techniques include online learning, transfer learning, fine-tuning, or using memory-based architectures.</p> </details> <h4 id=embeddings>Embeddings<a class=headerlink href=#embeddings title="Permanent link">&para;</a></h4> <p>Embeddings are the numerical representation of the document chunks, and should help to represent semantic concepts. They are key part of the NLP processing and used in Gen AI models. <a href=../../ml/nlp/#embedding>See dedicated Embedding chapter</a></p> <h2 id=challenges>Challenges<a class=headerlink href=#challenges title="Permanent link">&para;</a></h2> <p>Naive RAG has very important limitations which has generated some adoption challenges:</p> <ul> <li>It is hard to do a reliable, scalable RAG on a large knowledge corpus</li> <li>Limited to single-shot prompt</li> <li>No query understanding, just a semantic search</li> <li>No query decomposition for planning</li> <li>No tool use, to query an external data source to enrich the context</li> <li>No reflection and error correction to improve the quality of the response.</li> <li>No persistence or memory of previous queries</li> </ul> <p>See also the <a href=#retriever-considerations>retriever considerations section</a> for more challenges.</p> <p>Those challenges can be addressed by adding agent before RAG. </p> <h3 id=rag-assessment-scoping-questions>RAG assessment scoping questions<a class=headerlink href=#rag-assessment-scoping-questions title="Permanent link">&para;</a></h3> <p>Before doing an efficient RAG implementation, we need to address a set of important questions:</p> <ul> <li>Who is the end user? External users, clients of the enterprise, may have an incredible power to impact the enterprise's brand. </li> <li>What is the source of the documentation? What are the current data pipelines in place for each data sources, what are the touch points and security boundaries in place?</li> <li>Is there any structured data sources we need to consider for the search?</li> <li>How often the documentation change over time? Is there any governance in place to manage documents quality?</li> <li>How to extract relevant information from the document? Is there any PII in any document that may breach?</li> <li>How to avoid ingestion of malicious content into the knowledge corpus? Who can access what?</li> <li>How does the content look like in the document to determine chunk size and overlap?</li> <li>When to retrieve? How and what to retrieve?</li> <li>What are the expected set of most asked questions on the current corpus?</li> <li>How to post process the answer?</li> <li>How to learn new information?</li> <li>What to optimize in this entire system?</li> <li>How to handle queries outside of the domain? This may lead to adopt a domain specific LLM and combined with a generalized LLM.</li> <li>How the deployment will occur? Self-hosted, using API-based LLM? What is the expected latency supported? What cost is expected?</li> <li>Is there any compliances and regulations to follow?</li> </ul> <p>The above questions should help to address:</p> <ul> <li>Can RAG give good answers?</li> <li>Do we need to fine tuning an existing model?</li> </ul> <p>Training time includes addressing how to update LLM, how to update the document encoder, and the query encoder. Some major questions to address are: Do we need to pre-train from zero or leverage an existing model? What is the current data source quality? how far the indexing data will be from the future query data. </p> <h2 id=zoom-to-the-rag-process>Zoom to the RAG process<a class=headerlink href=#zoom-to-the-rag-process title="Permanent link">&para;</a></h2> <h3 id=document-pipeline>Document Pipeline<a class=headerlink href=#document-pipeline title="Permanent link">&para;</a></h3> <p>The RAG preparation is a very important part of the process to ensure good retrieval results.</p> <p><img alt src=../diagrams/rag_preparation.drawio.png width=600></p> <ul> <li> <p><strong>Text extraction</strong> is employed to isolate relevant textual information and remove any noise. Document content is used for keyword or similarity search in any RAG apps. In a document, some elements are important for RAG preparation: the title, narrative text, list item, table, image, but also element's metadata like filename, type, page number and section. It is the most expensive task in the RAG process. <a href=https://jbcodeforce.github.io/spark-studies/ >Tools like Apache Spark may be used for the data preparation</a>.</p> </li> <li> <p><strong>Chunk creation</strong> is to segment text into smaller chunks or sections. This step is essential for efficient retrieval and processing. Chunk size is important to keep the context and some sort of semantic, while overlapping between chunks will help to create vectors that are still close while the text section is bigger than the chunk size. Tuning chunk parameters is balancing between preserving context and keeping accuracy. Sophisticated sentence segmentation techniques can be used. Some references are <a href=https://www.sbert.net/ >SBERT</a> and HuggingFace <a href=https://www.philschmid.de/optimize-sentence-transformers>SentenceTransformer</a>.</p> </li> </ul> <p>Document processing is hard because of the different document types (HTML, PDF, Doc, Markdown, Tex...) and structure. It is better to use template and document structure to facilitate content extraction. Extracting metadata requires understanding of the document structure. For pdf, advanced solutions mix OCR and neural network.</p> <p>For short sentence smaller than 256 chars, the all sentence can be used as chunk. For bigger text section, the embedding used may have some better results with chunks from 256 to 512 tokens. The bigger the size of the chunk, the higher the risk of hallucination. While too short, will miss the meaning. The word embeddings introduced with <code>Word2Vec</code> or <code>Bert</code> have still given good results but with a lot of computation.</p> <p>One chunk strategy is to construct chunk from document elements: the document is broken down into atomic elements, which are combined into chunk until reaching a token threshold. Align the chunking break condition to the structure of the document: per section, new title...</p> <ul> <li>The <a href=https://unstructured-io.github.io/unstructured/api.html>Unstructured.io API Services</a> complemented with a python library can be used for ingesting and digesting unstructured documents of various type (See also deeplearning.ai tutorial).</li> </ul> <p>Code with text splitting:</p> <table> <thead> <tr> <th>Code</th> <th>Cover</th> </tr> </thead> <tbody> <tr> <td><a href=https://github.com/jbcodeforce/ML-studies/blob/7a9d7b86fac629e01ad65bc390b2c7e83d019da5/llm-langchain/openAI/build_agent_domain_rag.py#L17-L43>build_agent_domain_rag.py</a></td> <td>LangChain <code>RecursiveCharacterTextSplitter</code>, OpenAI embeddings and Chroma DB for vector store and retriever.</td> </tr> <tr> <td><a href=https://github.com/jbcodeforce/athena-owl-core/blob/main/owl-agent-backend/src/athena/itg/store/content_mgr.py>Content manager in owl agent framework</a></td> <td>integrate pdf,docs, html, text, markdown parsers</td> </tr> </tbody> </table> <p>Langchain <a href=https://python.langchain.com/v0.2/docs/integrations/document_loaders/ >examples for text processing</a></p> <p>ChromaDB document processing <a href>example</a></p> <details class="- info"> <summary>Extract from pdf</summary> <p>There are different techniques to get content from unstructured file like a pdf. The first is to use object detection to draw and label bounding boxes around the layout elements on a document image (Document Layout Detection). OCR is used to extract text from bounding box. Some pdf as text the extraction can be done without OCR. The second technique use vision transformers.</p> </details> <h3 id=vector-store>Vector Store<a class=headerlink href=#vector-store title="Permanent link">&para;</a></h3> <p>The vector store is a database used to store the document chunks and the corresponding embeddings. It is used by the retriever to search for the document chunks related to the query. The first vector databases were based on <a href=https://github.com/facebookresearch/faiss>FAISS</a>, a library for efficient similarity search and clustering of dense vectors.</p> <p>The vector store can be implemented using different databases such as:</p> <ul> <li><a href=https://www.elastic.co/ >Elasticsearch</a>.</li> <li><a href=https://github.com/facebookresearch/faiss>Faiss</a>.</li> <li><a href=https://github.com/spotify/annoy>Annoy</a>.</li> <li><a href=https://github.com/nmslib/hnswlib>HNSW</a>.</li> <li><a href=https://docs.trychroma.com/embeddings>ChromaDB</a></li> </ul> <p>The vector store is usually implemented using a distributed database to improve the performance of the retrieval process. </p> <h3 id=frozen-rag>Frozen RAG<a class=headerlink href=#frozen-rag title="Permanent link">&para;</a></h3> <p>This is the Retrieval augmented generation with no training. Data are in context only. The prompt drives the LLM to maximize the in-context learning performance. The selection of the right data chunk and the correct embedding model are crucial.</p> <p><img alt src=../diagrams/frozen-rag.drawio.png></p> <p>The result of the search is pass to the LLM as context. This is limited to in-context learning. </p> <h3 id=retrievers>Retrievers<a class=headerlink href=#retrievers title="Permanent link">&para;</a></h3> <p>RAG architecture is based by the retrieval method used, such as BM25 (a traditional one) or more advanced dense retrievers which use neural network based embeddings.</p> <p>The main concept is using the TF-IDF measure: it is a parse (most words never occur) retrieval approach using to compute a cost function for a query within a document, based on the term-frequency (TF) and the <a href=https://en.wikipedia.org/wiki/Tf%E2%80%93idf>inverse document frequency (IDF)</a> which measures the importance of a word to a document. </p> <p><a href=https://arxiv.org/abs/2004.04906>Dense retrieval</a> brings semantic similarity (cosinus score between embeddings) on a dense representations of words and documents by pre-training the retriever with relevant information. </p> <h4 id=retriever-considerations>Retriever considerations<a class=headerlink href=#retriever-considerations title="Permanent link">&para;</a></h4> <ul> <li>RAG models need to be fine-tuned to improve the retrieval and the generation processes. The fine tuning includes integrating the specific, domain knowledge to assess the quality of the retrieved information and the generated one.</li> <li>RAG processing needs to take into consideration the different configuration parameters to get good results with minimum latency: consider the number of documents to return, the size of the returned vectors, the total length of the text returned, then number of query to run in parallel. The retrieval vector size impacts the granularity of the semantic match between the query and the documents.</li> <li>Adding efficient metadata about the document (section or chunk) may help to implement an <strong>Hybrid search</strong> strategy which combines semantic search with filtering on metadata or search on keywords.</li> <li>In a multi-step question answering system, it is challenging to select the correct documents based on the question alone. <a href=https://arxiv.org/abs/2212.10509>IRCoT</a> uses LLM to generate a thought sentence used to retrieve documents from the corpus. The documents are then added to the context and the prompt.</li> <li>Effective retrieval is fundamental in RAG system. Assessing the quality of the search results is not easy, and may combine similarity matrix and rule based systems. Different algorithms may be used like cosine similarity, multi query retrievers, ensemble retrievers.</li> <li>RAG may generate wrong results, so some quality control needs to be deployed to remove noise.</li> <li>RAG application design should address all the discovery questions and so use extensive planning, extensive testing using multi-scenario of user behavior and query. Use "what-if" simulations. Address hallucination prevention, privacy protection, and source quality control.</li> <li>Start small with all the guard rails in place.</li> <li>Using small LLM for embedding may lead to issues as some terms used in the knowledge based may not be part of the primary LLM corpus. If cost and skill are not an issue, then training its own LLM may be a better solution to reach higher quality, as even fine tuning a model may not bring enough quality to the responses.</li> </ul> <h4 id=assessing-the-quality-of-rag-systems>Assessing the quality of RAG systems<a class=headerlink href=#assessing-the-quality-of-rag-systems title="Permanent link">&para;</a></h4> <p>Evaluating the performance of a RAG systems, brings its own challenges, specially the ability to retrieve relevant information. Traditional approach uses human annotations, or heuristic prompts. Annotations is time consuming and expensive, and subject to human biases.</p> <p>Looking at the results, we can have too many matches, or not match at all because of loss of important information within the document which is relevant to the search. The user may want to get the most recent information which may not be the most semantically similar.</p> <p>There are two new approaches to evaluate RAG, the Facts as a Function (FaaS) <a href=https://arxiv.org/html/2403.03888v2>Katranidis-Barany</a> or <a href=https://arxiv.org/html>ARES (Automated Retrieval Augmented Generation Evaluation System)</a>. With FaaS, a fact is a callable function using json objects, used to improve the interpretability of the RAG evaluation.</p> <p>ARES uses LLM to generate query-passage-answer triples and then fine-tuned LLM judges to assess the RAG. It leverages both human annotated data and LLM generated ones.</p> <h2 id=more-advanced-rag>More advanced RAG<a class=headerlink href=#more-advanced-rag title="Permanent link">&para;</a></h2> <p>The following advanced techniques to improve RAG limitations can be done with sequences of LLM call in a controlled manner or via Agents.</p> <p>The important part is to be able to build a better context content by being able to retrieve the relevant documents (using keyword retrieval, embedding retrieval or an hybrid approach), and order the documents by relevance. Most LLM tends to do better response from the beginning of the context.</p> <h3 id=query-transformations>Query transformations<a class=headerlink href=#query-transformations title="Permanent link">&para;</a></h3> <p>Query transformations focus on re-writing and / or modifying questions for retrieval using LLM to create other related questions. The prompt declares to generate different version of the user's question. The goal is to try to address limitations of the distance-based similarity search. </p> <ul> <li>With <strong>multiple query</strong> LLM returns 4 to 5 questions which are used to query the vector store via the retriever and then merge the returned documents by removing any duplicate, to finally send the merged documents to the LLM for generation. </li> </ul> <p><img alt src=../diagrams/rag_multi_query.drawio.png></p> <p>See the code in <a href=https://github.com/jbcodeforce/ML-studies/blob/7a9d7b86fac629e01ad65bc390b2c7e83d019da5/llm-langchain/openAI/multiple_queries_rag.py>multiple_queries_rag.py</a>.</p> <ul> <li>With <strong>Rag fusion</strong> the approach is to apply merging logic using a function, so developer can apply some filtering and heuristics:</li> </ul> <p><img alt src=../diagrams/rag_fusion.drawio.png></p> <p>And the related code in <a href=https://github.com/jbcodeforce/ML-studies/blob/7a9d7b86fac629e01ad65bc390b2c7e83d019da5/llm-langchain/openAI/rag_fusion.py>rag_fusion.py</a>.</p> <ul> <li><strong>Answer recursively</strong> chain the Q&amp;A and use the response of previous calls as part of the input context for the next question. The prompt looks like:</li> </ul> <div class=highlight><pre><span></span><code><span class=n>template</span> <span class=o>=</span> <span class=s2>&quot;&quot;&quot;Here is the question you need to answer:</span>
<span class=se>\n</span><span class=s2> --- </span><span class=se>\n</span><span class=s2> </span><span class=si>{question}</span><span class=s2> </span><span class=se>\n</span><span class=s2> --- </span><span class=se>\n</span>
<span class=s2>Here is any available background question + answer pairs:</span>
<span class=se>\n</span><span class=s2> --- </span><span class=se>\n</span><span class=s2> </span><span class=si>{q_a_pairs}</span><span class=s2> </span><span class=se>\n</span><span class=s2> --- </span><span class=se>\n</span>
<span class=s2>Here is additional context relevant to the question: </span>
<span class=se>\n</span><span class=s2> --- </span><span class=se>\n</span><span class=s2> </span><span class=si>{context}</span><span class=s2> </span><span class=se>\n</span><span class=s2> --- </span><span class=se>\n</span>
<span class=s2>Use the above context and any background question + answer pairs to answer the question: </span><span class=se>\n</span><span class=s2> </span><span class=si>{question}</span>
<span class=s2>&quot;&quot;&quot;</span>
</code></pre></div> <ul> <li><strong>Answer individually</strong> use LLM for each question and regroup the answers in the context for the original question. </li> </ul> <p><img alt src=../diagrams/rag_answ_rec.drawio.png></p> <p>The question creation prompt is <a href=https://smith.langchain.com/hub/rlm/rag-prompt>rlm/rag-prompt</a>. The final prompt looks like:</p> <div class=highlight><pre><span></span><code><span class=n>context</span> <span class=o>=</span> <span class=n>format_qa_pairs</span><span class=p>(</span><span class=n>questions</span><span class=p>,</span> <span class=n>answers</span><span class=p>)</span>
<span class=n>template</span> <span class=o>=</span> <span class=s2>&quot;&quot;&quot;Here is a set of Q+A pairs:</span>

<span class=si>{context}</span>

<span class=s2>Use these to synthesize an answer to the question: </span><span class=si>{question}</span>
<span class=s2>&quot;&quot;&quot;</span>
</code></pre></div> <ul> <li>With <strong>Hypothetical Document Embedding (HyDE)</strong> the first prompt create an hypothetical document using a prompt like:</li> </ul> <div class=highlight><pre><span></span><code><span class=n>template</span> <span class=o>=</span> <span class=s2>&quot;&quot;&quot;Please write a scientific paper passage to answer the question</span>
<span class=s2>Question: </span><span class=si>{question}</span>
<span class=s2>Passage:&quot;&quot;</span>
</code></pre></div> <p>See the related code in <a href=https://github.com/jbcodeforce/ML-studies/blob/master/RAG/rag_HyDE.py>rag_HyDE.py</a>.</p> <h3 id=query-routing>Query Routing<a class=headerlink href=#query-routing title="Permanent link">&para;</a></h3> <p>When we have multiple indexes for different domains, and for different questions, we want to query different subsets of these indexes. Query routing is the process of classifying which index or subset of indexes a query should be performed on. For that we use logical and semantic routing.</p> <h3 id=knowledge-graph-integration-in-rag>Knowledge graph integration in RAG<a class=headerlink href=#knowledge-graph-integration-in-rag title="Permanent link">&para;</a></h3> <p>From the standard RAG architecture, the pre-processing step may be modified by adding context to the query before it performs a retrieval from the vector database. This context may specify enterprise specific ontology and term definitions. Since years, Knowledge graphs (KG) are helping search engine to build acronym dictionaries.</p> <details class=info open=open> <summary>Knowledge Graph</summary> <p><em>A Knowledge Graph is a set of data points connected by relations that describe a domain, for instance, a business, an organization, or a field of study.</em></p> </details> <p>Question may be broken down into sub-questions and can require numerous documents to be provided to the LLM to generate an accurate answer.</p> <p>For chunks selection, document hierarchies can be used to reference which documents the query needs to use. One KG with document hierarchy to chunks in the vector database.</p> <p>Use contextual dictionary to understand which document chunks contain important topics. Natural language rules define how to search document related to the meaning of the query.</p> <p>KG may help to add additional information that must exist in any answer referring to a specific concept that failed to be retrieved or did not exist in the vector database. This is the concept of <strong>answer augmentation</strong>. </p> <p>Rules may be used to eliminate repetition within the LLM results, and personalize response to the users.</p> <p>An hypothetical sequence diagram for a RAG orchestrator enhanced by a knowledge graph, may look like:</p> <pre class=mermaid><code>sequenceDiagram
    UI->>Orchestrator: initial query
    activate Orchestrator
    Orchestrator->>KG: initial query
    deactivate Orchestrator
    activate KG
    KG->>Orchestrator: specific terms and ontology
    activate Orchestrator
    deactivate KG
    Orchestrator->>Vector Database: augmented query
    deactivate Orchestrator
    activate Vector Database
    Vector Database->>KG: search document context
    activate KG
    KG->>Vector Database: chunks mapping
    deactivate KG
    Vector Database->>Orchestrator: context for llm
    deactivate Vector Database
    activate Orchestrator
    Orchestrator->>LLM: query, context
    activate LLM
    LLM->>Orchestrator: LLM response
    deactivate LLM
    Orchestrator->>KG: LLM response
    KG->>Orchestrator: augmented LLM response
    Orchestrator->>UI: augmented LLM response
    deactivate Orchestrator
</code></pre> <p>Knowledge graph is easily extractable in a coherent form. </p> <h2 id=sources-of-information>Sources of information<a class=headerlink href=#sources-of-information title="Permanent link">&para;</a></h2> <ul> <li> <p><a href=https://medium.com/enterprise-rag/injecting-knowledge-graphs-in-different-rag-stages-a3cd1221f57b>Read more from this medium article</a>.</p> </li> <li> <p><a href=https://github.com/Sujit-O/pykg2vec>Pykg2vec- Python Library for KGE Methods</a></p> </li> <li><a href=https://learn.deeplearning.ai/courses/preprocessing-unstructured-data-for-llm-application>Deeplearning.ai - Preprocessing Unstructured Data for LLM Applications</a></li> </ul> <h2 id=some-code-studies>Some Code Studies<a class=headerlink href=#some-code-studies title="Permanent link">&para;</a></h2> <h3 id=langchain>LangChain<a class=headerlink href=#langchain title="Permanent link">&para;</a></h3> <p>For a classical RAG using LangChain:</p> <p><img alt src=../../coding/diagrams/rag-process.drawio.png></p> <p>RAG produces good results, due to augmenting use-case specific context coming directly from vectorized information stores. It has the highest degree of flexibility when it comes to changes in the architecture. We can change the embedding model, vector store and LLM independently with minimal to moderate impact on other components.</p> <p>Training from scratch produces the highest quality result amongst Prompt, RAG, fine tuning, but cost far more and need deep data science skill set.</p> <p><a href=../../coding/langchain/#retrieval-augmented-generation>See hands-on with LangChain</a>.</p> <ul> <li><a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/rag>Llm-langchain RAG folder</a></li> </ul> <h3 id=langgraph>LangGraph<a class=headerlink href=#langgraph title="Permanent link">&para;</a></h3> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../architecture/sol-design/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Solution Design"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Solution Design </div> </div> </a> <a href=../agentic/ class="md-footer__link md-footer__link--next" aria-label="Next: Agents"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Agents </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2018 - 2024 Jerome Boyer </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/jbcodeforce target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill=currentColor d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://linkedin.com/in/jeromeboyer target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill=currentColor d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill=currentColor d="M536.4-26.3c9.8-3.5 20.6-1 28 6.3s9.8 18.2 6.3 28l-178 496.9c-5 13.9-18.1 23.1-32.8 23.1-14.2 0-27-8.6-32.3-21.7l-64.2-158c-4.5-11-2.5-23.6 5.2-32.6l94.5-112.4c5.1-6.1 4.7-15-.9-20.6s-14.6-6-20.6-.9l-112.4 94.3c-9.1 7.6-21.6 9.6-32.6 5.2L38.1 216.8c-13.1-5.3-21.7-18.1-21.7-32.3 0-14.7 9.2-27.8 23.1-32.8z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "content.code.copy", "search.highlight", "navigation.instant", "navigation.instant.progress", "navigation.tabs", "navigation.tabs.sticky", "navigation.instant", "navigation.tracking", "navigation.top", "navigation.footer"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.92b07e13.min.js></script> </body> </html>