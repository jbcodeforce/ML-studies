<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://jeromeboyer.net/ML-studies/genAI/ rel=canonical><link href=../coding/ddp/ rel=prev><link href=review/ rel=next><link rel=icon href=../assets/logo.drawio.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>Introduction - Machine Learning Studies - Jerome Boyer</title><link rel=stylesheet href=../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#generative-ai class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="Machine Learning Studies - Jerome Boyer" class="md-header__button md-logo" aria-label="Machine Learning Studies - Jerome Boyer" data-md-component=logo> <img src=../assets/logo.drawio.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Machine Learning Studies - Jerome Boyer </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Introduction </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/jbcodeforce/ML-studies title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> About </a> </li> <li class=md-tabs__item> <a href=../ml/ class=md-tabs__link> Machine Learning </a> </li> <li class=md-tabs__item> <a href=../ml/deep-learning/ class=md-tabs__link> Deep Learning </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=./ class=md-tabs__link> Generative AI </a> </li> <li class=md-tabs__item> <a href=../techno/airflow/ class=md-tabs__link> Techno </a> </li> <li class=md-tabs__item> <a href=../coding/ class=md-tabs__link> Coding </a> </li> <li class=md-tabs__item> <a href=../solutions/ class=md-tabs__link> Solutions </a> </li> <li class=md-tabs__item> <a href=https://jbcodeforce.github.io class=md-tabs__link> Home </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="Machine Learning Studies - Jerome Boyer" class="md-nav__button md-logo" aria-label="Machine Learning Studies - Jerome Boyer" data-md-component=logo> <img src=../assets/logo.drawio.png alt=logo> </a> Machine Learning Studies - Jerome Boyer </label> <div class=md-nav__source> <a href=https://github.com/jbcodeforce/ML-studies title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <label class=md-nav__link for=__nav_1 id=__nav_1_label tabindex=0> <span class=md-ellipsis> About </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> About </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../guide_for_ai/ class=md-nav__link> <span class=md-ellipsis> Guide for AI/ML </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_3> <label class=md-nav__link for=__nav_1_3 id=__nav_1_3_label tabindex=0> <span class=md-ellipsis> Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_1_3> <span class="md-nav__icon md-icon"></span> Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../concepts/ class=md-nav__link> <span class=md-ellipsis> Core concepts </span> </a> </li> <li class=md-nav__item> <a href=../concepts/maths/ class=md-nav__link> <span class=md-ellipsis> Math summary </span> </a> </li> <li class=md-nav__item> <a href=../concepts/skill/ class=md-nav__link> <span class=md-ellipsis> Data scientist skill </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Architecture </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Architecture </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../architecture/sol-design/ class=md-nav__link> <span class=md-ellipsis> Solution Design </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_5> <label class=md-nav__link for=__nav_1_5 id=__nav_1_5_label tabindex=0> <span class=md-ellipsis> Data management </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_5_label aria-expanded=false> <label class=md-nav__title for=__nav_1_5> <span class="md-nav__icon md-icon"></span> Data management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../data/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../data/features/ class=md-nav__link> <span class=md-ellipsis> Feature Engineering </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Machine Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Machine Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ml/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../ml/classifier/ class=md-nav__link> <span class=md-ellipsis> Classifier </span> </a> </li> <li class=md-nav__item> <a href=../ml/unsupervised/ class=md-nav__link> <span class=md-ellipsis> Unsupervised Learning </span> </a> </li> <li class=md-nav__item> <a href=../anomaly/ class=md-nav__link> <span class=md-ellipsis> Anomaly detection </span> </a> </li> <li class=md-nav__item> <a href=../neuro-symbolic/ class=md-nav__link> <span class=md-ellipsis> Hybrid AI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Deep Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Deep Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ml/deep-learning/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../ml/nlp/ class=md-nav__link> <span class=md-ellipsis> NLP </span> </a> </li> <li class=md-nav__item> <a href=../coding/ddp/ class=md-nav__link> <span class=md-ellipsis> Distributed Data Parallel </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> Generative AI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Generative AI </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Introduction </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Introduction </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> <nav class=md-nav aria-label=Introduction> <ul class=md-nav__list> <li class=md-nav__item> <a href=#transformer-architecture class=md-nav__link> <span class=md-ellipsis> Transformer Architecture </span> </a> </li> <li class=md-nav__item> <a href=#pre-training-process class=md-nav__link> <span class=md-ellipsis> Pre-training process </span> </a> </li> <li class=md-nav__item> <a href=#generic-development-approach class=md-nav__link> <span class=md-ellipsis> Generic development approach </span> </a> </li> <li class=md-nav__item> <a href=#model-fine-tuning class=md-nav__link> <span class=md-ellipsis> Model fine-tuning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#use-cases class=md-nav__link> <span class=md-ellipsis> Use cases </span> </a> <nav class=md-nav aria-label="Use cases"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#industries class=md-nav__link> <span class=md-ellipsis> Industries </span> </a> </li> <li class=md-nav__item> <a href=#classical-concerns-and-challenges class=md-nav__link> <span class=md-ellipsis> Classical concerns and challenges </span> </a> </li> <li class=md-nav__item> <a href=#some-fallacies class=md-nav__link> <span class=md-ellipsis> Some fallacies </span> </a> </li> <li class=md-nav__item> <a href=#interesting-legal-considerations class=md-nav__link> <span class=md-ellipsis> Interesting legal considerations </span> </a> </li> <li class=md-nav__item> <a href=#discovery-assessment class=md-nav__link> <span class=md-ellipsis> Discovery assessment </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#concepts class=md-nav__link> <span class=md-ellipsis> Concepts </span> </a> <nav class=md-nav aria-label=Concepts> <ul class=md-nav__list> <li class=md-nav__item> <a href=#nlp-processing class=md-nav__link> <span class=md-ellipsis> NLP processing </span> </a> </li> <li class=md-nav__item> <a href=#important-terms class=md-nav__link> <span class=md-ellipsis> Important Terms </span> </a> </li> <li class=md-nav__item> <a href=#summarization class=md-nav__link> <span class=md-ellipsis> Summarization </span> </a> </li> <li class=md-nav__item> <a href=#retrieval-augmented-generation-rag class=md-nav__link> <span class=md-ellipsis> Retrieval augmented generation (RAG) </span> </a> </li> <li class=md-nav__item> <a href=#common-llm-inference-parameters class=md-nav__link> <span class=md-ellipsis> Common LLM inference parameters </span> </a> </li> <li class=md-nav__item> <a href=#vector-database class=md-nav__link> <span class=md-ellipsis> Vector Database </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#current-technology-landscape class=md-nav__link> <span class=md-ellipsis> Current Technology Landscape </span> </a> <nav class=md-nav aria-label="Current Technology Landscape"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#openai-chatgpt class=md-nav__link> <span class=md-ellipsis> OpenAI - ChatGPT </span> </a> </li> <li class=md-nav__item> <a href=#meta-with-llama-2-3 class=md-nav__link> <span class=md-ellipsis> Meta with LLama 2 &amp; 3 </span> </a> </li> <li class=md-nav__item> <a href=#mistral-mixture-of-experts class=md-nav__link> <span class=md-ellipsis> Mistral - Mixture of Experts </span> </a> </li> <li class=md-nav__item> <a href=#google-gemini class=md-nav__link> <span class=md-ellipsis> Google Gemini </span> </a> </li> <li class=md-nav__item> <a href=#amazon-sagemaker class=md-nav__link> <span class=md-ellipsis> Amazon SageMaker </span> </a> </li> <li class=md-nav__item> <a href=#local-inference-and-training class=md-nav__link> <span class=md-ellipsis> Local inference and training </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#deeper-dive class=md-nav__link> <span class=md-ellipsis> Deeper dive </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=review/ class=md-nav__link> <span class=md-ellipsis> Skill set </span> </a> </li> <li class=md-nav__item> <a href=prompt-eng/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> </a> </li> <li class=md-nav__item> <a href=../architecture/sol-design/ class=md-nav__link> <span class=md-ellipsis> Solution Design </span> </a> </li> <li class=md-nav__item> <a href=rag/ class=md-nav__link> <span class=md-ellipsis> RAG </span> </a> </li> <li class=md-nav__item> <a href=agentic/ class=md-nav__link> <span class=md-ellipsis> Agentic AI </span> </a> </li> <li class=md-nav__item> <a href=anthropic/ class=md-nav__link> <span class=md-ellipsis> Anthropic </span> </a> </li> <li class=md-nav__item> <a href=openai/ class=md-nav__link> <span class=md-ellipsis> OpenAI </span> </a> </li> <li class=md-nav__item> <a href=mcp/ class=md-nav__link> <span class=md-ellipsis> MCP </span> </a> </li> <li class=md-nav__item> <a href=mistral/ class=md-nav__link> <span class=md-ellipsis> Mistral </span> </a> </li> <li class=md-nav__item> <a href=cohere/ class=md-nav__link> <span class=md-ellipsis> Cohere </span> </a> </li> <li class=md-nav__item> <a href=../techno/watsonx/ class=md-nav__link> <span class=md-ellipsis> WatsonX.ai </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Techno </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Techno </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../techno/airflow/ class=md-nav__link> <span class=md-ellipsis> Airflow </span> </a> </li> <li class=md-nav__item> <a href=../techno/feature_store/ class=md-nav__link> <span class=md-ellipsis> Feature Store </span> </a> </li> <li class=md-nav__item> <a href=../kaggle/ class=md-nav__link> <span class=md-ellipsis> Kaggle </span> </a> </li> <li class=md-nav__item> <a href=../techno/opensearch/ class=md-nav__link> <span class=md-ellipsis> OpenSearch </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/python-code/ class=md-nav__link> <span class=md-ellipsis> Python studies </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/yarfba/ai-ml/sagemaker class=md-nav__link> <span class=md-ellipsis> SageMaker </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/spark-studies/ class=md-nav__link> <span class=md-ellipsis> Spark studies </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_8> <label class=md-nav__link for=__nav_5_8 id=__nav_5_8_label tabindex=0> <span class=md-ellipsis> UI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_8_label aria-expanded=false> <label class=md-nav__title for=__nav_5_8> <span class="md-nav__icon md-icon"></span> UI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../techno/gradio/ class=md-nav__link> <span class=md-ellipsis> Gradio </span> </a> </li> <li class=md-nav__item> <a href=../techno/streamlit/ class=md-nav__link> <span class=md-ellipsis> Streamlit </span> </a> </li> <li class=md-nav__item> <a href=../techno/taipy/ class=md-nav__link> <span class=md-ellipsis> TaiPy </span> </a> </li> <li class=md-nav__item> <a href=../techno/nicegui/ class=md-nav__link> <span class=md-ellipsis> NiceGUI </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../techno/gcp/ class=md-nav__link> <span class=md-ellipsis> GCP </span> </a> </li> <li class=md-nav__item> <a href=../techno/watsonx/ class=md-nav__link> <span class=md-ellipsis> WatsonX.ai </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Coding </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Coding </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../coding/ class=md-nav__link> <span class=md-ellipsis> Coding getting started </span> </a> </li> <li class=md-nav__item> <a href=../coding/visualization/ class=md-nav__link> <span class=md-ellipsis> Data Visualization </span> </a> </li> <li class=md-nav__item> <a href=../coding/haystack/ class=md-nav__link> <span class=md-ellipsis> Haystack.ai </span> </a> </li> <li class=md-nav__item> <a href=../coding/langchain/ class=md-nav__link> <span class=md-ellipsis> LangChain </span> </a> </li> <li class=md-nav__item> <a href=../coding/langgraph/ class=md-nav__link> <span class=md-ellipsis> LangGraph </span> </a> </li> <li class=md-nav__item> <a href=../coding/llama-index/ class=md-nav__link> <span class=md-ellipsis> LlamaIndex </span> </a> </li> <li class=md-nav__item> <a href=../coding/pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../coding/pytorch/ class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../coding/sklearn/ class=md-nav__link> <span class=md-ellipsis> Scikit-learn </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> <span class=md-ellipsis> Solutions </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Solutions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../solutions/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../techno/players_to_look/ class=md-nav__link> <span class=md-ellipsis> Key AI startups </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> <nav class=md-nav aria-label=Introduction> <ul class=md-nav__list> <li class=md-nav__item> <a href=#transformer-architecture class=md-nav__link> <span class=md-ellipsis> Transformer Architecture </span> </a> </li> <li class=md-nav__item> <a href=#pre-training-process class=md-nav__link> <span class=md-ellipsis> Pre-training process </span> </a> </li> <li class=md-nav__item> <a href=#generic-development-approach class=md-nav__link> <span class=md-ellipsis> Generic development approach </span> </a> </li> <li class=md-nav__item> <a href=#model-fine-tuning class=md-nav__link> <span class=md-ellipsis> Model fine-tuning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#use-cases class=md-nav__link> <span class=md-ellipsis> Use cases </span> </a> <nav class=md-nav aria-label="Use cases"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#industries class=md-nav__link> <span class=md-ellipsis> Industries </span> </a> </li> <li class=md-nav__item> <a href=#classical-concerns-and-challenges class=md-nav__link> <span class=md-ellipsis> Classical concerns and challenges </span> </a> </li> <li class=md-nav__item> <a href=#some-fallacies class=md-nav__link> <span class=md-ellipsis> Some fallacies </span> </a> </li> <li class=md-nav__item> <a href=#interesting-legal-considerations class=md-nav__link> <span class=md-ellipsis> Interesting legal considerations </span> </a> </li> <li class=md-nav__item> <a href=#discovery-assessment class=md-nav__link> <span class=md-ellipsis> Discovery assessment </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#concepts class=md-nav__link> <span class=md-ellipsis> Concepts </span> </a> <nav class=md-nav aria-label=Concepts> <ul class=md-nav__list> <li class=md-nav__item> <a href=#nlp-processing class=md-nav__link> <span class=md-ellipsis> NLP processing </span> </a> </li> <li class=md-nav__item> <a href=#important-terms class=md-nav__link> <span class=md-ellipsis> Important Terms </span> </a> </li> <li class=md-nav__item> <a href=#summarization class=md-nav__link> <span class=md-ellipsis> Summarization </span> </a> </li> <li class=md-nav__item> <a href=#retrieval-augmented-generation-rag class=md-nav__link> <span class=md-ellipsis> Retrieval augmented generation (RAG) </span> </a> </li> <li class=md-nav__item> <a href=#common-llm-inference-parameters class=md-nav__link> <span class=md-ellipsis> Common LLM inference parameters </span> </a> </li> <li class=md-nav__item> <a href=#vector-database class=md-nav__link> <span class=md-ellipsis> Vector Database </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#current-technology-landscape class=md-nav__link> <span class=md-ellipsis> Current Technology Landscape </span> </a> <nav class=md-nav aria-label="Current Technology Landscape"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#openai-chatgpt class=md-nav__link> <span class=md-ellipsis> OpenAI - ChatGPT </span> </a> </li> <li class=md-nav__item> <a href=#meta-with-llama-2-3 class=md-nav__link> <span class=md-ellipsis> Meta with LLama 2 &amp; 3 </span> </a> </li> <li class=md-nav__item> <a href=#mistral-mixture-of-experts class=md-nav__link> <span class=md-ellipsis> Mistral - Mixture of Experts </span> </a> </li> <li class=md-nav__item> <a href=#google-gemini class=md-nav__link> <span class=md-ellipsis> Google Gemini </span> </a> </li> <li class=md-nav__item> <a href=#amazon-sagemaker class=md-nav__link> <span class=md-ellipsis> Amazon SageMaker </span> </a> </li> <li class=md-nav__item> <a href=#local-inference-and-training class=md-nav__link> <span class=md-ellipsis> Local inference and training </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#deeper-dive class=md-nav__link> <span class=md-ellipsis> Deeper dive </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=generative-ai>Generative AI<a class=headerlink href=#generative-ai title="Permanent link">&para;</a></h1> <details class="- info"> <summary>Updates</summary> <p>Created Aug 2023 - Updated 08/2024</p> </details> <h2 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h2> <p>Generative AI is a combination of neural network models to create new content (text, image, music, videos..) from a requesting query. Models are pre-trained on vast amounts of unlabeled data, using from 7B up to 500B of parameters. Current Gen AI models are based on the Transformer architecture.</p> <p>Gen AI applies well to different category of use cases: improve customer experiences, improve employee's productivity, help around creativity, and help optimizing business process (<a href=#use-cases>See also the Use Case section</a>). </p> <h3 id=transformer-architecture>Transformer Architecture<a class=headerlink href=#transformer-architecture title="Permanent link">&para;</a></h3> <p>Transformer is a neural network used to generate the next word in the sentence using the best probability. This is what most chat application use to propose the next word sugestion we can select. If we just select the most likely words we got paragraph with no real meaning. The context of the text is lost at each word. Adding the self-<strong>attention</strong> mechanism to the transformer, it helps to weight the significance of different words by taking into account the previously seen context. </p> <p>The <strong>attention</strong> mechanism computes the similarity between tokens (from the embeddings of words) in a sequence. That way, the model builds an intuition of what the text is saying. The closer two words are in a vector space, the higher the attention scores they will obtain and the higher the attention they will give to each other (Recall the example of "bank of the river" vs "money in the bank"). </p> <p>The transformer architecture looks like:</p> <p><img alt src=diagrams/transform-arch.drawio.png></p> <ul> <li>The tokenization step takes every word, prefix, suffix, and punctuation signs and assign a matching token</li> <li>Embedding to transform token to numerical vector</li> <li>Positional encoding consists of adding a sequence of predefined vectors to the embedding vectors of the words. This ensures we get a unique vector for every sentence, and sentences with the same words in different order will be assigned different vectors</li> <li>The <a href=https://docs.cohere.com/docs/the-attention-mechanism>attention</a> component is added at every block of the feedforward network. It uses multi-head attention where several different embeddings are used to modify the vectors and add context to them. </li> <li>There is a large number of transformer blocks in the network. The architecture has the <strong>layers</strong> of transformers stacked on top of each other. Within each layer, there are feed-forward neural networks to process the data.</li> <li>The last step of a transformer is a softmax layer, which turns these scores into probabilities (that add to 1). The model returns result tokens which are then turned back into readable text.</li> </ul> <p>The models are trained on vast amounts (Terabytes) of text data like books, articles, websites etc. This helps the model learn grammar, facts, reasoning abilities and even some level of common sense from the content. </p> <p><strong>GPT-3 (Generative Pre-trained Transformer 3)</strong> breaks the NLP boundaries with training on 175B parameters. </p> <p>The training has two stages: <strong>Pre-training</strong> where the model attempts to predict the next word in a sentence using its own corpus, and <strong>fine tuning</strong> where the model can be tuned for specific <em>tasks</em> or <em>content</em>. During the pre-training process, the model automatically takes context into account from all the training data, and tracks relationships in sequential data, like the words in a sentence, to develop some understanding of the real world. </p> <p>The models are commonly referred to as <strong>foundation models</strong> (FMs).</p> <p>The unlabeled data used for pre-training is usually obtained by crawling the web and public sources.</p> <p>At <strong>inference</strong> time, the input text is tokenized into individual tokens which are fed into the model. </p> <details class="- info"> <summary>Difference between ML and LLM</summary> <ul> <li><strong>Foundational Models</strong> can perform many tasks because they contain a large number of parameters that make them capable of learning complex concepts. Through their pre-training exposure to <strong>internet-scale</strong> unstructured data in all its various forms and myriad of patterns, FMs learn to apply their knowledge within a wide range of contexts.</li> <li><strong>Regular models</strong> are trained for one <strong>specific task</strong>, like image classification or speech recognition. ML models require lots of <strong>labeled data</strong> relevant to their task.</li> </ul> </details> <p>The largest pre-trained model in 2019 (BERT) was 330M parameters while the state-of-the-art LLM in 2023 is 540B parameters.</p> <p>A transformer-based model has an encoder component that converts the input text into embeddings, and a decoder component that consumes these embeddings to emit some output text. Transformers process the entire input all at once, during the learning cycle, and therefore can be parallelized.</p> <p>Three types of transformer:</p> <ol> <li><strong>Encoded only</strong>: generate no human readable content, used when applications need to efficiently query content to find similar items.</li> <li><strong>Encoder-decoder</strong> model is trained to treat every natural language processing (NLP) problem (e.g., translate an input string in one language to another) as a text-to-text conversion problem.</li> <li><strong>Decoder-only</strong> model is for text generation.</li> </ol> <p>Models with encoder-decoder and decoder-only architectures are <strong>generative</strong> models.</p> <p>The process is text -&gt; tokens (a token may be less than a word, and on average a 5 chars) -&gt; vector. Vectors of similar word are close in the multi-dimensional space. A vector, in NLP, has a lot of dimensions, representing its characteristics in the world of meaning. The best tokenization method for a given dataset and task is not always clear, and different methods have their own strengths and weaknesses. Sub-word tokenization combines the benefits of character and word tokenization by breaking down rare words into smaller units while keeping frequent words as unique entities.</p> <h4 id=huggingface-transformer>HuggingFace Transformer<a class=headerlink href=#huggingface-transformer title="Permanent link">&para;</a></h4> <p><a href=https://github.com/huggingface/transformers>HuggingFace Transformer</a> provides thousands of pre-trained models to perform tasks on text, images and audio.</p> <h3 id=pre-training-process>Pre-training process<a class=headerlink href=#pre-training-process title="Permanent link">&para;</a></h3> <p>The goal of pre-training is to teach the model the structure, patterns and semantics of the human language. The pre-training process for GPT-3 involves collecting and preprocessing vast amounts of diverse text data, training a Transformer-based model to predict the next token in a sequence, and optimizing the model using powerful computational resources.</p> <p><strong>Corpus</strong> = a collection of texts, and a vocabulary is the set of unique tokens found within the corpus. Corpus needs to be large and with high quality data.</p> <p>The process looks like in the figure below:</p> <p><img alt src=diagrams/pre-training.drawio.png></p> <p>For the <strong>data collection</strong>, it is import to get diverse source of data, including web sites, books, curated datasets to address wide range of topics, writing styles and linguistic nuances. Data preparation is still key, but complex as to remove low-quality text, harmful content... As part of this preparation, text can be converted to lowercase to reduce variability. Tokenization helps to handle rare words and different languages.</p> <p>The primary objective during pre-training is to predict the next token in a sequence. This is a form of unsupervised learning where the model learns from the context provided by preceding tokens. </p> <p>The training phase includes the forward pass where input tokens go through the transformer layers. The loss calculation is computing the difference between predicted token and actual next token. Finally the backward pass applies gradients computation to minimize the loss, and tune the model parameters.</p> <p>The entire dataset is split into <strong>batches</strong>, and the model is trained over multiple <strong>epochs</strong>.</p> <p>The optimization phase includes tuning hyper parameters like learning rate and batch size. To be able to scale we need to run training on distributed computers.</p> <p>A portion of the data is set aside as a validation set to monitor the model's performance and prevent overfitting.</p> <p><strong>Perplexity</strong> is a common metric used to evaluate language models, measuring how well the model predicts a sample</p> <h3 id=generic-development-approach>Generic development approach<a class=headerlink href=#generic-development-approach title="Permanent link">&para;</a></h3> <p>Some ways to use Generative AI in business applications:</p> <ul> <li>Build foundation model from scratch: very expensive and time consuming, with highly skilled ML scientists.</li> <li>Reuse existing foundation models available as open-source (hundred of model on <a href=https://huggingface.co/models>Hugging Face hub</a>), then add own corpus on top of it, to fine tune the model for better accuracy.</li> <li>Use generative AI services or APIs offered by foundation model vendors. There is no control over the data, cost and customization. Use <a href=prompt-eng/#prompting-techniques>prompt engineering</a> or <a href=rag/ >RAG techniques</a> to get better answers, .</li> </ul> <details class=-> <summary>Hugging Face</summary> <p><a href=https://huggingface.co/ >Hugging Face</a> is an open-source provider of natural language processing (NLP), which makes it easy to add state of the art ML models to applications. We can deploy and fine-tune pre-trained models reducing the time it takes to set up and use these NLP models from weeks to minutes.</p> </details> <h3 id=model-fine-tuning>Model fine-tuning<a class=headerlink href=#model-fine-tuning title="Permanent link">&para;</a></h3> <p>The <a href=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard>Huggingface LLM leader board</a> is a good source of information for model quality assessments relative to certain use cases.</p> <p>See this detailed article on <a href=https://medium.com/towards-artificial-intelligence/build-your-own-large-language-model-llm-from-scratch-using-pytorch-9e9945c24858>developing a LLM</a> and this one on <a href=https://medium.com/@tuanatran/fine-tuning-large-language-model-with-hugging-face-pytorch-adce80dce2ad>fine tuning</a>.</p> <h2 id=use-cases>Use cases<a class=headerlink href=#use-cases title="Permanent link">&para;</a></h2> <p>We can group the Generative AI use cases in different categories:</p> <details class="- info"> <summary>Improve customer experiences</summary> <ul> <li>Chatbot functionality with context, with better user's experiences. Reduce operational costs using automated response.</li> <li>Documentation summarization: See model like Jurassic-2 Jumbo from <a href=https://www.ai21.com/studio>AI21 studio</a>, claude-v2 works well too.</li> <li>Personalization</li> </ul> </details> <details class="- info"> <summary>Improve employee productivity</summary> <ul> <li>Code generation</li> <li>Translation, reports, summarization...</li> <li>Search via Q&amp;A Agent for specific subject, based on Corporate document processing. LLM helps understanding the text and the questions. The LLM is enriched, trained on proprietary corpus:</li> </ul> <p><img alt src=diagrams/qa-llm.drawio.png width=400></p> <ul> <li>Self service tutor based on student progress, prompt activities, and respond to questions</li> <li>Personalized learning path generation</li> <li>Low-code development with GenAI agents</li> </ul> </details> <details class="- info"> <summary>Creativity</summary> <ul> <li>Auto-generation of marketing material</li> <li>Personalized emails</li> <li>Sales scripts for customer's industry or segment</li> <li>Speeding the ideation phase of a product development</li> </ul> </details> <details class="- info"> <summary>Business process optimization</summary> <ul> <li>Automatically extracting and summarizing data from documents: combine OCR with prompt to extract data and build json doc to be structured for downstream processing: Gen AI based intelligent document processing may looks like this:</li> </ul> <p><img alt src=diagrams/idp-genai.drawio.png></p> <ul> <li>Data augmentation to improve data set quality. Keep the privacy of original data sources, and help trains other models: generate image of rusted pumps to train an anomaly detection model on pumps.</li> <li>Propose some supply chain scenario</li> </ul> </details> <ul> <li> <p>Uber has <a href=https://www.uber.com/blog/the-transformative-power-of-generative-ai/ >conducted Hackathon using genAI</a> and identified that all Software Development Life Cycle phases are impacted by Generative AI usages, including: capturing complete specifications faster, explain existing code, generating UI code, automate code refactoring, unit test generation or e2e testing for mobile app, review code, code relationship map automatically created from call stacks, Pull Request and code review automation, code documentation generation based on PRs. But LLMs may generate buggy code, as well as spreading error-prone code pattern. </p> </li> <li> <p><a href=https://towardsai.net/p/l/gans-for-synthetic-data-generation>Generative Adversarial Networks</a> are used to limit the risk of adversarial manipulation in deep learning image recognition. It attempts to generate fake data that looks real by learning the features from the real data.</p> </li> </ul> <p>It would be difficult to find any business use-case where a base FM can be used effectively. Added techniques are needed to be useful in enterprise, like RAG, fine tuning, new training, knowledge graph and neuro-symbolic AI solutions.</p> <h3 id=industries>Industries<a class=headerlink href=#industries title="Permanent link">&para;</a></h3> <p>These following industry-specific use cases present the potential applications of Generative AI:</p> <ol> <li> <p>Supply Chain Management:</p> <ul> <li>Improve visibility into multi-tier supplier performance concerns</li> <li>Identify potential risk areas within the supply chain</li> </ul> </li> <li> <p>Quality Control and Nonconformance Management:</p> <ul> <li>Identify the root cause of nonconformance issues</li> <li>Prescribe resolutions to address quality concerns</li> </ul> </li> <li> <p>Engineering Cost Optimization:</p> <ul> <li>Promote the reuse of common parts across different platforms to reduce costs</li> </ul> </li> <li> <p>Cross industry:</p> <ul> <li>Improve chatbot user's experience, with open responses more empathic to the user.</li> <li>Sentiment analysis: Gauge customer sentiment towards products, services, or brands</li> <li>Assist with proofreading tasks</li> <li>Update and maintain databases</li> <li>Analyze customer reviews</li> <li>Monitor social media platforms</li> </ul> </li> <li> <p>Education and Universities:</p> <ul> <li>Moderate and develop educational content</li> <li>Help students find the most effective pathways to graduation</li> </ul> </li> <li> <p>Safety and Risk Management:</p> <ul> <li>Identify potential safety risks, such as gas leaks</li> <li>Generate recommendations for remedial work to mitigate risks</li> </ul> </li> <li> <p>Travel Industry:</p> <ul> <li>Enhance trip planning with personalized recommendations, services, and offers</li> </ul> </li> <li> <p>Product Review Summarization:</p> <ul> <li>Offload the task of summarizing product reviews from humans to LLMs</li> <li>Add unstructured reviews as a new corpus for search functionality</li> <li>Separate reviews based on user-provided ratings</li> <li>Task an LLM to extract different sets of information from each high-level category of reviews</li> </ul> </li> </ol> <h3 id=classical-concerns-and-challenges>Classical concerns and challenges<a class=headerlink href=#classical-concerns-and-challenges title="Permanent link">&para;</a></h3> <p>LLM's are amazing tools for doing natural language processing. But they come with challenges due to the underlying training and inference technology, due to the fact that they are trained only occasionally and are thus always out of date, and also due to the fact that natural language generation is not grounded in any model of reality or reasoning but instead uses probabilistic techniques based on correlations of a huge number of strings of tokens (words). Which means hallucination and approximate retrieval are core of their architecture: the completion they are generating is in the same distribution as the text they have been trained on. Prompt engineering does not change hallucination as the decision to assess the response is a factual completion depends of the knowledge of the prompter and requires to continuously assess all the responses.</p> <ul> <li> <p><strong>Accuracy</strong>: The accuracy of LLM's is not acceptable to any enterprise that must follow regulations and policies and respect contractual agreements with suppliers and customers. Because they cannot truly reason or take into account regulations and policies precisely, models often produce incorrect and contradictory answers when asked for decisions or actions to undertake. A single large language model is unlikely to solve every business problem effectively. With classical ML, probabilistic output is expected. Symbolic approaches like business rules that precisely express policies produce reliable results at the cost of coding the policies mostly manually.</p> </li> <li> <p><strong>Specificity</strong>: A single large model is unlikely to solve every business problem effectively because it is trained on generally-available information rather than enterprise-specific information. To differentiate their generative AI applications and achieve optimal performance, enterprises should rely on their <strong>own data sets</strong> tailored to their unique use case. Even then, enterprise data changes constantly, so techniques such as RAG and tool calling are needed to leverage the most up-to-date and relevant information for a specific query.</p> </li> <li> <p><strong>Cost and Risk</strong> of training and inference, as well as privacy and intellectual property are top concerns. LLM's can be "fine-tuned" for a specific task by using a small number of labeled examples specific to the company's industry or use case. Fine-tuned models can deliver more accurate and relevant outputs. But training and retraining models, hosting them, and doing inference with them are expensive. Cloud providers see this opportunity to sell more virtual servers with GPU's at a higher price. </p> </li> <li> <p><strong>Skills</strong>: developing a new LLM may not make sense today, but fine tuning an existing model may in some circumstances. There are relatively few developers with expertise in model tuning, understanding their architecture and limitations, integrating them in applications, and in tuning their hyper parameters. Reinforcement learning to fine-tune existing LLM requires a huge number of trials, and data quality is still a very difficult and poorly-mastered topic.</p> </li> <li> <p><strong>Reliability and reasoning</strong>: Generative AI models do not reason and do not plan accurately. New versions of LLMs attempt to improve in this domain, but by design the transformer architecture is probabilistic and greedy for text generation and does not inherently do any kind of structured symbolic reasoning or manage ontologies of concepts (knowledge graphs). LLM is a very big system-1 with their knowledge based from digital representation of humanity created content.</p> </li> <li> <p>For generative AI, the input is very ambiguous, but also the output: there is no determinist output. Models produce incorrect and contradictory answers. With classical ML, output is well expected. Trained sentiment analysis algorithms on labelled data will perform better than any LLM for that task. Always try to assess when to use ML versus using an existing LLM.</p> </li> <li>There are a lot of models available today, each with unique strengths and characteristics. How to get the ones best suited for business needs? The size of the model is linked to the compute power developers have and how much they can pay (availability of the hardware is also an issue). Effectice mechanisms need to be set up to assess the LLM's responses for each different use cases.</li> <li>There is difficulty to determine which source documents are leading to the given answers. Model that links results to source, via citations help to assess for any hallucinations.</li> <li>Developers need to optimize for training and inference cost, then assess how to amortize the training cost to better evaluate what may be billed to end-users.</li> <li>Current models comparison is based on Multi-task Language Understanding on <a href=https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu>MMLU</a> benchmark. But CIOs care less about standard results, they want models that work well on their data.</li> <li>For large enterprise, adopting LLM at scale means running hundreds, even thousands, of models at any time. A high frequency of innovation, leads customers to replace their models quicker than expected, reinforcing the need to train and deploy new models in production quickly and seamlessly.</li> <li>Cost being a major issue in short term, model may become smaller and access to powerful distributed smaller hardware will help to do inference locally (smartphone with TPU).</li> </ul> <h3 id=some-fallacies>Some fallacies<a class=headerlink href=#some-fallacies title="Permanent link">&para;</a></h3> <ul> <li>LLMs can't do planning in autonomous modes. They may support planning activities done by planner software and human to translate formats, or to elaborate the problem specifications.</li> <li>Chain of Though, ReAct, Fine tuning do not help for planning as they do not generalize well</li> <li>There is no self-verification as LLM has no mean to do self verification</li> </ul> <h3 id=interesting-legal-considerations>Interesting legal considerations<a class=headerlink href=#interesting-legal-considerations title="Permanent link">&para;</a></h3> <ul> <li>Think not created by a human could not be copyrighted.</li> <li>Model deployed will not use data sent to improve itself.</li> <li>The right to use an image/photo to train a model is a problem.</li> <li>Protect Intellectual Property: never pass confidential information to Gen AI SaaS based API.</li> <li>Protect the brand, avoid bias, discrimination, aligned to company values: any business decision should not be done by uncontrolled Gen AI.</li> </ul> <h3 id=discovery-assessment>Discovery assessment<a class=headerlink href=#discovery-assessment title="Permanent link">&para;</a></h3> <p><strong>Classical questions</strong> to address before starting a Gen AI solutions:</p> <ul> <li>How to leverage open model like Mistral, LLama, DBRX?</li> <li>How to verify accuracy on unstructured data such as queries and document shunks? </li> <li>AI Models are becoming stronger when connected to data, enterprise data, and fine tuning. It is important to adopt a Gen AI strategy that is linked to the data strategy too. </li> <li>When transitioning to more stateful Gen AI-based solutions to make the model more specialized or better tuned for dedicated use cases, the model will require self-improvement capabilities. To support this, the solution will need to maintain states, which will be persisted using Lake House technology</li> <li>Current main stakeholder for Gen AI, is the developer, moving to business users will be challenging as some of their jobs are at risk, they may reject the technology at a all.</li> <li>How to stop doing a lot of prompt engineering and start doing model fine tuning? Always address what we should evaluate the solution on. Gen AI can help to build prompt with the use of <a href=prompt-eng/#automatic-prompt-engineering>meta prompting</a> techniques. </li> <li>How to measure hallucination where the models make up inaccurate responses that are not consistent with the training data.</li> <li>How to integrate AI capability in the enterprise business processes and decisions? How does human in the loop step can be added to the process?</li> <li>Whenever we want to teach an LLM to use a tool, we need enough annotated tool calls to fine tune the LLM. We can use in-context learning to create a model that annotates tool calls for the input query. Incorrect calls can be filtered by executing the tools and filtering the outputs based on the ground truth answer.</li> </ul> <h2 id=concepts>Concepts<a class=headerlink href=#concepts title="Permanent link">&para;</a></h2> <p>A LLM is part of the evolution of NLP as it is a trained deep learning model that understands and generates text in a human like fashion.</p> <h3 id=nlp-processing>NLP processing<a class=headerlink href=#nlp-processing title="Permanent link">&para;</a></h3> <p>To process an input text with a transformer model, the text is <strong>tokenized</strong> into a sequence of words or part of words. These tokens are then <strong>encoded</strong> as numbers and converted into <strong>embeddings</strong>, which are vector-space representations of the tokens that preserve their meaning: for example a word dog will have 512 potential numerical attributes used to describe what is a dog. Below is a simple representation of the embedding in the 3 dimension space:</p> <p><img alt src=images/vector-embedding.png></p> <p><em>See the web site <a href=https://projector.tensorflow.org/ >projector.tensorflow.org/</a></em></p> <details class="- info"> <summary>Embedding</summary> <p>See the <a href=https://encord.com/blog/embeddings-machine-learning/ >Encord's guide to embeddings in machine learning</a> and <a href=../ml/nlp/#embedding>this section</a></p> </details> <p>The encoder, in the transformer, transforms the embeddings of all the tokens into a <strong>context vector</strong>. Using this vector, the transformer decoder generates output based on clues. The decoder can produce the subsequent word. We can reuse the same decoder, but this time the clue will be the previously produced next-word. This process can be repeated to create an entire paragraph. This process is called <strong>auto-regressive generation</strong>.</p> <p>When processing text, the AI looks at a few tokens around each word to help understand the context. This surrounding group of tokens is called the <strong>context window</strong>. It is the sliding group of tokens around a word that provides contextual information to help the AI understand and generate natural language.</p> <details class="- info"> <summary>Context Window</summary> <p>A <strong>context window</strong> is the sliding group of tokens around a word that provides contextual information to help the AI understand and generate natural language.</p> <p>If the current word is "apple", the AI might look at a context window of the 5 tokens before and after it. So the context window could be: "I ate a sweet red [apple] this morning for breakfast". The tokens in the context window give the AI useful information about the current word. In this case, they indicate [apple] is probably a noun referring to the fruit. </p> <p>With a narrow context window, the AI has less context to ensure the content flows logically and coherently over a long text. Restrictive context windows can result in more generic, impersonal text. The model has less perspective to generate nuanced or creative content.</p> </details> <p>Transformers do not need to code the grammar rules, they acquire them implicitly from big corpus.</p> <p>During the training process, the model learns the statistical relationships between words, phrases, and sentences, allowing it to generate coherent and contextually relevant responses when given a prompt or query.</p> <p>The techniques to customize LLM applications from simplest to more complex are:</p> <ul> <li><strong>Zero-shot inference</strong>: allows a pre-trained LLM to generate responses to tasks that it hasnt been specifically trained for. In this technique, the model is provided with an input text and a prompt that describes the expected output from the model in natural language. </li> <li>Prompt engineering with zero-shot inference.</li> <li><strong>Prompt engineering with few-shot inference</strong>: <strong>Few-shot</strong> learning involves training a model to perform new tasks by providing few examples. This is useful where limited labeled data is available for training.</li> <li><a href=#retrieval-augmented-generation-rag>Retrieval augmented generation (more complex)</a>.</li> <li>Fine tune an existing foundation model.</li> <li>Pre-train an existing foundation model: example is domain specific model, like the Bloomberg's LLM.</li> <li>Build a foundation model from scratch.</li> <li>Support human in the loop to create high quality data sets.</li> </ul> <h3 id=important-terms>Important Terms<a class=headerlink href=#important-terms title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Term</th> <th>Definition</th> </tr> </thead> <tbody> <tr> <td><strong>Agent</strong></td> <td>Agents give AI apps a fundamentally new set of capabilities: to solve complex problems, to act on the outside world, and to learn from experience post-deployment. <a href=https://github.com/Significant-Gravitas/AutoGPT>Ex. Auto GPT</a>, <a href=../coding/langgraph/ >LangGraph</a></td> </tr> <tr> <td><strong>AI21 Labs</strong></td> <td>AI21 Studio provides API access to Jurassic-2 large language models. Their models power text generation and comprehension features in thousands of live applications. AI21 is building state of the art language models with a focus on understanding meaning.</td> </tr> <tr> <td><strong>BARD</strong></td> <td>AI chat service from Google - powered by the LaMDA model. Similar to ChatGPT. Evolved to Gemini</td> </tr> <tr> <td><strong>BLOOM</strong></td> <td><a href=https://huggingface.co/bigscience/bloom>BLOOM</a> is an auto regressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks. It is a popular open source instructor based model. Developers who want an open source alternative to GPT might look at this.</td> </tr> <tr> <td><strong>co:here</strong></td> <td><a href=cohere/ >Co:here</a> platform can be used to generate or analyze text to do things like write copy, moderate content, classify data and extract information, all at a massive scale.</td> </tr> <tr> <td>Model <strong>compression</strong></td> <td>Technique to reduce the size of the model in memory, it includes <em>quantization</em> (approximating a neural network by using smaller precision 8-bit integers instead of 32-bit floating point numbers) and <em>distillation</em> (transferring of knowledge from a larger teacher model to a smaller student model).</td> </tr> <tr> <td><strong>Data Distributed Training</strong></td> <td>A distributed training algorithm which can speed up ML training by distributing batches of data between forward and backward passes in a model. This can be very helpful when we have large datasets but does not solve the problem of not being able to fit a model on one machine</td> </tr> <tr> <td><strong>DeepSpeed</strong></td> <td>DeepSpeed is an open source deep learning optimization library for PyTorch. The library is designed to reduce computing power and memory usage and to train large distributed models with better parallelism on existing computer hardware. DeepSpeed is optimized for low latency, high throughput training. It can be used to help both inference and training of large models which don't fit on a single GPU.</td> </tr> <tr> <td><strong>Distributed Training</strong></td> <td>In distributed training the workload to train a model is split up and shared among multiple mini processors, called worker nodes. These worker nodes work in parallel to speed up model training.</td> </tr> <tr> <td><strong>Few shot Learning</strong></td> <td>or <em>few-shot prompting</em> is a prompting technique that allows a model to process examples before attempting a task.</td> </tr> <tr> <td><strong>Fine Tuning</strong></td> <td>Foundation model further trained to specific tasks. Example: training BLOOM to summarize chat history where we have examples of these text examples.</td> </tr> <tr> <td><strong>FLAN</strong></td> <td>FLAN(Fine-tuned LAnguage Net): is a LLM with Instruction Fine-Tuning. It is a popular open source instructor based model which scientists can train. Persons who want an open source alternative to GPT might look at this.</td> </tr> <tr> <td><strong>Generative adversarial network (GAN)</strong></td> <td>A deep learning architecture where two networks compete in a zero sum game. When one network wins, the other loses and vice versa. Common applications of this, includes creating new datasets, image generation, and data augmentation. This is a common design paradigm for generative models.</td> </tr> <tr> <td><strong>GPT</strong></td> <td>OpenAI's generalized pre-trained transformer foundation model family. GPT 1 and 2 are open source while 3 and 4 are proprietary. GPT1,2,3 are text-to-text while gpt4 is multi-modal.</td> </tr> <tr> <td><strong>Hallucinations</strong></td> <td>LLMs may give answers which are incorrect or seemingly made up. Hallucinations are mainly a data problem, LLMs suffer from knowledge cut-off where they only know up to the point their training data stops. They also are trained on wide varieties of data some of which can be inaccurate or incomplete. To minimize it, use Top-P, Top-K, Temperature and RAG models.</td> </tr> <tr> <td><strong>Jurassic</strong></td> <td>This is AI21 lab's foundation text to text model. It has instructor and non-instructor based versions and is available on AWS marketplace. This is very appealing for customers because they can get 1) extremely high model quality/accuracy and 2) deploy the model to a dedicated endpoint for dedicated compute.</td> </tr> <tr> <td><strong>LaMDA</strong></td> <td>Language model was trained on dialogue from Google. Very similar to ChatGPT but produced by Google. It is a proprietary model.</td> </tr> <tr> <td><strong>Model compilation</strong></td> <td>Model compilation is the act of tracing a model computational graph in order to deploy to lower level hardware and code. This is a necessary step to run on specialized hardware.</td> </tr> <tr> <td><strong>Model Distribution</strong></td> <td>When a model's size prohibits it from being stored on one GPU. This occurs when models start to be in the 10's of billions of parameter range. This has a few consequences 1) it costs a lot to train and host these models 2) specialized libraries are required to help.</td> </tr> <tr> <td><strong>MultiModal Models</strong></td> <td>Multi-modal learning attempts to model the combination of different modalities of data, often arising in real-world applications. An example of multi-modal data is data that combines text (typically represented as discrete word count vectors) with imaging data consisting of pixel intensities and annotation tags.</td> </tr> <tr> <td><strong>Pre-training</strong></td> <td>Unsupervised learning method which is used to steer foundation models to domain specific information. Example: pre-training FLAN with Medical documents to understand medical context previously missing from the model.</td> </tr> <tr> <td><strong>Reinforcement learning with human feedback (RLHF)</strong></td> <td>The secret sauce to making chat based foundation models. The process involves using human feedback with LLM chat interactions to inform a reinforcement learning procedure to help train an LLM to "talk to humans" instead of only prompts. There are two huge benefits 1/ this substantially reduces the amount of prompt engineering required and 2/ this allow the LLM to take into account chat context as well as the information it has available to it.</td> </tr> <tr> <td><strong>Single shot learning</strong></td> <td><em>Zero-shot learning</em> (ZSL) is a problem setup in ML where, at test time, a learner observes samples from classes which were not observed during training, and needs to predict the class that they belong to</td> </tr> <tr> <td><strong>Stability.ai</strong></td> <td>Stability AI is open source generative AI company currently developing breakthrough AI models applied to imaging, language, code, audio, video, 3D content, design, biotech. With AWS they provide the worlds fifth-largest supercomputer  the Ezra-1 UltraCluster  supplying the necessary power to generate these advancements. Stability AIs premium imaging application DreamStudio, alongside externally built products like Lensa, Wonder and NightCafe, have amassed over 40 million users.</td> </tr> <tr> <td><strong>Stable Diffusion</strong></td> <td>Stable diffusion is a popular open source text to image generation tool. It can be used for use cases like 1/ marketing content generation 2/ game design 3/ fashion design and more.</td> </tr> <tr> <td><strong>Text to text</strong></td> <td>Any model which takes in text inputs and produces text outputs. Ex: entity extraction, summarization, question answer.</td> </tr> <tr> <td><strong>Transfer learning</strong></td> <td>The act of transferring the power of a foundation model to a specific task.</td> </tr> <tr> <td><strong>Transformer</strong></td> <td>A ML model for transforming one sequence into another, using attention.</td> </tr> </tbody> </table> <h3 id=summarization>Summarization<a class=headerlink href=#summarization title="Permanent link">&para;</a></h3> <p>Text summarization is a Natural Language Processing (NLP) technique that involves extracting the most relevant information from a text document and presenting it in a concise and coherent format.</p> <p>Summarization works by sending a prompt instruction to the model, asking the model to summarize our text.</p> <p><a href=../coding/langchain/#summarization-chain>See hands-on notes on LangChain.</a></p> <h3 id=retrieval-augmented-generation-rag>Retrieval augmented generation (RAG)<a class=headerlink href=#retrieval-augmented-generation-rag title="Permanent link">&para;</a></h3> <p><a href=rag/ >See separate chapter</a>.</p> <h3 id=common-llm-inference-parameters>Common LLM inference parameters<a class=headerlink href=#common-llm-inference-parameters title="Permanent link">&para;</a></h3> <h4 id=randomness-and-diversity>Randomness and Diversity<a class=headerlink href=#randomness-and-diversity title="Permanent link">&para;</a></h4> <p>Foundation models support the following parameters to control randomness and diversity in the response:</p> <p><strong>Temperature</strong>  Large language models use probability to construct the words in a sequence. For any given next word, there is a probability distribution of options for the next word in the sequence. When we set the temperature closer to zero, the model tends to select the higher-probability words. When we set the temperature further away from zero, the model may select a lower-probability word which leads to creative output.</p> <p>In technical terms, the temperature modulates the probability density function for the next tokens, implementing the temperature sampling technique. This parameter can deepen or flatten the density function curve. A lower value results in a steeper curve with more deterministic responses, and a higher value results in a flatter curve with more random responses.</p> <p><strong>Top K</strong>  Top K defines the cut off where the model no longer selects the words. For example, if K=50, the model selects from 50 of the most probable words that could be next in a given sequence. This reduces the probability that an unusual word gets selected next in a sequence.</p> <p>In technical terms, Top K is the number of the highest-probability vocabulary tokens to keep for Top-K-filtering - This limits the distribution of probable tokens, so the model chooses one of the highest-probability tokens.</p> <p><strong>Top P</strong>  Top P defines a cut off based on the sum of probabilities of the potential choices. If we set Top P below 1.0, the model considers the most probable options and ignores less probable ones. Top P is similar to Top K, but instead of capping the number of choices, it caps choices based on the sum of their probabilities. For the example prompt "I hear the hoof beats of ," we may want the model to provide "horses," "zebras" or "unicorns" as the next word. If we set the temperature to its maximum, without capping Top K or Top P, we increase the probability of getting unusual results such as "unicorns." If we set the temperature to 0, we increase the probability of "horses." If we set a high temperature and set Top K or Top P to the maximum, we increase the probability of "horses" or "zebras," and decrease the probability of "unicorns."</p> <h4 id=lengths>Lengths<a class=headerlink href=#lengths title="Permanent link">&para;</a></h4> <p>The following parameters control the length of the generated response.</p> <p><strong>Response length</strong>  Configures the minimum and maximum number of tokens to use in the generated response.</p> <p><strong>Length penalty</strong>  Length penalty optimizes the model to be more concise in its output by penalizing longer responses. Length penalty differs from response length as the response length is a hard cut off for the minimum or maximum response length.</p> <p>In technical terms, the length penalty penalizes the model exponentially for lengthy responses. 0.0 means no penalty. Set a value less than 0.0 for the model to generate longer sequences, or set a value greater than 0.0 for the model to produce shorter sequences.</p> <h4 id=repetitions>Repetitions<a class=headerlink href=#repetitions title="Permanent link">&para;</a></h4> <p>The following parameters help control repetition in the generated response.</p> <p><strong>Repetition penalty (presence penalty)</strong>  Prevents repetitions of the same words (tokens) in responses. 1.0 means no penalty. Greater than 1.0 decreases repetition.</p> <h3 id=vector-database>Vector Database<a class=headerlink href=#vector-database title="Permanent link">&para;</a></h3> <p>A vector database is optimized for storing and querying large vector arrays using machine learning techniques. It's highly scalable and fast at performing operations like similarity searches across vectors. </p> <p>Similarity search helps to identify items (vectors) that share similar characteristics or properties with the query item (a new vector). </p> <p><img alt src="https://latex.codecogs.com/svg.latex?j=argmin_i \left|| x - x_i\right||"></p> <p>Queries return results based on vector similarity scores, revealing hidden semantic connections in data. </p> <p><a href=https://faiss.ai/index.html>FAISS</a> from Facebook is a library for efficient similarity search and clustering of dense vectors. Faiss can compute vector Euclidien distance using GPU or CPU.</p> <p><a href=https://github.com/chroma-core/chroma>ChromaDB</a> is an open source embedding database which supports Queries, filtering, density estimation and similarity search. It can persist on local disk or use a server deployment. It uses collection for storing the documents, metadatas, embeddings, and ids. Chroma DB by default uses a sentence transformer model to calculate embeddings.</p> <p>(Code using ChromaDB <a href=https://github.com/jbcodeforce/ML-studies/blob/master/e2e-demos/qa_retrieval/Main.py>end to end solution with qa-retrieval</a>) or <a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/rag>code langchain/rag folder</a> specially <a href>build_agent_domain_rag.py</a>https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/rag/build_agent_domain_rag.py</p> <p>Docker compose to start chromadb</p> <div class=highlight><pre><span></span><code><span class=w>  </span><span class=nt>chroma</span><span class=p>:</span>
<span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ghcr.io/chroma-core/chroma:latest</span>
<span class=w>    </span><span class=nt>volumes</span><span class=p>:</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">./chromadb/.chroma/index</span>
<span class=w>    </span><span class=nt>ports</span><span class=p>:</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">8005:8000</span>
</code></pre></div> <p>Traditional open source index or database such as OpenSearch, Postgresql support now vector store and similarity search. </p> <h2 id=current-technology-landscape>Current Technology Landscape<a class=headerlink href=#current-technology-landscape title="Permanent link">&para;</a></h2> <h3 id=openai-chatgpt>OpenAI - <a href=https://openai.com/blog/chatgpt>ChatGPT</a><a class=headerlink href=#openai-chatgpt title="Permanent link">&para;</a></h3> <p>OpenAI is an AI research and deployment company. Their vision: intelligenceAI systems are generally smarter than humans: </p> <ol> <li>With broad general knowledge and domain expertise, GPT-4 can follow complex instructions in natural language and solve difficult problems with accuracy.</li> <li>DALLE 2 can create original, realistic images and art from a text description. It can combine concepts, attributes, and styles.</li> <li>Whisper can transcribe speech into text and translate many languages into English.</li> </ol> <p>Chat Generative pre-trained Transformer is a proprietary instruction-following model, which was released in November 2022. It is a system of models designed to create human like conversations and generating text by using statistics. It is a Causal Language Model (CLM) trained to predict the next token.</p> <p>The model was trained on trillions of words from the web, requiring massive numbers of GPUs to develop. The model was trained using Reinforcement Learning from Human Feedback (RLHF), using the same methods as <a href=https://en.wikipedia.org/wiki/GPT-3>InstructGPT</a>, but with different data collection setup. </p> <ul> <li><a href=https://platform.openai.com/docs/tutorials/web-qa-embeddings>Build an AI that can answer questions about your website</a>: crawl, use embeddings, and a search function. It is a good starting point for knowledge based app.</li> </ul> <h3 id=meta-with-llama-2-3>Meta with <a href=https://meta.ai/llama3/ >LLama 2 &amp; 3</a><a class=headerlink href=#meta-with-llama-2-3 title="Permanent link">&para;</a></h3> <p>A foundational, 65-billion-parameter large language model created by Facebook which has been open sourced for academic use. Many models have been released based on Llama2, but they also inherit the license requirement for non-commercial use.</p> <p>It is possible to run LLama2 on local machine with <a href=https://ollama.com/ >ollama</a>, and a <a href=https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/ollama>simple LangChain</a></p> <ul> <li><a href=https://anakin.ai/ >Anakin</a> is a platform to access different models </li> <li><a href=https://labs.perplexity.ai/ >Perplexity Labs</a>, a part of Perplexity AI, provides a user-friendly platform for developers to explore and experiment with large language models, including Llama 3</li> <li><a href=https://huggingface.co/chat/ >https://huggingface.co/chat/</a></li> <li><a href=https://replicate.com/ >Replicate</a> to run, fine-tune open-source models, and expose them as APIs. They also lead <a href=https://github.com/replicate/cog>Cog</a> an open-source tool for packaging machine learning models.</li> <li><a href=https://vercel.com/ >Vercel</a> serverless, hosting platform for web app and AI app. See a <a href=https://github.com/langchain-ai/langchain-nextjs-template/tree/main>langchain nextjs template</a> to easily deploy to Vercel.</li> </ul> <h3 id=mistral-mixture-of-experts><a href=https://mistral.ai/news/mixtral-of-experts/ >Mistral - Mixture of Experts</a><a class=headerlink href=#mistral-mixture-of-experts title="Permanent link">&para;</a></h3> <p>A french company who has developed the <strong>Mixtral 8x7B</strong> model, a high-quality sparse mixture of experts model (SMoE) with open weights.</p> <h3 id=google-gemini><a href=https://gemini.google.com/ >Google Gemini</a><a class=headerlink href=#google-gemini title="Permanent link">&para;</a></h3> <p>Gemini is the public Generative multimodal AI from Google DeepMind team with the support of 3 different sizes, the smallest being able to run on Mobile. Its <a href=https://blog.google/technology/ai/google-gemini-ai/#capabilities>reasoning capabilities</a> can help make sense of complex written and visual information.</p> <h3 id=amazon-sagemaker>Amazon SageMaker<a class=headerlink href=#amazon-sagemaker title="Permanent link">&para;</a></h3> <p><a href=https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html>SageMaker Jumpstart</a> provides pre-trained, open-source models for a wide range of problem types to get started on ML.</p> <p>It supports training on LLMs not in Bedrock, like <a href=https://github.com/openlm-research/open_llama>OpenLLama</a>, <a href=https://github.com/togethercomputer/RedPajama-Data>RedPajama</a>, <a href=https://www.mosaicml.com/blog/mpt-7b>Mosaic Pre-trained Transformer-7B</a>, <a href=https://huggingface.co/docs/transformers/main/model_doc/flan-ul2>Flan-T5/UL2</a>, <a href=https://huggingface.co/EleutherAI/gpt-j-6b>GPT-J-6B</a>, <a href=https://huggingface.co/EleutherAI/gpt-neox-20b>NEOX-20B</a> and <a href=https://huggingface.co/bigscience/bloom>Bloom/BloomZ</a>, with a gain of up to 40% faster.</p> <p>Some useful articles:</p> <ul> <li><a href=https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/ >AWS- Quickly build high-accuracy Generative AI applications on enterprise data using Amazon Kendra, LangChain, and large language models.</a></li> <li><a href=https://jbcodeforce.github.io/yarfba/ai-ml/sagemaker/ >SageMaker my own personal study.</a>.</li> </ul> <h3 id=local-inference-and-training>Local inference and training<a class=headerlink href=#local-inference-and-training title="Permanent link">&para;</a></h3> <h4 id=ollama>Ollama<a class=headerlink href=#ollama title="Permanent link">&para;</a></h4> <h4 id=vllm>vLLM<a class=headerlink href=#vllm title="Permanent link">&para;</a></h4> <h4 id=shimmy>shimmy<a class=headerlink href=#shimmy title="Permanent link">&para;</a></h4> <h2 id=deeper-dive>Deeper dive<a class=headerlink href=#deeper-dive title="Permanent link">&para;</a></h2> <ul> <li><a href=https://owasp.org/www-project-top-10-for-large-language-model-applications/descriptions/ >Vulnerabilities of LLM</a>.</li> <li><a href=https://towardsai.net/p/l/gans-for-synthetic-data-generation>GANs for Synthetic Data Generation.</a></li> <li><a href=https://www2.ed.gov/documents/ai-report/ai-report.pdf>Artificial Intelligence and the Future of Teaching and Learning</a>.</li> <li><a href=https://huggingface.co/docs/transformers/training>Fine-tune a pre-trained model HuggingFace tutorial</a>.</li> <li><a href=https://www.amazon.science/blog/emnlp-prompt-engineering-is-the-new-feature-engineering>Prompt engineering is the new feature engineering.</a></li> <li><a href=https://www.amazon.science/blog/amazon-sponsored-workshop-advances-deep-learning-for-code>Amazon-sponsored workshop advances deep learning for code.</a></li> <li><a href=https://catalog.workshops.aws/semantic-search/en-US/module-7-retrieval-augmented-generation>RAG with OpenSearch Service</a>.</li> <li><a href=https://github.com/ggerganov/llama.cpp>Running LLM on local laptop using llama.cpp</a></li> <li><a href="https://openreview.net/pdf?id=ntIq8Wm79G-">BertNet knowledge graphs from llm</a></li> <li><a href=https://jalammar.github.io/illustrated-transformer/ >Git repo explaining the transformer from Alammar.</a></li> <li><a href=https://arxiv.org/abs/1706.03762>Attention Is All You Need - research paper</a></li> <li><a href="https://www.youtube.com/watch?v=KTagKkWT2n4">On the role of LLM in planning - tutorial</a></li> </ul> <p><a href=prompt-eng/ >&gt;&gt;&gt; Prompt Engineering</a> </p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../coding/ddp/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Distributed Data Parallel"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Distributed Data Parallel </div> </div> </a> <a href=review/ class="md-footer__link md-footer__link--next" aria-label="Next: Skill set"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Skill set </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2018 - 2024 Jerome Boyer </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/jbcodeforce target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://linkedin.com/in/jeromeboyer target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M536.4-26.3c9.8-3.5 20.6-1 28 6.3s9.8 18.2 6.3 28l-178 496.9c-5 13.9-18.1 23.1-32.8 23.1-14.2 0-27-8.6-32.3-21.7l-64.2-158c-4.5-11-2.5-23.6 5.2-32.6l94.5-112.4c5.1-6.1 4.7-15-.9-20.6s-14.6-6-20.6-.9l-112.4 94.3c-9.1 7.6-21.6 9.6-32.6 5.2L38.1 216.8c-13.1-5.3-21.7-18.1-21.7-32.3 0-14.7 9.2-27.8 23.1-32.8z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"annotate": null, "base": "..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "content.code.copy", "search.highlight", "navigation.instant", "navigation.instant.progress", "navigation.tabs", "navigation.tabs.sticky", "navigation.instant", "navigation.tracking", "navigation.top", "navigation.footer"], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.79ae519e.min.js></script> </body> </html>