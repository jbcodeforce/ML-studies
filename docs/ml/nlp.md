# Natural Language Processing (NLP)


## Embedding

An embedding is a mathematical representation of a set of data points in a lower-dimensional space that captures their underlying relationships and patterns. There are different types: image, word, graph, video embeddings. Principal Component Analysis (PCA) and Singular Value Decomposition (SVD), auto-encoder, are dimensionality reduction techniques. [**Word2vec**](https://arxiv.org/abs/1301.3781) was used in GPT, it represents words in a high-dimensional vector space. The resulting embeddings capture semantic and syntactic relationships between word. The technique works by training a neural network on a large corpus of text data, to predict the context in which a given word appears. Embedding can improve data quality, reduce the need for manual data labeling, and enable more efficient computation.

See the [Encord's guide to embeddings in machine learning](https://encord.com/blog/embeddings-machine-learning/)

### 

## BERT



## Named Entity Recognition

Named Entity Recognition (NER) is a Natural Language Processing (NLP) technique used to identify and extract important entities from unstructured text data. It is achieved by using NN trained on labeled data to recognize patterns and extract entity from text.

Some techniques uses Gen AI model to do NER with a good prompt.


## Deeper Dive

* [PyTorch based NLP tutorial](https://github.com/graykode/nlp-tutorial)