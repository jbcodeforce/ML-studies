# AI discussions

The goal of this section is to get a set of links and content to know to support deep discussions around Gen AI, organized by interviewer category of subjects.


## 1. 𝗘𝘅𝗽𝗹𝗮𝗶𝗻 𝗟𝗟𝗠 𝗳𝘂𝗻𝗱𝗮𝗺𝗲𝗻𝘁𝗮𝗹𝘀

Cover the high-level workings of models like GPT-3, including transformers, pre-training, fine-tuning, etc.

* [x] [General LLM introduction](./index.md/#introduction)
* [x] [Transformer and GPT-3 summary](./index.md/#transformer-architecture)
* [x] [How LLM pre-training is done](./index.md/#pre-training-process)
* [ ] [How to fine tune existing model](./index.md/#model-fine-tuning)
* [ ] [How RAG works](./rag.md)

* [OpenAI API Code review from openai_api.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/openai_api.py). See [readme](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/openAI) to run the code.

## 𝟮. 𝗗𝗶𝘀𝗰𝘂𝘀𝘀 𝗽𝗿𝗼𝗺𝗽𝘁 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴

Talk through techniques like demonstrations, examples, and plain language prompts to optimize model performance.

* [Prompt Engineering](./prompt-eng.md)
* [Demonstrate prompt engineering]()
* [How to optimize model response performance with prompt]()

## 𝟯. 𝗦𝗵𝗮𝗿𝗲 𝗟𝗟𝗠 𝗽𝗿𝗼𝗷𝗲𝗰𝘁 𝗲𝘅𝗮𝗺𝗽𝗹𝗲𝘀

Walk through hands-on experiences leveraging models like GPT-3, Langchain, or Vector Databases.

* [Review RAG positioning, architecture](./rag.md)
* [Streamlit app to demonstrate RAG with Chromadb](https://github.com/jbcodeforce/ML-studies/blob/master/e2e-demos/qa_retrieval/Main.py). Offline tool to create vector store and indexing [build_agent_domain_rag.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/rag/build_agent_domain_rag.py) using a Lilian Weng's multi-agents blog.
* [Multiple queries RAG with LangChain]()

## 𝟰. 𝗦𝘁𝗮𝘆 𝘂𝗽𝗱𝗮𝘁𝗲𝗱 𝗼𝗻 𝗿𝗲𝘀𝗲𝗮𝗿𝗰𝗵

Mention latest papers and innovations in few-shot learning, prompt tuning, chain of thought prompting, etc.

* [few-shot learning]()
* [chain of thought]()
* [Agentic]()
* 

## 𝟱. 𝗗𝗶𝘃𝗲 𝗶𝗻𝘁𝗼 𝗺𝗼𝗱𝗲𝗹 𝗮𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲𝘀

Compare transformer networks like GPT-3 vs Codex. Explain self-attention, encodings, model depth, etc.

## 𝟲. 𝗗𝗶𝘀𝗰𝘂𝘀𝘀 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴 𝘁𝗲𝗰𝗵𝗻𝗶𝗾𝘂𝗲𝘀

Explain supervised fine-tuning, parameter efficient fine tuning, few-shot learning, and other methods to specialize pre-trained models for specific tasks.

* [supervised fine-tuning]()
* 

## 𝟳. 𝗗𝗲𝗺𝗼𝗻𝘀𝘁𝗿𝗮𝘁𝗲 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴 𝗲𝘅𝗽𝗲𝗿𝘁𝗶𝘀𝗲

- From tokenization to embeddings to deployment, showcase your ability to operationalize models at scale, and monitoring model inference.

## 𝟴. 𝗔𝘀𝗸 𝘁𝗵𝗼𝘂𝗴𝗵𝘁𝗳𝘂𝗹 𝗾𝘂𝗲𝘀𝘁𝗶𝗼𝗻𝘀

Inquire about model safety, bias, transparency, generalization, etc. to show strategic thinking.