# AI discussions

The goal of this section is to get a set of links and content to know to support deep discussions around Gen AI, organized by interviewer category of subjects.


## 1. ğ—˜ğ˜…ğ—½ğ—¹ğ—®ğ—¶ğ—» ğ—Ÿğ—Ÿğ—  ğ—³ğ˜‚ğ—»ğ—±ğ—®ğ—ºğ—²ğ—»ğ˜ğ—®ğ—¹ğ˜€

Cover the high-level workings of models like GPT-3, including transformers, pre-training, fine-tuning, etc.

* [x] [General LLM introduction](./index.md/#introduction)
* [x] [Transformer and GPT-3 summary](./index.md/#transformer-architecture)
* [x] [How LLM pre-training is done](./index.md/#pre-training-process)
* [ ] [How to fine tune existing model](./index.md/#model-fine-tuning)
* [ ] [How RAG works](./rag.md)

* [OpenAI API Code review from openai_api.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/openai_api.py). See [readme](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/openAI) to run the code.

## ğŸ®. ğ——ğ—¶ğ˜€ğ—°ğ˜‚ğ˜€ğ˜€ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´

Talk through techniques like demonstrations, examples, and plain language prompts to optimize model performance.

* [Prompt Engineering](./prompt-eng.md)
* [Demonstrate prompt engineering]()
* [How to optimize model response performance with prompt]()

## ğŸ¯. ğ—¦ğ—µğ—®ğ—¿ğ—² ğ—Ÿğ—Ÿğ—  ğ—½ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜ ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²ğ˜€

Walk through hands-on experiences leveraging models like GPT-3, Langchain, or Vector Databases.

* [Review RAG positioning, architecture](./rag.md)
* [Streamlit app to demonstrate RAG with Chromadb](https://github.com/jbcodeforce/ML-studies/blob/master/e2e-demos/qa_retrieval/Main.py). Offline tool to create vector store and indexing [build_agent_domain_rag.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/rag/build_agent_domain_rag.py) using a Lilian Weng's multi-agents blog.
* [Multiple queries RAG with LangChain]()

## ğŸ°. ğ—¦ğ˜ğ—®ğ˜† ğ˜‚ğ—½ğ—±ğ—®ğ˜ğ—²ğ—± ğ—¼ğ—» ğ—¿ğ—²ğ˜€ğ—²ğ—®ğ—¿ğ—°ğ—µ

Mention latest papers and innovations in few-shot learning, prompt tuning, chain of thought prompting, etc.

* [few-shot learning]()
* [chain of thought]()
* [Agentic]()
* 

## ğŸ±. ğ——ğ—¶ğ˜ƒğ—² ğ—¶ğ—»ğ˜ğ—¼ ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—®ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—²ğ˜€

Compare transformer networks like GPT-3 vs Codex. Explain self-attention, encodings, model depth, etc.

## ğŸ². ğ——ğ—¶ğ˜€ğ—°ğ˜‚ğ˜€ğ˜€ ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğ˜ğ—²ğ—°ğ—µğ—»ğ—¶ğ—¾ğ˜‚ğ—²ğ˜€

Explain supervised fine-tuning, parameter efficient fine tuning, few-shot learning, and other methods to specialize pre-trained models for specific tasks.

* [supervised fine-tuning]()
* 

## ğŸ³. ğ——ğ—²ğ—ºğ—¼ğ—»ğ˜€ğ˜ğ—¿ğ—®ğ˜ğ—² ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ—²ğ˜…ğ—½ğ—²ğ—¿ğ˜ğ—¶ğ˜€ğ—²

- From tokenization to embeddings to deployment, showcase your ability to operationalize models at scale, and monitoring model inference.

## ğŸ´. ğ—”ğ˜€ğ—¸ ğ˜ğ—µğ—¼ğ˜‚ğ—´ğ—µğ˜ğ—³ğ˜‚ğ—¹ ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€

Inquire about model safety, bias, transparency, generalization, etc. to show strategic thinking.