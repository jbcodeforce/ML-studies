# ML-Studies Project Context

This repository is a personal knowledge base and code laboratory for Machine Learning and Generative AI studies. It contains documentation (MkDocs), Jupyter notebooks, and Python code samples covering classical ML, deep learning, and LLM/agent frameworks.

## Project Structure

```
ML-studies/
├── src/                     # Shared library code
│   └── mlstudies/           # Reusable utilities
│       ├── config.py        # Centralized env vars and settings
│       ├── rag.py           # RAG utilities (text splitting, vector stores)
│       └── llm.py           # LLM client factories
├── examples/                # Python implementations organized by technology
│   ├── llm-langchain/       # LangChain/LangGraph examples with multiple LLM providers
│   ├── llm-ollama/          # Local LLM experiments with Ollama
│   ├── llamaindex/          # LlamaIndex RAG implementations
│   ├── pytorch/             # PyTorch deep learning experiments
│   ├── ml-python/           # Classical ML with scikit-learn
│   ├── deep-neural-net/     # Neural network fundamentals
│   └── haystack/            # Haystack framework experiments
├── docs/                    # MkDocs markdown documentation
├── notebooks/               # Jupyter notebooks for interactive learning
├── e2e-demos/               # End-to-end application demos
├── techno/                  # Technology-specific experiments (Gradio, Streamlit, etc.)
└── data/                    # Sample datasets for experiments
```

## Technology Stack

### Core Dependencies
- Python 3.11+
- Package management: **uv** with `pyproject.toml` per subproject

### Machine Learning
- scikit-learn: Classical ML algorithms (classifiers, regression, clustering)
- PyTorch: Deep learning, distributed training (DDP)
- TensorFlow/Keras: Neural networks (legacy examples)

### LLM Frameworks
- LangChain: Chains, prompts, document loaders, embeddings
- LangGraph: Stateful agent graphs with tool calling
- LlamaIndex: RAG pipelines and query engines

### LLM Providers
- OpenAI: GPT models via langchain-openai
- Anthropic: Claude models
- AWS Bedrock: Claude, Stability Diffusion
- IBM WatsonX: Granite, Llama models
- Mistral: via langchain-mistralai
- Ollama: Local model inference
- Cohere: Embeddings and chat
- Google Gemini

### Vector Stores
- ChromaDB: Primary vector store for RAG
- FAISS: Fast similarity search
- OpenSearch: Enterprise search

### UI Frameworks
- Gradio: Quick ML demos with UI
- Streamlit: Data apps and dashboards
- TaiPy: Enterprise data pipelines with UI
- NiceGUI: Python-native web UIs

### Documentation
- MkDocs with Material theme
- Jupyter notebooks for interactive content

## Coding Conventions

### File Organization
- Each subproject has its own `pyproject.toml` managed by **uv**
- Environment variables stored in `.env` at project root
- Use `dotenv` for loading environment configuration

### Python Style

**Preferred**: Use the shared config module:
```python
from mlstudies.config import OPENAI_API_KEY, ANTHROPIC_API_KEY

# Or import specific utilities
from mlstudies.llm import get_openai_chat, get_anthropic_chat
from mlstudies.rag import create_chroma_vectorstore, split_documents
```

**Legacy pattern** (still found in some files):
```python
from dotenv import load_dotenv
import os

load_dotenv("../../.env")
api_key = os.getenv("OPENAI_API_KEY")
```

### LangChain/LangGraph Patterns
```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages

# Define state with TypedDict
class State(TypedDict):
    messages: Annotated[list, add_messages]

# Build graph with nodes and edges
graph_builder = StateGraph(State)
graph_builder.add_node("node_name", node_function)
graph_builder.set_entry_point("node_name")
graph = graph_builder.compile()
```

### RAG Implementation Pattern
```python
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# 1. Load documents
loader = WebBaseLoader(web_paths=(url,))
docs = loader.load()

# 2. Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = splitter.split_documents(docs)

# 3. Create vector store with embeddings
vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())

# 4. Create retriever
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 6})
```

### Agent Pattern with Tools
```python
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.tools.retriever import create_retriever_tool

# Create tools
retriever_tool = create_retriever_tool(retriever, "tool_name", "Tool description")
tools = [retriever_tool, other_tools]

# Create agent with prompt
agent = create_tool_calling_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# Invoke
result = agent_executor.invoke({"input": "user query"})
```

## Environment Setup with uv

### Installing uv
```bash
# macOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Or with Homebrew
brew install uv
```

### Working with Subprojects

Each subproject is self-contained with its own `pyproject.toml`. Navigate to the subproject directory and use uv commands:

```bash
# Navigate to a subproject
cd examples/llm-langchain

# Create virtual environment and install dependencies
uv sync

# Install with optional dependencies (e.g., for OpenAI experiments)
uv sync --extra openai

# Install all optional dependencies
uv sync --all-extras

# Run a script
uv run python langgraph/basic_graph.py

# Add a new dependency
uv add <package-name>
```

### Subproject Dependency Structure

| Subproject | Optional Extras | Usage |
|------------|-----------------|-------|
| `src/` | `langchain`, `rag`, `openai`, `anthropic`, `mistral`, `watsonx`, `all` | Shared utilities |
| `examples/llm-langchain/` | `openai`, `anthropic`, `bedrock`, `mistral`, `cohere`, `gemini`, `watsonx`, `langgraph`, `rag`, `qa`, `featurestores`, `all` | LLM framework experiments |
| `examples/pytorch/` | `ddp`, `notebook` | Deep learning |
| `examples/ml-python/` | `anomaly`, `kaggle`, `neural`, `notebook` | Classical ML |
| `examples/llamaindex/` | `anthropic`, `chromadb` | LlamaIndex RAG |
| `techno/gradio/` | - | Gradio UI demos |
| `techno/streamlit/` | - | Streamlit apps |
| `techno/nicegui/` | `llm` | NiceGUI experiments |
| `techno/crew-ai/` | - | Multi-agent systems |
| `notebooks/` | `deep-learning`, `spark` | Jupyter notebooks |

### Example: Running LangGraph with OpenAI
```bash
cd examples/llm-langchain
uv sync --extra openai --extra langgraph
uv run python langgraph/basic_graph.py
```

### Example: Running PyTorch DDP
```bash
cd examples/pytorch
uv sync --extra ddp
uv run python ddp/train.py
```

### Docker Development
```bash
# Use provided Docker files
./startPythonDocker.sh
./startJupyter.sh

# Or use MkDocs container for docs
docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material
```

### Required Environment Variables (.env)
```
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
TAVILY_API_KEY=
IBM_WATSON_PROJECT_ID=
IBM_WATSONX_APIKEY=
IBM_WATSONX_URL=
MISTRAL_API_KEY=
COHERE_API_KEY=
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=
```

## Documentation Guidelines

- Write markdown files in `docs/` directory
- Follow MkDocs structure defined in `mkdocs.yml`
- Use admonitions for notes and warnings
- Include code examples with proper syntax highlighting
- Reference Python code samples from `examples/` directory

## Testing and Running Code

### Running Examples with uv
```bash
# Navigate to specific technology directory
cd examples/llm-langchain

# Sync dependencies with required extras
uv sync --extra langgraph

# Run Python scripts
uv run python langgraph/basic_graph.py
```

### Running Demos
```bash
cd e2e-demos/qa_retrieval
uv sync
uv run streamlit run main.py
```

### Building Documentation
```bash
# Local development
mkdocs serve

# Deploy to GitHub Pages
mkdocs gh-deploy
```

## Key Concepts Covered

### Machine Learning
- Supervised learning: Classification, Regression
- Unsupervised learning: Clustering, PCA
- Deep learning: Neural networks, CNNs, RNNs
- Distributed training: PyTorch DDP

### Generative AI
- Prompt engineering techniques
- RAG (Retrieval-Augmented Generation)
- Agent architectures (ReAct, tool calling)
- Multi-agent systems
- Model Context Protocol (MCP)

### Data Processing
- Feature engineering
- Data visualization (matplotlib, seaborn)
- Pandas data manipulation

---

## Remaining Refactoring Opportunities

### 1. Migrate Example Files to Use Shared Library

The `src/mlstudies/` library provides centralized utilities. Example files can be updated to use:

```python
# Instead of manual dotenv loading:
from mlstudies.config import OPENAI_API_KEY, ANTHROPIC_API_KEY

# Instead of duplicate RAG code:
from mlstudies.rag import split_documents, create_chroma_vectorstore

# Instead of manual LLM instantiation:
from mlstudies.llm import get_openai_chat, get_anthropic_chat
```

### 2. File Reference Updates

Documentation and internal imports may still reference old paths. Files that may need updates:
- `examples/llm-langchain/rag/build_agent_domain_rag.py`
- `examples/llm-langchain/openai/openai_retrieval_lc.py`
- `examples/llm-langchain/langgraph/adaptive_rag.py`
- `examples/llm-langchain/qa/qa-pipeline.py`

## Completed Refactoring

1. ~~**High**: Consolidate requirements.txt files~~ - DONE: Migrated to uv with pyproject.toml
2. ~~**High**: Remove duplicate file (`TestCallToOneLLM copy.py`)~~ - DONE: Removed
3. ~~**High**: Restructure to src/examples layout~~ - DONE: `code/` renamed to `examples/`, `src/mlstudies/` created
4. ~~**Medium**: Create shared config module~~ - DONE: `src/mlstudies/config.py`
5. ~~**Medium**: Extract common RAG patterns~~ - DONE: `src/mlstudies/rag.py`
6. ~~**Low**: Standardize file naming~~ - DONE: All Python files now use snake_case
